{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"","text":"Programming for Data Science <p>       This documentation covers essential technologies and tools you need to become a confident Fullstack Data Scientist. These courses can be taken individually or as a whole comprehensive course.     </p> Python Programming Fundamentals <p>Start with the basics of Python, a versatile and powerful programming language. Master syntax, data structures, algorithms, and essential programming concepts.</p> Python Basics DSA Functions OOP Duration 4 weeks Level Beginner Enroll Now Use Free Resources Version Control with Git &amp; GitHub <p>Master version control with Git and collaborate effectively using GitHub. Learn branching, merging, pull requests, and modern development workflows.</p> Version Control Collaboration Branching Pull Requests Duration 2 weeks Level Beginner Enroll Now Use Free Resources SQL for Data Analysis <p>Learn to query, manipulate, and analyze data using SQL. Master joins, aggregations, subqueries, and database management for data-driven insights.</p> Queries Joins Aggregations Subqueries Duration 3 weeks Level Beginner Enroll Now Use Free Resources Data Analysis &amp; Visualization with Python <p>Explore data analysis using NumPy, Pandas, and Matplotlib. Learn to clean, transform, visualize, and extract actionable insights from real-world datasets.</p> NumPy Pandas Matplotlib Data Cleaning Duration 5 weeks Level Intermediate Enroll Now Use Free Resources Mathematics &amp; Statistics for Data Science <p>Build a strong mathematical foundation for data science. Master statistics, probability, hypothesis testing, linear algebra, and essential calculus concepts.</p> Statistics Probability Linear Algebra Hypothesis Testing Duration 6 weeks Level Intermediate Enroll Now Use Free Resources Machine Learning with Python <p>Discover machine learning principles and gain hands-on experience with scikit-learn. Build, evaluate, and optimize predictive models for real-world problems.</p> Scikit-learn Algorithms Model Evaluation Hyperparameter Tuning Duration 6 weeks Level Advanced Enroll Now Use Free Resources Time Series Analysis <p>Learn time series analysis and forecasting techniques. Master ARIMA, seasonal decomposition, trend analysis, and advanced forecasting methods for temporal data.</p> ARIMA Forecasting Seasonality Trends Duration 4 weeks Level Intermediate Enroll Now Use Free Resources Deep Learning with PyTorch <p>Master neural networks and deep learning architectures using PyTorch. Build CNNs, implement gradient descent, and create cutting-edge AI models.</p> PyTorch Neural Networks CNNs Deep Learning Duration 8 weeks Level Advanced Enroll Now Use Free Resources Alumni Success Our Alumni Have Worked With <p>       Our graduates and partners have gone on to work with leading organizations across East Africa and beyond.     </p> Why Choose Our Courses? <p>Our comprehensive approach ensures you gain practical, job-ready skills</p> Hands-On Learning <p>Each module is designed with practical exercises and real-world projects to ensure you can apply what you've learned.</p> Flexible Learning Path <p>Choose to follow the entire course or focus on specific modules that meet your individual learning goals.</p> Expert Guidance <p>Gain insights from industry professionals who are passionate about data science and dedicated to your success.</p> notifications Don't Miss Any Updates! <p>       To be among the first to hear about future updates of the course materials, simply enter your email below, follow us on       open_in_new Twitter,       or subscribe to our play_circle YouTube channel.     </p> <p> Last Updated: November 5, 2025 | Author: Juma Shafara </p>"},{"location":"books/","title":"Books","text":"<p>A curated collection of essential books for your data science journey. All books are available for free download.</p>"},{"location":"books/#programming-for-data-science-complete-roadmap","title":"Programming for Data Science Complete RoadMap","text":"<p>Author: Juma Shafara</p> <p>This guide is designed to help you navigate the essential skills needed to become a successful data scientist. Whether you ' re just starting out or looking to enhance your existing skills, this roadmap will provide a clear and structured path.</p> <p>Topics: Python | Data Analysis | Pandas</p> <p>Download PDF</p>"},{"location":"books/#python-for-data-analysis","title":"Python for Data Analysis","text":"<p>Author: Wes McKinney</p> <p>The definitive guide to using Python for data analysis. Learn pandas, NumPy, and other essential tools from the creator of pandas.</p> <p>Topics: Python | Data Analysis | Pandas</p> <p>Download PDF</p>"},{"location":"books/#hands-on-machine-learning","title":"Hands-On Machine Learning","text":"<p>Author: Aurelien Geron</p> <p>A comprehensive guide to machine learning with scikit-learn, Keras, and TensorFlow. Perfect for beginners and practitioners.</p> <p>Topics: Machine Learning | Scikit-learn | TensorFlow</p> <p>Download PDF</p>"},{"location":"books/#deep-learning","title":"Deep Learning","text":"<p>Authors: Ian Goodfellow, Yoshua Bengio, Aaron Courville</p> <p>The definitive textbook on deep learning. Covers the mathematical foundations and practical applications of neural networks.</p> <p>Topics: Deep Learning | Neural Networks | Advanced</p> <p>Download PDF</p>"},{"location":"books/#feature-engineering-for-machine-learning","title":"Feature Engineering for Machine Learning","text":"<p>Authors: Ian Goodfellow, Yoshua Bengio, Aaron Courville</p> <p>Machine learning fits mathematical models to data in order to derive insights or make predictions.</p> <p>Topics: Feature Engineering, Machine Learning, Neural Networks</p> <p>Download PDF</p> <p>Suggest a Book</p> <p>Know a great data science book that should be on this list? Email us your suggestions!</p>"},{"location":"DSA/01_introduction/","title":"Introduction to Data Structures &amp; Algorithms","text":"<p>Data Structures and Algorithms (DSA) are fundamental concepts in computer science that help you write efficient, scalable, and optimized code. Understanding DSA is essential for problem-solving and technical interviews.</p>"},{"location":"DSA/01_introduction/#what-are-data-structures","title":"What are Data Structures?","text":"<p>Data structures are specialized formats for organizing, processing, storing, and retrieving data. They provide a way to manage large amounts of data efficiently.</p>"},{"location":"DSA/01_introduction/#common-data-structures","title":"Common Data Structures:","text":"<ul> <li>Arrays &amp; Lists - Sequential collection of elements</li> <li>Stacks - Last-In-First-Out (LIFO) structure</li> <li>Queues - First-In-First-Out (FIFO) structure</li> <li>Linked Lists - Chain of nodes with data and pointers</li> <li>Trees - Hierarchical structure with parent-child relationships</li> <li>Graphs - Network of nodes connected by edges</li> <li>Hash Tables - Key-value pairs for fast lookup</li> </ul>"},{"location":"DSA/01_introduction/#what-are-algorithms","title":"What are Algorithms?","text":"<p>Algorithms are step-by-step procedures or formulas for solving problems. They define the logic and operations needed to accomplish a specific task.</p>"},{"location":"DSA/01_introduction/#algorithm-categories","title":"Algorithm Categories:","text":"<ul> <li>Searching - Finding elements (Linear Search, Binary Search)</li> <li>Sorting - Arranging elements (Bubble Sort, Quick Sort, Merge Sort)</li> <li>Traversal - Visiting all elements (DFS, BFS)</li> <li>Dynamic Programming - Breaking problems into subproblems</li> <li>Greedy - Making locally optimal choices</li> <li>Divide and Conquer - Splitting problems into smaller parts</li> </ul>"},{"location":"DSA/01_introduction/#why-learn-dsa","title":"Why Learn DSA?","text":""},{"location":"DSA/01_introduction/#1-write-efficient-code","title":"1. Write Efficient Code","text":"<p>Understanding DSA helps you choose the right data structure and algorithm for optimal performance.</p>"},{"location":"DSA/01_introduction/#2-problem-solving-skills","title":"2. Problem-Solving Skills","text":"<p>DSA teaches you how to break down complex problems into manageable parts.</p>"},{"location":"DSA/01_introduction/#3-technical-interviews","title":"3. Technical Interviews","text":"<p>Most tech companies (Google, Amazon, Microsoft, Meta) heavily test DSA knowledge.</p>"},{"location":"DSA/01_introduction/#4-foundation-for-advanced-topics","title":"4. Foundation for Advanced Topics","text":"<p>DSA is essential for machine learning, database design, and system architecture.</p>"},{"location":"DSA/01_introduction/#time-and-space-complexity","title":"Time and Space Complexity","text":"<p>One of the most important concepts in DSA is analyzing algorithm efficiency using Big O notation.</p>"},{"location":"DSA/01_introduction/#big-o-notation","title":"Big O Notation","text":"<p>Big O describes the worst-case performance of an algorithm:</p> Notation Name Example O(1) Constant Accessing array element by index O(log n) Logarithmic Binary search O(n) Linear Iterating through array O(n log n) Linearithmic Merge sort, quick sort O(n\u00b2) Quadratic Nested loops, bubble sort O(2\u207f) Exponential Recursive fibonacci"},{"location":"DSA/01_introduction/#example-analyzing-complexity","title":"Example: Analyzing Complexity","text":"<pre><code># O(1) - Constant time\ndef get_first_element(arr):\n    return arr[0]\n\n# O(n) - Linear time\ndef find_max(arr):\n    max_val = arr[0]\n    for num in arr:\n        if num &gt; max_val:\n            max_val = num\n    return max_val\n\n# O(n\u00b2) - Quadratic time\ndef bubble_sort(arr):\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n-i-1):\n            if arr[j] &gt; arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]\n    return arr\n</code></pre>"},{"location":"DSA/01_introduction/#data-structure-comparison","title":"Data Structure Comparison","text":"Data Structure Access Search Insert Delete Use Case Array O(1) O(n) O(n) O(n) Fixed-size collections Linked List O(n) O(n) O(1) O(1) Dynamic size, frequent insertions Stack O(n) O(n) O(1) O(1) Undo operations, recursion Queue O(n) O(n) O(1) O(1) Task scheduling, BFS Hash Table O(1) O(1) O(1) O(1) Fast lookups, caching Binary Tree O(log n) O(log n) O(log n) O(log n) Hierarchical data, BST"},{"location":"DSA/01_introduction/#python-for-dsa","title":"Python for DSA","text":"<p>Python is an excellent language for learning DSA due to its:</p> <ul> <li>Simple syntax - Easy to focus on concepts</li> <li>Built-in data structures - Lists, dictionaries, sets</li> <li>Rich libraries - Collections, heapq, itertools</li> </ul>"},{"location":"DSA/01_introduction/#python-built-in-data-structures","title":"Python Built-in Data Structures","text":"<pre><code># List (Dynamic Array)\nnumbers = [1, 2, 3, 4, 5]\n\n# Dictionary (Hash Table)\nstudent = {\"name\": \"John\", \"age\": 20, \"grade\": \"A\"}\n\n# Set (Unique elements)\nunique_numbers = {1, 2, 3, 4, 5}\n\n# Tuple (Immutable list)\ncoordinates = (10, 20)\n\n# Stack using list\nstack = []\nstack.append(1)  # push\nstack.pop()      # pop\n\n# Queue using collections\nfrom collections import deque\nqueue = deque()\nqueue.append(1)      # enqueue\nqueue.popleft()      # dequeue\n</code></pre>"},{"location":"DSA/01_introduction/#learning-path","title":"Learning Path","text":"<p>This tutorial series will cover:</p> <ol> <li>Arrays and Lists - Foundation of sequential data</li> <li>Stacks and Queues - LIFO and FIFO structures</li> <li>Linked Lists - Dynamic data structures</li> <li>Trees - Hierarchical structures and BST</li> <li>Hash Tables - Fast key-value storage</li> <li>Sorting Algorithms - Organizing data efficiently</li> <li>Searching Algorithms - Finding elements quickly</li> <li>Graph Algorithms - Network and relationship problems</li> </ol>"},{"location":"DSA/01_introduction/#practice-resources","title":"Practice Resources","text":"<p>To master DSA, practice on these platforms:</p> <ol> <li>LeetCode - leetcode.com</li> <li>HackerRank - hackerrank.com</li> <li>CodeForces - codeforces.com</li> <li>GeeksforGeeks - geeksforgeeks.org</li> </ol> <p>Consistent Practice</p> <p>Spend 30-60 minutes daily solving DSA problems. Start with easy problems and gradually move to medium and hard ones.</p> <p>Interview Preparation</p> <p>For technical interviews, focus on understanding the problem-solving approach rather than memorizing solutions.</p> <p>Next Tutorial: Arrays and Lists</p>"},{"location":"DSA/02_arrays_and_lists/","title":"Arrays and Lists","text":"<p>Arrays and lists are fundamental data structures that store collections of elements in sequential order. They are the building blocks for many other data structures and algorithms.</p>"},{"location":"DSA/02_arrays_and_lists/#what-is-an-array","title":"What is an Array?","text":"<p>An array is a collection of elements stored in contiguous memory locations. Each element can be accessed using an index.</p>"},{"location":"DSA/02_arrays_and_lists/#key-characteristics","title":"Key Characteristics:","text":"<ul> <li>Fixed Size - Size is defined at creation (in traditional arrays)</li> <li>Same Data Type - All elements are of the same type</li> <li>Index-based Access - O(1) time to access any element</li> <li>Contiguous Memory - Elements stored next to each other</li> </ul>"},{"location":"DSA/02_arrays_and_lists/#python-lists-vs-arrays","title":"Python Lists vs Arrays","text":"<p>Python's <code>list</code> is actually a dynamic array that can: - Grow and shrink in size - Store different data types - Provide built-in methods for manipulation</p> <pre><code># Python list (dynamic array)\nnumbers = [1, 2, 3, 4, 5]\n\n# Access by index\nprint(numbers[0])  # 1\nprint(numbers[-1]) # 5 (last element)\n\n# Modify elements\nnumbers[2] = 10\nprint(numbers)  # [1, 2, 10, 4, 5]\n</code></pre>"},{"location":"DSA/02_arrays_and_lists/#common-array-operations","title":"Common Array Operations","text":""},{"location":"DSA/02_arrays_and_lists/#1-accessing-elements","title":"1. Accessing Elements","text":"<pre><code># Direct access - O(1)\narr = [10, 20, 30, 40, 50]\nfirst = arr[0]      # 10\nlast = arr[-1]      # 50\nmiddle = arr[2]     # 30\n\n# Slicing - O(k) where k is slice size\nsubset = arr[1:4]   # [20, 30, 40]\nreversed_arr = arr[::-1]  # [50, 40, 30, 20, 10]\n</code></pre>"},{"location":"DSA/02_arrays_and_lists/#2-insertion","title":"2. Insertion","text":"<pre><code>arr = [1, 2, 3, 4, 5]\n\n# Append at end - O(1) amortized\narr.append(6)\nprint(arr)  # [1, 2, 3, 4, 5, 6]\n\n# Insert at specific position - O(n)\narr.insert(2, 99)\nprint(arr)  # [1, 2, 99, 3, 4, 5, 6]\n\n# Extend with another list - O(k)\narr.extend([7, 8, 9])\nprint(arr)  # [1, 2, 99, 3, 4, 5, 6, 7, 8, 9]\n</code></pre>"},{"location":"DSA/02_arrays_and_lists/#3-deletion","title":"3. Deletion","text":"<pre><code>arr = [10, 20, 30, 40, 50]\n\n# Remove by value - O(n)\narr.remove(30)\nprint(arr)  # [10, 20, 40, 50]\n\n# Remove by index - O(n)\ndel arr[1]\nprint(arr)  # [10, 40, 50]\n\n# Pop last element - O(1)\nlast = arr.pop()\nprint(last)  # 50\nprint(arr)   # [10, 40]\n\n# Pop specific index - O(n)\nelement = arr.pop(0)\nprint(element)  # 10\n</code></pre>"},{"location":"DSA/02_arrays_and_lists/#4-searching","title":"4. Searching","text":"<pre><code>arr = [5, 2, 8, 1, 9, 3]\n\n# Linear search - O(n)\nif 8 in arr:\n    print(\"Found!\")\n\n# Find index - O(n)\nindex = arr.index(8)\nprint(index)  # 2\n\n# Count occurrences - O(n)\ncount = arr.count(2)\nprint(count)  # 1\n</code></pre>"},{"location":"DSA/02_arrays_and_lists/#time-complexity-summary","title":"Time Complexity Summary","text":"Operation Time Complexity Access by index O(1) Search by value O(n) Insert at end O(1) amortized Insert at beginning O(n) Insert at middle O(n) Delete from end O(1) Delete from beginning O(n) Delete from middle O(n)"},{"location":"DSA/02_arrays_and_lists/#common-array-problems","title":"Common Array Problems","text":""},{"location":"DSA/02_arrays_and_lists/#problem-1-find-maximum-element","title":"Problem 1: Find Maximum Element","text":"<pre><code>def find_max(arr):\n    \"\"\"\n    Find the maximum element in an array\n    Time Complexity: O(n)\n    Space Complexity: O(1)\n    \"\"\"\n    if not arr:\n        return None\n\n    max_val = arr[0]\n    for num in arr:\n        if num &gt; max_val:\n            max_val = num\n\n    return max_val\n\n# Test\nnumbers = [3, 7, 2, 9, 1, 5]\nprint(find_max(numbers))  # 9\n\n# Using built-in\nprint(max(numbers))  # 9\n</code></pre>"},{"location":"DSA/02_arrays_and_lists/#problem-2-reverse-an-array","title":"Problem 2: Reverse an Array","text":"<pre><code>def reverse_array(arr):\n    \"\"\"\n    Reverse an array in-place\n    Time Complexity: O(n)\n    Space Complexity: O(1)\n    \"\"\"\n    left = 0\n    right = len(arr) - 1\n\n    while left &lt; right:\n        # Swap elements\n        arr[left], arr[right] = arr[right], arr[left]\n        left += 1\n        right -= 1\n\n    return arr\n\n# Test\nnumbers = [1, 2, 3, 4, 5]\nprint(reverse_array(numbers))  # [5, 4, 3, 2, 1]\n\n# Using slicing (creates new list)\nprint(numbers[::-1])  # [5, 4, 3, 2, 1]\n</code></pre>"},{"location":"DSA/02_arrays_and_lists/#problem-3-remove-duplicates","title":"Problem 3: Remove Duplicates","text":"<pre><code>def remove_duplicates(arr):\n    \"\"\"\n    Remove duplicates from sorted array\n    Time Complexity: O(n)\n    Space Complexity: O(1)\n    \"\"\"\n    if not arr:\n        return 0\n\n    write_index = 1\n\n    for i in range(1, len(arr)):\n        if arr[i] != arr[i-1]:\n            arr[write_index] = arr[i]\n            write_index += 1\n\n    return write_index\n\n# Test\nsorted_nums = [1, 1, 2, 2, 3, 4, 4, 5]\nlength = remove_duplicates(sorted_nums)\nprint(sorted_nums[:length])  # [1, 2, 3, 4, 5]\n\n# Using set (for unsorted arrays)\nunique = list(set(sorted_nums))\nprint(sorted(unique))  # [1, 2, 3, 4, 5]\n</code></pre>"},{"location":"DSA/02_arrays_and_lists/#problem-4-two-sum-problem","title":"Problem 4: Two Sum Problem","text":"<pre><code>def two_sum(arr, target):\n    \"\"\"\n    Find two numbers that sum to target\n    Time Complexity: O(n)\n    Space Complexity: O(n)\n    \"\"\"\n    seen = {}\n\n    for i, num in enumerate(arr):\n        complement = target - num\n\n        if complement in seen:\n            return [seen[complement], i]\n\n        seen[num] = i\n\n    return None\n\n# Test\nnumbers = [2, 7, 11, 15]\ntarget = 9\nprint(two_sum(numbers, target))  # [0, 1] (indices)\n</code></pre>"},{"location":"DSA/02_arrays_and_lists/#problem-5-rotate-array","title":"Problem 5: Rotate Array","text":"<pre><code>def rotate_array(arr, k):\n    \"\"\"\n    Rotate array to the right by k steps\n    Time Complexity: O(n)\n    Space Complexity: O(1)\n    \"\"\"\n    n = len(arr)\n    k = k % n  # Handle k &gt; n\n\n    # Reverse entire array\n    arr.reverse()\n    # Reverse first k elements\n    arr[:k] = arr[:k][::-1]\n    # Reverse remaining elements\n    arr[k:] = arr[k:][::-1]\n\n    return arr\n\n# Test\nnumbers = [1, 2, 3, 4, 5, 6, 7]\nk = 3\nprint(rotate_array(numbers, k))  # [5, 6, 7, 1, 2, 3, 4]\n</code></pre>"},{"location":"DSA/02_arrays_and_lists/#multi-dimensional-arrays","title":"Multi-dimensional Arrays","text":"<p>Arrays can have multiple dimensions, commonly used for matrices and grids.</p> <pre><code># 2D Array (Matrix)\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\n# Access elements\nprint(matrix[0][0])  # 1\nprint(matrix[1][2])  # 6\n\n# Traverse 2D array\nfor row in matrix:\n    for element in row:\n        print(element, end=' ')\n    print()\n\n# Create 2D array with list comprehension\nrows, cols = 3, 4\nmatrix = [[0 for _ in range(cols)] for _ in range(rows)]\n</code></pre>"},{"location":"DSA/02_arrays_and_lists/#matrix-operations","title":"Matrix Operations","text":"<pre><code>def transpose_matrix(matrix):\n    \"\"\"\n    Transpose a matrix (swap rows and columns)\n    \"\"\"\n    rows = len(matrix)\n    cols = len(matrix[0])\n\n    transposed = [[matrix[r][c] for r in range(rows)]\n                  for c in range(cols)]\n\n    return transposed\n\n# Test\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6]\n]\nprint(transpose_matrix(matrix))\n# [[1, 4],\n#  [2, 5],\n#  [3, 6]]\n</code></pre>"},{"location":"DSA/02_arrays_and_lists/#python-list-comprehensions","title":"Python List Comprehensions","text":"<p>List comprehensions provide a concise way to create and manipulate lists.</p> <pre><code># Basic comprehension\nsquares = [x**2 for x in range(10)]\nprint(squares)  # [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n# With condition\nevens = [x for x in range(20) if x % 2 == 0]\nprint(evens)  # [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n\n# With transformation\nwords = ['hello', 'world', 'python']\nupper_words = [word.upper() for word in words]\nprint(upper_words)  # ['HELLO', 'WORLD', 'PYTHON']\n\n# Nested comprehension\nmatrix = [[i*j for j in range(3)] for i in range(3)]\nprint(matrix)\n# [[0, 0, 0],\n#  [0, 1, 2],\n#  [0, 2, 4]]\n</code></pre>"},{"location":"DSA/02_arrays_and_lists/#best-practices","title":"Best Practices","text":"<ol> <li>Use Built-in Methods - Python's list methods are optimized</li> <li>Avoid Growing Arrays in Loops - Pre-allocate if size is known</li> <li>Use Slicing Carefully - Creates new list (memory overhead)</li> <li>Consider NumPy - For numerical operations on large arrays</li> <li>Use Generators - For large datasets to save memory</li> </ol> <pre><code># Good: Pre-allocate\nresult = [0] * n\nfor i in range(n):\n    result[i] = compute(i)\n\n# Bad: Growing in loop\nresult = []\nfor i in range(n):\n    result.append(compute(i))\n\n# Better: List comprehension\nresult = [compute(i) for i in range(n)]\n</code></pre> <p>Practice Problems</p> <p>Try solving these classic array problems: - Find missing number in array (1 to n) - Merge two sorted arrays - Find majority element - Stock buy and sell problem - Maximum subarray sum (Kadane's algorithm)</p> <p>NumPy for Numerical Arrays</p> <p>For scientific computing and numerical operations, consider using NumPy arrays which are more efficient than Python lists for mathematical operations.</p> <p>Next Tutorial: Stacks and Queues</p>"},{"location":"DSA/03_stacks_and_queues/","title":"Stacks and Queues","text":"<p>Stacks and Queues are linear data structures that follow specific ordering principles for adding and removing elements. They are widely used in algorithms and system design.</p>"},{"location":"DSA/03_stacks_and_queues/#stack","title":"Stack","text":"<p>A Stack is a Last-In-First-Out (LIFO) data structure where the last element added is the first one to be removed.</p>"},{"location":"DSA/03_stacks_and_queues/#real-world-analogies","title":"Real-World Analogies:","text":"<ul> <li>Stack of plates (add/remove from top)</li> <li>Browser back button (most recent page first)</li> <li>Undo operation in text editor</li> <li>Function call stack in programming</li> </ul>"},{"location":"DSA/03_stacks_and_queues/#stack-operations","title":"Stack Operations","text":"Operation Description Time Complexity push(item) Add item to top O(1) pop() Remove and return top item O(1) peek() View top item without removing O(1) isEmpty() Check if stack is empty O(1) size() Get number of elements O(1)"},{"location":"DSA/03_stacks_and_queues/#implementing-stack-in-python","title":"Implementing Stack in Python","text":"<pre><code># Using Python list\nclass Stack:\n    def __init__(self):\n        self.items = []\n\n    def push(self, item):\n        \"\"\"Add item to top of stack\"\"\"\n        self.items.append(item)\n\n    def pop(self):\n        \"\"\"Remove and return top item\"\"\"\n        if not self.is_empty():\n            return self.items.pop()\n        return None\n\n    def peek(self):\n        \"\"\"View top item without removing\"\"\"\n        if not self.is_empty():\n            return self.items[-1]\n        return None\n\n    def is_empty(self):\n        \"\"\"Check if stack is empty\"\"\"\n        return len(self.items) == 0\n\n    def size(self):\n        \"\"\"Return number of items\"\"\"\n        return len(self.items)\n\n# Usage\nstack = Stack()\nstack.push(1)\nstack.push(2)\nstack.push(3)\nprint(stack.pop())    # 3\nprint(stack.peek())   # 2\nprint(stack.size())   # 2\n</code></pre>"},{"location":"DSA/03_stacks_and_queues/#simple-stack-using-list","title":"Simple Stack using List","text":"<pre><code># Stack using built-in list\nstack = []\n\n# Push\nstack.append(1)\nstack.append(2)\nstack.append(3)\n\n# Pop\ntop = stack.pop()  # 3\n\n# Peek\ntop = stack[-1] if stack else None  # 2\n\n# Check empty\nis_empty = len(stack) == 0\n</code></pre>"},{"location":"DSA/03_stacks_and_queues/#stack-applications","title":"Stack Applications","text":""},{"location":"DSA/03_stacks_and_queues/#1-balanced-parentheses","title":"1. Balanced Parentheses","text":"<pre><code>def is_balanced(expression):\n    \"\"\"\n    Check if parentheses are balanced\n    Time Complexity: O(n)\n    Space Complexity: O(n)\n    \"\"\"\n    stack = []\n    matching = {'(': ')', '{': '}', '[': ']'}\n\n    for char in expression:\n        if char in matching:\n            stack.append(char)\n        elif char in matching.values():\n            if not stack or matching[stack.pop()] != char:\n                return False\n\n    return len(stack) == 0\n\n# Test\nprint(is_balanced(\"()\"))           # True\nprint(is_balanced(\"()[]{}\"))       # True\nprint(is_balanced(\"(]\"))           # False\nprint(is_balanced(\"([)]\"))         # False\nprint(is_balanced(\"{[]}\"))         # True\n</code></pre>"},{"location":"DSA/03_stacks_and_queues/#2-reverse-string","title":"2. Reverse String","text":"<pre><code>def reverse_string(s):\n    \"\"\"\n    Reverse a string using stack\n    Time Complexity: O(n)\n    Space Complexity: O(n)\n    \"\"\"\n    stack = []\n\n    # Push all characters\n    for char in s:\n        stack.append(char)\n\n    # Pop all characters\n    reversed_s = ''\n    while stack:\n        reversed_s += stack.pop()\n\n    return reversed_s\n\n# Test\nprint(reverse_string(\"hello\"))  # \"olleh\"\n\n# Python way\nprint(\"hello\"[::-1])  # \"olleh\"\n</code></pre>"},{"location":"DSA/03_stacks_and_queues/#3-evaluate-postfix-expression","title":"3. Evaluate Postfix Expression","text":"<pre><code>def evaluate_postfix(expression):\n    \"\"\"\n    Evaluate postfix expression (Reverse Polish Notation)\n    Time Complexity: O(n)\n    Space Complexity: O(n)\n    \"\"\"\n    stack = []\n    operators = {'+', '-', '*', '/'}\n\n    for token in expression.split():\n        if token not in operators:\n            stack.append(int(token))\n        else:\n            b = stack.pop()\n            a = stack.pop()\n\n            if token == '+':\n                result = a + b\n            elif token == '-':\n                result = a - b\n            elif token == '*':\n                result = a * b\n            elif token == '/':\n                result = a // b\n\n            stack.append(result)\n\n    return stack.pop()\n\n# Test\nprint(evaluate_postfix(\"2 3 +\"))        # 5\nprint(evaluate_postfix(\"2 3 + 4 *\"))    # 20\nprint(evaluate_postfix(\"5 1 2 + 4 * + 3 -\"))  # 14\n</code></pre>"},{"location":"DSA/03_stacks_and_queues/#queue","title":"Queue","text":"<p>A Queue is a First-In-First-Out (FIFO) data structure where the first element added is the first one to be removed.</p>"},{"location":"DSA/03_stacks_and_queues/#real-world-analogies_1","title":"Real-World Analogies:","text":"<ul> <li>Line at a ticket counter</li> <li>Print job queue</li> <li>Task scheduling in operating systems</li> <li>Breadth-First Search in graphs</li> </ul>"},{"location":"DSA/03_stacks_and_queues/#queue-operations","title":"Queue Operations","text":"Operation Description Time Complexity enqueue(item) Add item to rear O(1) dequeue() Remove and return front item O(1) front() View front item O(1) isEmpty() Check if queue is empty O(1) size() Get number of elements O(1)"},{"location":"DSA/03_stacks_and_queues/#implementing-queue-in-python","title":"Implementing Queue in Python","text":"<pre><code>from collections import deque\n\nclass Queue:\n    def __init__(self):\n        self.items = deque()\n\n    def enqueue(self, item):\n        \"\"\"Add item to rear of queue\"\"\"\n        self.items.append(item)\n\n    def dequeue(self):\n        \"\"\"Remove and return front item\"\"\"\n        if not self.is_empty():\n            return self.items.popleft()\n        return None\n\n    def front(self):\n        \"\"\"View front item without removing\"\"\"\n        if not self.is_empty():\n            return self.items[0]\n        return None\n\n    def is_empty(self):\n        \"\"\"Check if queue is empty\"\"\"\n        return len(self.items) == 0\n\n    def size(self):\n        \"\"\"Return number of items\"\"\"\n        return len(self.items)\n\n# Usage\nqueue = Queue()\nqueue.enqueue(1)\nqueue.enqueue(2)\nqueue.enqueue(3)\nprint(queue.dequeue())  # 1\nprint(queue.front())    # 2\nprint(queue.size())     # 2\n</code></pre>"},{"location":"DSA/03_stacks_and_queues/#simple-queue-using-deque","title":"Simple Queue using deque","text":"<pre><code>from collections import deque\n\n# Queue using deque\nqueue = deque()\n\n# Enqueue\nqueue.append(1)\nqueue.append(2)\nqueue.append(3)\n\n# Dequeue\nfront = queue.popleft()  # 1\n\n# Front\nfront = queue[0] if queue else None  # 2\n\n# Check empty\nis_empty = len(queue) == 0\n</code></pre>"},{"location":"DSA/03_stacks_and_queues/#queue-applications","title":"Queue Applications","text":""},{"location":"DSA/03_stacks_and_queues/#1-hot-potato-simulation","title":"1. Hot Potato Simulation","text":"<pre><code>def hot_potato(names, num):\n    \"\"\"\n    Simulate hot potato game\n    Time Complexity: O(n * num)\n    Space Complexity: O(n)\n    \"\"\"\n    from collections import deque\n\n    queue = deque(names)\n\n    while len(queue) &gt; 1:\n        # Pass the potato num times\n        for _ in range(num):\n            queue.append(queue.popleft())\n\n        # Remove the person holding potato\n        queue.popleft()\n\n    return queue[0]\n\n# Test\nplayers = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"]\nprint(hot_potato(players, 3))  # Winner\n</code></pre>"},{"location":"DSA/03_stacks_and_queues/#2-level-order-traversal-bfs","title":"2. Level Order Traversal (BFS)","text":"<pre><code>def level_order_traversal(root):\n    \"\"\"\n    Print tree nodes level by level\n    Time Complexity: O(n)\n    Space Complexity: O(n)\n    \"\"\"\n    if not root:\n        return\n\n    from collections import deque\n    queue = deque([root])\n    result = []\n\n    while queue:\n        level_size = len(queue)\n        level = []\n\n        for _ in range(level_size):\n            node = queue.popleft()\n            level.append(node.value)\n\n            if node.left:\n                queue.append(node.left)\n            if node.right:\n                queue.append(node.right)\n\n        result.append(level)\n\n    return result\n</code></pre>"},{"location":"DSA/03_stacks_and_queues/#deque-double-ended-queue","title":"Deque (Double-Ended Queue)","text":"<p>A Deque allows insertion and deletion at both ends.</p> <pre><code>from collections import deque\n\n# Create deque\ndq = deque([1, 2, 3])\n\n# Add to right\ndq.append(4)        # [1, 2, 3, 4]\n\n# Add to left\ndq.appendleft(0)    # [0, 1, 2, 3, 4]\n\n# Remove from right\ndq.pop()            # 4\n\n# Remove from left\ndq.popleft()        # 0\n\n# Result: [1, 2, 3]\n</code></pre>"},{"location":"DSA/03_stacks_and_queues/#priority-queue","title":"Priority Queue","text":"<p>A Priority Queue where elements are served based on priority, not insertion order.</p> <pre><code>import heapq\n\nclass PriorityQueue:\n    def __init__(self):\n        self.heap = []\n\n    def push(self, item, priority):\n        \"\"\"Add item with priority (lower number = higher priority)\"\"\"\n        heapq.heappush(self.heap, (priority, item))\n\n    def pop(self):\n        \"\"\"Remove and return highest priority item\"\"\"\n        if self.heap:\n            return heapq.heappop(self.heap)[1]\n        return None\n\n    def is_empty(self):\n        return len(self.heap) == 0\n\n# Usage\npq = PriorityQueue()\npq.push(\"Task 1\", 3)\npq.push(\"Task 2\", 1)\npq.push(\"Task 3\", 2)\n\nprint(pq.pop())  # Task 2 (priority 1)\nprint(pq.pop())  # Task 3 (priority 2)\nprint(pq.pop())  # Task 1 (priority 3)\n</code></pre>"},{"location":"DSA/03_stacks_and_queues/#stack-vs-queue-comparison","title":"Stack vs Queue Comparison","text":"Feature Stack Queue Order LIFO (Last In First Out) FIFO (First In First Out) Primary Ops push(), pop() enqueue(), dequeue() Use Cases Undo/Redo, Recursion, DFS Task scheduling, BFS Python list with append/pop deque with append/popleft"},{"location":"DSA/03_stacks_and_queues/#common-interview-problems","title":"Common Interview Problems","text":""},{"location":"DSA/03_stacks_and_queues/#next-greater-element","title":"Next Greater Element","text":"<pre><code>def next_greater_element(arr):\n    \"\"\"\n    Find next greater element for each element\n    Time Complexity: O(n)\n    Space Complexity: O(n)\n    \"\"\"\n    stack = []\n    result = [-1] * len(arr)\n\n    for i in range(len(arr)):\n        while stack and arr[stack[-1]] &lt; arr[i]:\n            index = stack.pop()\n            result[index] = arr[i]\n        stack.append(i)\n\n    return result\n\n# Test\nnums = [4, 5, 2, 10, 8]\nprint(next_greater_element(nums))  # [5, 10, 10, -1, -1]\n</code></pre>"},{"location":"DSA/03_stacks_and_queues/#implement-queue-using-stacks","title":"Implement Queue using Stacks","text":"<pre><code>class QueueUsingStacks:\n    def __init__(self):\n        self.stack1 = []  # For enqueue\n        self.stack2 = []  # For dequeue\n\n    def enqueue(self, item):\n        \"\"\"O(1)\"\"\"\n        self.stack1.append(item)\n\n    def dequeue(self):\n        \"\"\"Amortized O(1)\"\"\"\n        if not self.stack2:\n            while self.stack1:\n                self.stack2.append(self.stack1.pop())\n\n        return self.stack2.pop() if self.stack2 else None\n\n# Test\nq = QueueUsingStacks()\nq.enqueue(1)\nq.enqueue(2)\nq.enqueue(3)\nprint(q.dequeue())  # 1\nprint(q.dequeue())  # 2\n</code></pre> <p>Practice Problems</p> <p>Try solving these problems: - Min Stack (get minimum in O(1)) - Valid Parentheses - Implement Stack using Queues - Sliding Window Maximum - Largest Rectangle in Histogram</p> <p>When to Use</p> <ul> <li>Stack: When you need LIFO behavior (undo operations, parsing, DFS)</li> <li>Queue: When you need FIFO behavior (task scheduling, BFS, buffering)</li> <li>Deque: When you need operations at both ends</li> <li>Priority Queue: When elements have different priorities</li> </ul> <p>Next Tutorial: Linked Lists</p>"},{"location":"DSA/04_linked_lists/","title":"Linked Lists","text":"<p>A Linked List is a linear data structure where elements are stored in nodes, and each node points to the next node in the sequence. Unlike arrays, linked lists don't require contiguous memory.</p>"},{"location":"DSA/04_linked_lists/#what-is-a-linked-list","title":"What is a Linked List?","text":""},{"location":"DSA/04_linked_lists/#structure","title":"Structure:","text":"<ul> <li>Node: Contains data and reference(s) to other node(s)</li> <li>Head: First node in the list</li> <li>Tail: Last node (points to None)</li> </ul> <pre><code>[Data|Next] -&gt; [Data|Next] -&gt; [Data|Next] -&gt; None\n   HEAD                          TAIL\n</code></pre>"},{"location":"DSA/04_linked_lists/#key-characteristics","title":"Key Characteristics:","text":"<ul> <li>Dynamic Size - Can grow/shrink at runtime</li> <li>No Random Access - Must traverse from head</li> <li>Efficient Insertion/Deletion - No shifting required</li> <li>Extra Memory - For storing pointers</li> </ul>"},{"location":"DSA/04_linked_lists/#types-of-linked-lists","title":"Types of Linked Lists","text":""},{"location":"DSA/04_linked_lists/#1-singly-linked-list","title":"1. Singly Linked List","text":"<p>Each node points to the next node.</p> <pre><code>1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; None\n</code></pre>"},{"location":"DSA/04_linked_lists/#2-doubly-linked-list","title":"2. Doubly Linked List","text":"<p>Each node points to both next and previous nodes.</p> <pre><code>None &lt;- 1 &lt;-&gt; 2 &lt;-&gt; 3 &lt;-&gt; 4 -&gt; None\n</code></pre>"},{"location":"DSA/04_linked_lists/#3-circular-linked-list","title":"3. Circular Linked List","text":"<p>Last node points back to the first node.</p> <pre><code>1 -&gt; 2 -&gt; 3 -&gt; 4 -|\n^__________________|\n</code></pre>"},{"location":"DSA/04_linked_lists/#singly-linked-list-implementation","title":"Singly Linked List Implementation","text":""},{"location":"DSA/04_linked_lists/#node-class","title":"Node Class","text":"<pre><code>class Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n</code></pre>"},{"location":"DSA/04_linked_lists/#linkedlist-class","title":"LinkedList Class","text":"<pre><code>class LinkedList:\n    def __init__(self):\n        self.head = None\n\n    def is_empty(self):\n        \"\"\"Check if list is empty\"\"\"\n        return self.head is None\n\n    def insert_at_beginning(self, data):\n        \"\"\"\n        Insert node at the beginning\n        Time Complexity: O(1)\n        \"\"\"\n        new_node = Node(data)\n        new_node.next = self.head\n        self.head = new_node\n\n    def insert_at_end(self, data):\n        \"\"\"\n        Insert node at the end\n        Time Complexity: O(n)\n        \"\"\"\n        new_node = Node(data)\n\n        if self.is_empty():\n            self.head = new_node\n            return\n\n        current = self.head\n        while current.next:\n            current = current.next\n\n        current.next = new_node\n\n    def insert_at_position(self, data, position):\n        \"\"\"\n        Insert node at specific position\n        Time Complexity: O(n)\n        \"\"\"\n        if position == 0:\n            self.insert_at_beginning(data)\n            return\n\n        new_node = Node(data)\n        current = self.head\n        count = 0\n\n        while current and count &lt; position - 1:\n            current = current.next\n            count += 1\n\n        if current:\n            new_node.next = current.next\n            current.next = new_node\n\n    def delete_at_beginning(self):\n        \"\"\"\n        Delete first node\n        Time Complexity: O(1)\n        \"\"\"\n        if self.is_empty():\n            return None\n\n        deleted_data = self.head.data\n        self.head = self.head.next\n        return deleted_data\n\n    def delete_at_end(self):\n        \"\"\"\n        Delete last node\n        Time Complexity: O(n)\n        \"\"\"\n        if self.is_empty():\n            return None\n\n        if self.head.next is None:\n            deleted_data = self.head.data\n            self.head = None\n            return deleted_data\n\n        current = self.head\n        while current.next.next:\n            current = current.next\n\n        deleted_data = current.next.data\n        current.next = None\n        return deleted_data\n\n    def delete_by_value(self, value):\n        \"\"\"\n        Delete first occurrence of value\n        Time Complexity: O(n)\n        \"\"\"\n        if self.is_empty():\n            return False\n\n        if self.head.data == value:\n            self.head = self.head.next\n            return True\n\n        current = self.head\n        while current.next:\n            if current.next.data == value:\n                current.next = current.next.next\n                return True\n            current = current.next\n\n        return False\n\n    def search(self, value):\n        \"\"\"\n        Search for a value\n        Time Complexity: O(n)\n        \"\"\"\n        current = self.head\n        position = 0\n\n        while current:\n            if current.data == value:\n                return position\n            current = current.next\n            position += 1\n\n        return -1\n\n    def display(self):\n        \"\"\"Print all elements\"\"\"\n        elements = []\n        current = self.head\n\n        while current:\n            elements.append(str(current.data))\n            current = current.next\n\n        print(\" -&gt; \".join(elements) + \" -&gt; None\")\n\n    def get_length(self):\n        \"\"\"\n        Get number of nodes\n        Time Complexity: O(n)\n        \"\"\"\n        count = 0\n        current = self.head\n\n        while current:\n            count += 1\n            current = current.next\n\n        return count\n\n# Usage Example\nll = LinkedList()\nll.insert_at_end(1)\nll.insert_at_end(2)\nll.insert_at_end(3)\nll.insert_at_beginning(0)\nll.display()  # 0 -&gt; 1 -&gt; 2 -&gt; 3 -&gt; None\n\nprint(f\"Length: {ll.get_length()}\")  # 4\nprint(f\"Search 2: {ll.search(2)}\")   # 2\n\nll.delete_by_value(2)\nll.display()  # 0 -&gt; 1 -&gt; 3 -&gt; None\n</code></pre>"},{"location":"DSA/04_linked_lists/#time-complexity-comparison","title":"Time Complexity Comparison","text":"Operation Array Linked List Access by index O(1) O(n) Search O(n) O(n) Insert at beginning O(n) O(1) Insert at end O(1) O(n) Insert at position O(n) O(n) Delete at beginning O(n) O(1) Delete at end O(1) O(n)"},{"location":"DSA/04_linked_lists/#doubly-linked-list","title":"Doubly Linked List","text":"<pre><code>class DNode:\n    def __init__(self, data):\n        self.data = data\n        self.prev = None\n        self.next = None\n\nclass DoublyLinkedList:\n    def __init__(self):\n        self.head = None\n\n    def insert_at_beginning(self, data):\n        \"\"\"\n        Insert at beginning\n        Time Complexity: O(1)\n        \"\"\"\n        new_node = DNode(data)\n\n        if self.head:\n            new_node.next = self.head\n            self.head.prev = new_node\n\n        self.head = new_node\n\n    def insert_at_end(self, data):\n        \"\"\"\n        Insert at end\n        Time Complexity: O(n)\n        \"\"\"\n        new_node = DNode(data)\n\n        if not self.head:\n            self.head = new_node\n            return\n\n        current = self.head\n        while current.next:\n            current = current.next\n\n        current.next = new_node\n        new_node.prev = current\n\n    def display_forward(self):\n        \"\"\"Display from head to tail\"\"\"\n        current = self.head\n        elements = []\n\n        while current:\n            elements.append(str(current.data))\n            current = current.next\n\n        print(\" &lt;-&gt; \".join(elements))\n\n    def display_backward(self):\n        \"\"\"Display from tail to head\"\"\"\n        if not self.head:\n            return\n\n        current = self.head\n        while current.next:\n            current = current.next\n\n        elements = []\n        while current:\n            elements.append(str(current.data))\n            current = current.prev\n\n        print(\" &lt;-&gt; \".join(elements))\n\n# Usage\ndll = DoublyLinkedList()\ndll.insert_at_end(1)\ndll.insert_at_end(2)\ndll.insert_at_end(3)\ndll.display_forward()   # 1 &lt;-&gt; 2 &lt;-&gt; 3\ndll.display_backward()  # 3 &lt;-&gt; 2 &lt;-&gt; 1\n</code></pre>"},{"location":"DSA/04_linked_lists/#common-linked-list-problems","title":"Common Linked List Problems","text":""},{"location":"DSA/04_linked_lists/#1-reverse-a-linked-list","title":"1. Reverse a Linked List","text":"<pre><code>def reverse_linked_list(head):\n    \"\"\"\n    Reverse a singly linked list\n    Time Complexity: O(n)\n    Space Complexity: O(1)\n    \"\"\"\n    prev = None\n    current = head\n\n    while current:\n        next_node = current.next\n        current.next = prev\n        prev = current\n        current = next_node\n\n    return prev\n\n# Method for LinkedList class\ndef reverse(self):\n    \"\"\"Reverse the linked list in place\"\"\"\n    self.head = reverse_linked_list(self.head)\n</code></pre>"},{"location":"DSA/04_linked_lists/#2-detect-cycle-floyds-algorithm","title":"2. Detect Cycle (Floyd's Algorithm)","text":"<pre><code>def has_cycle(head):\n    \"\"\"\n    Detect if linked list has a cycle\n    Time Complexity: O(n)\n    Space Complexity: O(1)\n    \"\"\"\n    if not head or not head.next:\n        return False\n\n    slow = head\n    fast = head\n\n    while fast and fast.next:\n        slow = slow.next\n        fast = fast.next.next\n\n        if slow == fast:\n            return True\n\n    return False\n</code></pre>"},{"location":"DSA/04_linked_lists/#3-find-middle-element","title":"3. Find Middle Element","text":"<pre><code>def find_middle(head):\n    \"\"\"\n    Find middle node using slow/fast pointers\n    Time Complexity: O(n)\n    Space Complexity: O(1)\n    \"\"\"\n    if not head:\n        return None\n\n    slow = head\n    fast = head\n\n    while fast and fast.next:\n        slow = slow.next\n        fast = fast.next.next\n\n    return slow\n</code></pre>"},{"location":"DSA/04_linked_lists/#4-merge-two-sorted-lists","title":"4. Merge Two Sorted Lists","text":"<pre><code>def merge_sorted_lists(l1, l2):\n    \"\"\"\n    Merge two sorted linked lists\n    Time Complexity: O(n + m)\n    Space Complexity: O(1)\n    \"\"\"\n    dummy = Node(0)\n    current = dummy\n\n    while l1 and l2:\n        if l1.data &lt;= l2.data:\n            current.next = l1\n            l1 = l1.next\n        else:\n            current.next = l2\n            l2 = l2.next\n        current = current.next\n\n    current.next = l1 if l1 else l2\n\n    return dummy.next\n</code></pre>"},{"location":"DSA/04_linked_lists/#5-remove-nth-node-from-end","title":"5. Remove Nth Node from End","text":"<pre><code>def remove_nth_from_end(head, n):\n    \"\"\"\n    Remove nth node from end of list\n    Time Complexity: O(n)\n    Space Complexity: O(1)\n    \"\"\"\n    dummy = Node(0)\n    dummy.next = head\n    fast = slow = dummy\n\n    # Move fast n steps ahead\n    for _ in range(n):\n        fast = fast.next\n\n    # Move both until fast reaches end\n    while fast.next:\n        fast = fast.next\n        slow = slow.next\n\n    # Remove nth node\n    slow.next = slow.next.next\n\n    return dummy.next\n</code></pre>"},{"location":"DSA/04_linked_lists/#6-check-if-palindrome","title":"6. Check if Palindrome","text":"<pre><code>def is_palindrome(head):\n    \"\"\"\n    Check if linked list is palindrome\n    Time Complexity: O(n)\n    Space Complexity: O(1)\n    \"\"\"\n    if not head or not head.next:\n        return True\n\n    # Find middle\n    slow = fast = head\n    while fast and fast.next:\n        slow = slow.next\n        fast = fast.next.next\n\n    # Reverse second half\n    prev = None\n    while slow:\n        next_node = slow.next\n        slow.next = prev\n        prev = slow\n        slow = next_node\n\n    # Compare both halves\n    left, right = head, prev\n    while right:\n        if left.data != right.data:\n            return False\n        left = left.next\n        right = right.next\n\n    return True\n</code></pre>"},{"location":"DSA/04_linked_lists/#7-remove-duplicates-from-sorted-list","title":"7. Remove Duplicates from Sorted List","text":"<pre><code>def remove_duplicates(head):\n    \"\"\"\n    Remove duplicates from sorted linked list\n    Time Complexity: O(n)\n    Space Complexity: O(1)\n    \"\"\"\n    current = head\n\n    while current and current.next:\n        if current.data == current.next.data:\n            current.next = current.next.next\n        else:\n            current = current.next\n\n    return head\n</code></pre>"},{"location":"DSA/04_linked_lists/#advantages-of-linked-lists","title":"Advantages of Linked Lists","text":"<ol> <li>Dynamic Size - Can grow/shrink at runtime</li> <li>Efficient Insertion/Deletion - O(1) at beginning</li> <li>No Memory Waste - Allocate only what's needed</li> <li>Easy Implementation - Of stacks and queues</li> </ol>"},{"location":"DSA/04_linked_lists/#disadvantages-of-linked-lists","title":"Disadvantages of Linked Lists","text":"<ol> <li>No Random Access - Must traverse sequentially</li> <li>Extra Memory - For storing pointers</li> <li>Not Cache Friendly - Non-contiguous memory</li> <li>Reverse Traversal - Difficult in singly linked lists</li> </ol>"},{"location":"DSA/04_linked_lists/#when-to-use-linked-lists","title":"When to Use Linked Lists","text":"<p>Use Linked Lists when: - Frequent insertions/deletions at beginning - Unknown size of data - Don't need random access - Implementing stacks, queues, or graphs</p> <p>Use Arrays when: - Need random access - Mostly reading data - Size is known or stable - Memory locality is important</p> <p>Practice Problems</p> <p>Try solving these classic problems: - Add two numbers represented as linked lists - Find intersection point of two lists - Rotate linked list - Clone linked list with random pointer - Sort linked list (Merge Sort)</p> <p>Implementation Note</p> <p>Python doesn't have built-in linked lists. For production code, consider using: - <code>collections.deque</code> for double-ended operations - <code>list</code> for most general-purpose needs - Only implement custom linked lists when specifically needed</p> <p>Next Tutorial: Trees and Binary Search Trees</p>"},{"location":"DSA/05_trees/","title":"Trees and Binary Search Trees","text":"<p>A Tree is a hierarchical data structure consisting of nodes connected by edges. Unlike linear data structures, trees represent hierarchical relationships.</p>"},{"location":"DSA/05_trees/#tree-terminology","title":"Tree Terminology","text":"<pre><code>         1         &lt;- Root\n       /   \\\n      2     3      &lt;- Children of 1 (Siblings)\n     / \\     \\\n    4   5     6    &lt;- Leaf nodes (4, 5, 6)\n</code></pre> <ul> <li>Root: Top node (1)</li> <li>Parent: Node with children (1, 2, 3)</li> <li>Child: Node descended from another (2, 3 are children of 1)</li> <li>Leaf: Node with no children (4, 5, 6)</li> <li>Sibling: Nodes with same parent (2 and 3)</li> <li>Edge: Connection between nodes</li> <li>Path: Sequence of nodes and edges</li> <li>Height: Longest path from root to leaf</li> <li>Depth: Distance from root to node</li> <li>Level: All nodes at same depth</li> </ul>"},{"location":"DSA/05_trees/#types-of-trees","title":"Types of Trees","text":""},{"location":"DSA/05_trees/#1-binary-tree","title":"1. Binary Tree","text":"<p>Each node has at most 2 children (left and right).</p>"},{"location":"DSA/05_trees/#2-binary-search-tree-bst","title":"2. Binary Search Tree (BST)","text":"<p>Binary tree where left child &lt; parent &lt; right child.</p>"},{"location":"DSA/05_trees/#3-avl-tree","title":"3. AVL Tree","text":"<p>Self-balancing BST.</p>"},{"location":"DSA/05_trees/#4-red-black-tree","title":"4. Red-Black Tree","text":"<p>Self-balancing BST with color properties.</p>"},{"location":"DSA/05_trees/#5-b-tree","title":"5. B-Tree","text":"<p>Self-balancing tree for databases.</p>"},{"location":"DSA/05_trees/#6-heap","title":"6. Heap","text":"<p>Complete binary tree with heap property.</p>"},{"location":"DSA/05_trees/#binary-tree-implementation","title":"Binary Tree Implementation","text":"<pre><code>class TreeNode:\n    def __init__(self, data):\n        self.data = data\n        self.left = None\n        self.right = None\n\nclass BinaryTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, data):\n        \"\"\"Insert node (level-order insertion)\"\"\"\n        new_node = TreeNode(data)\n\n        if not self.root:\n            self.root = new_node\n            return\n\n        from collections import deque\n        queue = deque([self.root])\n\n        while queue:\n            node = queue.popleft()\n\n            if not node.left:\n                node.left = new_node\n                return\n            else:\n                queue.append(node.left)\n\n            if not node.right:\n                node.right = new_node\n                return\n            else:\n                queue.append(node.right)\n</code></pre>"},{"location":"DSA/05_trees/#binary-search-tree-bst","title":"Binary Search Tree (BST)","text":"<p>A BST maintains the property: left child &lt; parent &lt; right child</p> <pre><code>class BSTNode:\n    def __init__(self, data):\n        self.data = data\n        self.left = None\n        self.right = None\n\nclass BinarySearchTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, data):\n        \"\"\"\n        Insert value into BST\n        Time Complexity: O(log n) average, O(n) worst\n        \"\"\"\n        self.root = self._insert_recursive(self.root, data)\n\n    def _insert_recursive(self, node, data):\n        if node is None:\n            return BSTNode(data)\n\n        if data &lt; node.data:\n            node.left = self._insert_recursive(node.left, data)\n        elif data &gt; node.data:\n            node.right = self._insert_recursive(node.right, data)\n\n        return node\n\n    def search(self, data):\n        \"\"\"\n        Search for value in BST\n        Time Complexity: O(log n) average, O(n) worst\n        \"\"\"\n        return self._search_recursive(self.root, data)\n\n    def _search_recursive(self, node, data):\n        if node is None or node.data == data:\n            return node\n\n        if data &lt; node.data:\n            return self._search_recursive(node.left, data)\n\n        return self._search_recursive(node.right, data)\n\n    def delete(self, data):\n        \"\"\"\n        Delete value from BST\n        Time Complexity: O(log n) average, O(n) worst\n        \"\"\"\n        self.root = self._delete_recursive(self.root, data)\n\n    def _delete_recursive(self, node, data):\n        if node is None:\n            return node\n\n        if data &lt; node.data:\n            node.left = self._delete_recursive(node.left, data)\n        elif data &gt; node.data:\n            node.right = self._delete_recursive(node.right, data)\n        else:\n            # Node with one or no child\n            if node.left is None:\n                return node.right\n            elif node.right is None:\n                return node.left\n\n            # Node with two children\n            min_node = self._find_min(node.right)\n            node.data = min_node.data\n            node.right = self._delete_recursive(node.right, min_node.data)\n\n        return node\n\n    def _find_min(self, node):\n        \"\"\"Find minimum value node\"\"\"\n        current = node\n        while current.left:\n            current = current.left\n        return current\n\n    def find_min(self):\n        \"\"\"Find minimum value in tree\"\"\"\n        if not self.root:\n            return None\n        node = self._find_min(self.root)\n        return node.data\n\n    def find_max(self):\n        \"\"\"Find maximum value in tree\"\"\"\n        if not self.root:\n            return None\n        current = self.root\n        while current.right:\n            current = current.right\n        return current.data\n\n# Usage\nbst = BinarySearchTree()\nvalues = [50, 30, 70, 20, 40, 60, 80]\n\nfor val in values:\n    bst.insert(val)\n\nprint(bst.search(40))  # Returns node with data 40\nprint(bst.find_min())  # 20\nprint(bst.find_max())  # 80\n</code></pre>"},{"location":"DSA/05_trees/#tree-traversals","title":"Tree Traversals","text":""},{"location":"DSA/05_trees/#1-inorder-traversal-left-root-right","title":"1. Inorder Traversal (Left, Root, Right)","text":"<p>For BST, gives sorted order.</p> <pre><code>def inorder_traversal(root):\n    \"\"\"\n    Left -&gt; Root -&gt; Right\n    Time Complexity: O(n)\n    \"\"\"\n    result = []\n\n    def traverse(node):\n        if node:\n            traverse(node.left)\n            result.append(node.data)\n            traverse(node.right)\n\n    traverse(root)\n    return result\n\n# For BST with values [50, 30, 70, 20, 40, 60, 80]\n# Output: [20, 30, 40, 50, 60, 70, 80]\n</code></pre>"},{"location":"DSA/05_trees/#2-preorder-traversal-root-left-right","title":"2. Preorder Traversal (Root, Left, Right)","text":"<p>Used for creating copy of tree.</p> <pre><code>def preorder_traversal(root):\n    \"\"\"\n    Root -&gt; Left -&gt; Right\n    Time Complexity: O(n)\n    \"\"\"\n    result = []\n\n    def traverse(node):\n        if node:\n            result.append(node.data)\n            traverse(node.left)\n            traverse(node.right)\n\n    traverse(root)\n    return result\n\n# Output: [50, 30, 20, 40, 70, 60, 80]\n</code></pre>"},{"location":"DSA/05_trees/#3-postorder-traversal-left-right-root","title":"3. Postorder Traversal (Left, Right, Root)","text":"<p>Used for deleting tree.</p> <pre><code>def postorder_traversal(root):\n    \"\"\"\n    Left -&gt; Right -&gt; Root\n    Time Complexity: O(n)\n    \"\"\"\n    result = []\n\n    def traverse(node):\n        if node:\n            traverse(node.left)\n            traverse(node.right)\n            result.append(node.data)\n\n    traverse(root)\n    return result\n\n# Output: [20, 40, 30, 60, 80, 70, 50]\n</code></pre>"},{"location":"DSA/05_trees/#4-level-order-traversal-bfs","title":"4. Level Order Traversal (BFS)","text":"<p>Visit nodes level by level.</p> <pre><code>def level_order_traversal(root):\n    \"\"\"\n    Level by level (BFS)\n    Time Complexity: O(n)\n    \"\"\"\n    if not root:\n        return []\n\n    from collections import deque\n    queue = deque([root])\n    result = []\n\n    while queue:\n        node = queue.popleft()\n        result.append(node.data)\n\n        if node.left:\n            queue.append(node.left)\n        if node.right:\n            queue.append(node.right)\n\n    return result\n\n# Output: [50, 30, 70, 20, 40, 60, 80]\n</code></pre>"},{"location":"DSA/05_trees/#tree-properties","title":"Tree Properties","text":""},{"location":"DSA/05_trees/#height-of-tree","title":"Height of Tree","text":"<pre><code>def height(root):\n    \"\"\"\n    Calculate height of tree\n    Time Complexity: O(n)\n    \"\"\"\n    if not root:\n        return -1  # or 0, depending on definition\n\n    return 1 + max(height(root.left), height(root.right))\n</code></pre>"},{"location":"DSA/05_trees/#count-nodes","title":"Count Nodes","text":"<pre><code>def count_nodes(root):\n    \"\"\"\n    Count total nodes\n    Time Complexity: O(n)\n    \"\"\"\n    if not root:\n        return 0\n\n    return 1 + count_nodes(root.left) + count_nodes(root.right)\n</code></pre>"},{"location":"DSA/05_trees/#check-if-balanced","title":"Check if Balanced","text":"<pre><code>def is_balanced(root):\n    \"\"\"\n    Check if tree is height-balanced\n    Time Complexity: O(n)\n    \"\"\"\n    def check_height(node):\n        if not node:\n            return 0\n\n        left_height = check_height(node.left)\n        if left_height == -1:\n            return -1\n\n        right_height = check_height(node.right)\n        if right_height == -1:\n            return -1\n\n        if abs(left_height - right_height) &gt; 1:\n            return -1\n\n        return 1 + max(left_height, right_height)\n\n    return check_height(root) != -1\n</code></pre>"},{"location":"DSA/05_trees/#common-tree-problems","title":"Common Tree Problems","text":""},{"location":"DSA/05_trees/#1-validate-bst","title":"1. Validate BST","text":"<pre><code>def is_valid_bst(root):\n    \"\"\"\n    Check if tree is valid BST\n    Time Complexity: O(n)\n    \"\"\"\n    def validate(node, min_val, max_val):\n        if not node:\n            return True\n\n        if node.data &lt;= min_val or node.data &gt;= max_val:\n            return False\n\n        return (validate(node.left, min_val, node.data) and\n                validate(node.right, node.data, max_val))\n\n    return validate(root, float('-inf'), float('inf'))\n</code></pre>"},{"location":"DSA/05_trees/#2-lowest-common-ancestor-bst","title":"2. Lowest Common Ancestor (BST)","text":"<pre><code>def lowest_common_ancestor(root, p, q):\n    \"\"\"\n    Find LCA in BST\n    Time Complexity: O(log n) average\n    \"\"\"\n    if not root:\n        return None\n\n    if p.data &lt; root.data and q.data &lt; root.data:\n        return lowest_common_ancestor(root.left, p, q)\n    elif p.data &gt; root.data and q.data &gt; root.data:\n        return lowest_common_ancestor(root.right, p, q)\n    else:\n        return root\n</code></pre>"},{"location":"DSA/05_trees/#3-maximum-path-sum","title":"3. Maximum Path Sum","text":"<pre><code>def max_path_sum(root):\n    \"\"\"\n    Find maximum path sum\n    Time Complexity: O(n)\n    \"\"\"\n    max_sum = float('-inf')\n\n    def helper(node):\n        nonlocal max_sum\n\n        if not node:\n            return 0\n\n        left = max(0, helper(node.left))\n        right = max(0, helper(node.right))\n\n        max_sum = max(max_sum, left + right + node.data)\n\n        return max(left, right) + node.data\n\n    helper(root)\n    return max_sum\n</code></pre>"},{"location":"DSA/05_trees/#4-serialize-and-deserialize","title":"4. Serialize and Deserialize","text":"<pre><code>def serialize(root):\n    \"\"\"\n    Serialize tree to string\n    Time Complexity: O(n)\n    \"\"\"\n    def helper(node):\n        if not node:\n            return \"None,\"\n\n        return str(node.data) + \",\" + helper(node.left) + helper(node.right)\n\n    return helper(root)\n\ndef deserialize(data):\n    \"\"\"\n    Deserialize string to tree\n    Time Complexity: O(n)\n    \"\"\"\n    def helper():\n        val = next(values)\n\n        if val == \"None\":\n            return None\n\n        node = TreeNode(int(val))\n        node.left = helper()\n        node.right = helper()\n\n        return node\n\n    values = iter(data.split(','))\n    return helper()\n</code></pre>"},{"location":"DSA/05_trees/#5-mirror-tree","title":"5. Mirror Tree","text":"<pre><code>def mirror_tree(root):\n    \"\"\"\n    Create mirror of tree\n    Time Complexity: O(n)\n    \"\"\"\n    if not root:\n        return None\n\n    # Swap left and right\n    root.left, root.right = root.right, root.left\n\n    # Recursively mirror subtrees\n    mirror_tree(root.left)\n    mirror_tree(root.right)\n\n    return root\n</code></pre>"},{"location":"DSA/05_trees/#bst-operations-complexity","title":"BST Operations Complexity","text":"Operation Average Worst Case Search O(log n) O(n) Insert O(log n) O(n) Delete O(log n) O(n) Find Min/Max O(log n) O(n) <p>Worst case occurs with unbalanced tree (like linked list)</p>"},{"location":"DSA/05_trees/#balanced-trees","title":"Balanced Trees","text":"<p>To ensure O(log n) operations, use self-balancing trees:</p> <ul> <li>AVL Tree: Strictly balanced, faster lookups</li> <li>Red-Black Tree: Less strict, faster insertions</li> <li>B-Tree: Multi-way tree for databases</li> </ul>"},{"location":"DSA/05_trees/#applications-of-trees","title":"Applications of Trees","text":"<ol> <li>File Systems - Directory structure</li> <li>Databases - Indexing (B-trees)</li> <li>Compilers - Abstract Syntax Trees</li> <li>Routing - Decision trees</li> <li>HTML DOM - Document structure</li> <li>AI - Game trees, decision trees</li> <li>Networking - Routing tables</li> </ol> <p>Practice Problems</p> <p>Try solving these tree problems: - Diameter of binary tree - Vertical order traversal - Right view of binary tree - Construct tree from inorder and preorder - Binary tree to doubly linked list</p> <p>When to Use BST</p> <p>Use BST when you need: - Dynamic sorted data - Fast search, insert, delete (O(log n)) - Range queries - Finding min/max efficiently</p> <p>Next Tutorial: Hash Tables</p>"},{"location":"DSA/06_hash_tables/","title":"Hash Tables","text":"<p>A Hash Table (or Hash Map) is a data structure that stores key-value pairs and provides fast lookup, insertion, and deletion operations using a hash function.</p>"},{"location":"DSA/06_hash_tables/#what-is-a-hash-table","title":"What is a Hash Table?","text":"<p>Hash tables use a hash function to compute an index into an array of buckets, from which the desired value can be found.</p>"},{"location":"DSA/06_hash_tables/#key-concepts","title":"Key Concepts:","text":"<ul> <li>Hash Function: Converts key to array index</li> <li>Bucket: Storage location for key-value pair</li> <li>Collision: When two keys hash to same index</li> <li>Load Factor: Ratio of stored elements to table size</li> </ul> <pre><code>Key -&gt; Hash Function -&gt; Index -&gt; Value\n\n\"apple\" -&gt; hash(\"apple\") -&gt; 3 -&gt; {\"color\": \"red\", \"taste\": \"sweet\"}\n</code></pre>"},{"location":"DSA/06_hash_tables/#how-hash-tables-work","title":"How Hash Tables Work","text":"<pre><code># Conceptual example\nhash_table = [None] * 10  # Array of 10 buckets\n\ndef hash_function(key):\n    \"\"\"Simple hash function\"\"\"\n    return sum(ord(c) for c in key) % 10\n\n# Insert\nkey = \"apple\"\nindex = hash_function(key)  # e.g., 3\nhash_table[index] = (\"apple\", \"red\")\n\n# Retrieve\nindex = hash_function(\"apple\")\nvalue = hash_table[index]  # (\"apple\", \"red\")\n</code></pre>"},{"location":"DSA/06_hash_tables/#python-dictionary-built-in-hash-table","title":"Python Dictionary (Built-in Hash Table)","text":"<p>Python's <code>dict</code> is an optimized hash table implementation.</p> <pre><code># Create dictionary\nstudent = {\n    \"name\": \"Alice\",\n    \"age\": 20,\n    \"grade\": \"A\"\n}\n\n# Access - O(1)\nprint(student[\"name\"])  # Alice\n\n# Insert/Update - O(1)\nstudent[\"major\"] = \"Computer Science\"\nstudent[\"age\"] = 21\n\n# Delete - O(1)\ndel student[\"grade\"]\n\n# Check key existence - O(1)\nif \"name\" in student:\n    print(\"Name exists\")\n\n# Get with default - O(1)\nmajor = student.get(\"major\", \"Undecided\")\n\n# Iterate\nfor key, value in student.items():\n    print(f\"{key}: {value}\")\n</code></pre>"},{"location":"DSA/06_hash_tables/#hash-table-operations","title":"Hash Table Operations","text":""},{"location":"DSA/06_hash_tables/#common-operations","title":"Common Operations","text":"<pre><code># Create\nhash_map = {}\n\n# Insert/Update\nhash_map[\"key1\"] = \"value1\"\nhash_map[\"key2\"] = \"value2\"\n\n# Get\nvalue = hash_map.get(\"key1\")  # \"value1\"\nvalue = hash_map.get(\"key3\", \"default\")  # \"default\"\n\n# Delete\nhash_map.pop(\"key1\")  # Returns \"value1\"\n# or\ndel hash_map[\"key2\"]\n\n# Check existence\nexists = \"key1\" in hash_map  # False\n\n# Get all keys\nkeys = hash_map.keys()\n\n# Get all values\nvalues = hash_map.values()\n\n# Get all items\nitems = hash_map.items()\n\n# Clear\nhash_map.clear()\n</code></pre>"},{"location":"DSA/06_hash_tables/#time-complexity","title":"Time Complexity","text":"Operation Average Case Worst Case Search O(1) O(n) Insert O(1) O(n) Delete O(1) O(n) Space O(n) O(n) <p>Worst case occurs when all keys hash to same index (many collisions)</p>"},{"location":"DSA/06_hash_tables/#implementing-a-hash-table","title":"Implementing a Hash Table","text":"<pre><code>class HashTable:\n    def __init__(self, size=10):\n        self.size = size\n        self.table = [[] for _ in range(size)]\n        self.count = 0\n\n    def _hash(self, key):\n        \"\"\"Hash function using built-in hash\"\"\"\n        return hash(key) % self.size\n\n    def insert(self, key, value):\n        \"\"\"\n        Insert key-value pair\n        Time Complexity: O(1) average\n        \"\"\"\n        index = self._hash(key)\n        bucket = self.table[index]\n\n        # Update if key exists\n        for i, (k, v) in enumerate(bucket):\n            if k == key:\n                bucket[i] = (key, value)\n                return\n\n        # Insert new key-value pair\n        bucket.append((key, value))\n        self.count += 1\n\n        # Resize if load factor &gt; 0.7\n        if self.count / self.size &gt; 0.7:\n            self._resize()\n\n    def get(self, key):\n        \"\"\"\n        Get value by key\n        Time Complexity: O(1) average\n        \"\"\"\n        index = self._hash(key)\n        bucket = self.table[index]\n\n        for k, v in bucket:\n            if k == key:\n                return v\n\n        raise KeyError(key)\n\n    def delete(self, key):\n        \"\"\"\n        Delete key-value pair\n        Time Complexity: O(1) average\n        \"\"\"\n        index = self._hash(key)\n        bucket = self.table[index]\n\n        for i, (k, v) in enumerate(bucket):\n            if k == key:\n                del bucket[i]\n                self.count -= 1\n                return v\n\n        raise KeyError(key)\n\n    def _resize(self):\n        \"\"\"Resize table when load factor is high\"\"\"\n        old_table = self.table\n        self.size *= 2\n        self.table = [[] for _ in range(self.size)]\n        self.count = 0\n\n        for bucket in old_table:\n            for key, value in bucket:\n                self.insert(key, value)\n\n    def __contains__(self, key):\n        \"\"\"Check if key exists\"\"\"\n        try:\n            self.get(key)\n            return True\n        except KeyError:\n            return False\n\n    def __len__(self):\n        \"\"\"Return number of items\"\"\"\n        return self.count\n\n    def __str__(self):\n        \"\"\"String representation\"\"\"\n        items = []\n        for bucket in self.table:\n            items.extend(bucket)\n        return str(dict(items))\n\n# Usage\nht = HashTable()\nht.insert(\"apple\", 5)\nht.insert(\"banana\", 3)\nht.insert(\"orange\", 7)\n\nprint(ht.get(\"apple\"))  # 5\nprint(\"banana\" in ht)   # True\nht.delete(\"orange\")\nprint(len(ht))          # 2\n</code></pre>"},{"location":"DSA/06_hash_tables/#collision-resolution","title":"Collision Resolution","text":""},{"location":"DSA/06_hash_tables/#1-chaining-separate-chaining","title":"1. Chaining (Separate Chaining)","text":"<p>Each bucket stores a list of key-value pairs.</p> <pre><code># Example: Multiple keys hash to same index\ntable = [\n    [],\n    [(\"key1\", \"val1\"), (\"key5\", \"val5\")],  # Collision\n    [(\"key2\", \"val2\")],\n    []\n]\n</code></pre>"},{"location":"DSA/06_hash_tables/#2-open-addressing","title":"2. Open Addressing","text":"<p>Find next available slot when collision occurs.</p> <pre><code>def linear_probing(self, key):\n    \"\"\"\n    Linear probing: try index, index+1, index+2, ...\n    \"\"\"\n    index = self._hash(key)\n    original_index = index\n\n    while self.table[index] is not None:\n        if self.table[index][0] == key:\n            return index\n        index = (index + 1) % self.size\n\n        # Checked all slots\n        if index == original_index:\n            return None\n\n    return index\n</code></pre>"},{"location":"DSA/06_hash_tables/#common-hash-table-problems","title":"Common Hash Table Problems","text":""},{"location":"DSA/06_hash_tables/#1-two-sum","title":"1. Two Sum","text":"<pre><code>def two_sum(nums, target):\n    \"\"\"\n    Find two numbers that sum to target\n    Time Complexity: O(n)\n    Space Complexity: O(n)\n    \"\"\"\n    seen = {}\n\n    for i, num in enumerate(nums):\n        complement = target - num\n\n        if complement in seen:\n            return [seen[complement], i]\n\n        seen[num] = i\n\n    return None\n\n# Test\nnums = [2, 7, 11, 15]\nprint(two_sum(nums, 9))  # [0, 1]\n</code></pre>"},{"location":"DSA/06_hash_tables/#2-first-non-repeating-character","title":"2. First Non-Repeating Character","text":"<pre><code>def first_non_repeating(s):\n    \"\"\"\n    Find first non-repeating character\n    Time Complexity: O(n)\n    Space Complexity: O(1) - at most 26 letters\n    \"\"\"\n    char_count = {}\n\n    # Count occurrences\n    for char in s:\n        char_count[char] = char_count.get(char, 0) + 1\n\n    # Find first with count 1\n    for char in s:\n        if char_count[char] == 1:\n            return char\n\n    return None\n\n# Test\nprint(first_non_repeating(\"leetcode\"))  # 'l'\nprint(first_non_repeating(\"aabbcc\"))    # None\n</code></pre>"},{"location":"DSA/06_hash_tables/#3-group-anagrams","title":"3. Group Anagrams","text":"<pre><code>def group_anagrams(words):\n    \"\"\"\n    Group anagrams together\n    Time Complexity: O(n * k log k) where k is max word length\n    Space Complexity: O(n * k)\n    \"\"\"\n    from collections import defaultdict\n\n    anagram_groups = defaultdict(list)\n\n    for word in words:\n        # Sort characters as key\n        key = ''.join(sorted(word))\n        anagram_groups[key].append(word)\n\n    return list(anagram_groups.values())\n\n# Test\nwords = [\"eat\", \"tea\", \"tan\", \"ate\", \"nat\", \"bat\"]\nprint(group_anagrams(words))\n# [[\"eat\", \"tea\", \"ate\"], [\"tan\", \"nat\"], [\"bat\"]]\n</code></pre>"},{"location":"DSA/06_hash_tables/#4-longest-consecutive-sequence","title":"4. Longest Consecutive Sequence","text":"<pre><code>def longest_consecutive(nums):\n    \"\"\"\n    Find longest consecutive sequence\n    Time Complexity: O(n)\n    Space Complexity: O(n)\n    \"\"\"\n    if not nums:\n        return 0\n\n    num_set = set(nums)\n    longest = 0\n\n    for num in num_set:\n        # Only start sequence if num-1 not in set\n        if num - 1 not in num_set:\n            current = num\n            length = 1\n\n            while current + 1 in num_set:\n                current += 1\n                length += 1\n\n            longest = max(longest, length)\n\n    return longest\n\n# Test\nnums = [100, 4, 200, 1, 3, 2]\nprint(longest_consecutive(nums))  # 4 (sequence: 1, 2, 3, 4)\n</code></pre>"},{"location":"DSA/06_hash_tables/#5-subarray-sum-equals-k","title":"5. Subarray Sum Equals K","text":"<pre><code>def subarray_sum(nums, k):\n    \"\"\"\n    Count subarrays with sum equal to k\n    Time Complexity: O(n)\n    Space Complexity: O(n)\n    \"\"\"\n    count = 0\n    prefix_sum = 0\n    sum_count = {0: 1}  # Initialize with 0 sum\n\n    for num in nums:\n        prefix_sum += num\n\n        # Check if (prefix_sum - k) exists\n        if prefix_sum - k in sum_count:\n            count += sum_count[prefix_sum - k]\n\n        # Update sum count\n        sum_count[prefix_sum] = sum_count.get(prefix_sum, 0) + 1\n\n    return count\n\n# Test\nnums = [1, 1, 1]\nk = 2\nprint(subarray_sum(nums, k))  # 2\n</code></pre>"},{"location":"DSA/06_hash_tables/#python-collections-for-hash-tables","title":"Python Collections for Hash Tables","text":""},{"location":"DSA/06_hash_tables/#1-defaultdict","title":"1. defaultdict","text":"<pre><code>from collections import defaultdict\n\n# Auto-initialize missing keys\nword_count = defaultdict(int)\nfor word in [\"apple\", \"banana\", \"apple\"]:\n    word_count[word] += 1\n\nprint(word_count)  # {'apple': 2, 'banana': 1}\n\n# With lists\ngroups = defaultdict(list)\ngroups[\"fruits\"].append(\"apple\")\ngroups[\"fruits\"].append(\"banana\")\n</code></pre>"},{"location":"DSA/06_hash_tables/#2-counter","title":"2. Counter","text":"<pre><code>from collections import Counter\n\n# Count occurrences\nwords = [\"apple\", \"banana\", \"apple\", \"cherry\", \"banana\", \"apple\"]\ncounter = Counter(words)\nprint(counter)  # Counter({'apple': 3, 'banana': 2, 'cherry': 1})\n\n# Most common\nprint(counter.most_common(2))  # [('apple', 3), ('banana', 2)]\n\n# Operations\nc1 = Counter(['a', 'b', 'c'])\nc2 = Counter(['b', 'c', 'd'])\nprint(c1 + c2)  # Counter({'b': 2, 'c': 2, 'a': 1, 'd': 1})\n</code></pre>"},{"location":"DSA/06_hash_tables/#3-ordereddict","title":"3. OrderedDict","text":"<pre><code>from collections import OrderedDict\n\n# Maintains insertion order (note: regular dict also maintains order in Python 3.7+)\nod = OrderedDict()\nod['a'] = 1\nod['b'] = 2\nod['c'] = 3\n\n# Move to end\nod.move_to_end('a')\nprint(od)  # OrderedDict([('b', 2), ('c', 3), ('a', 1)])\n</code></pre>"},{"location":"DSA/06_hash_tables/#hash-function-design","title":"Hash Function Design","text":""},{"location":"DSA/06_hash_tables/#good-hash-function-properties","title":"Good Hash Function Properties:","text":"<ol> <li>Deterministic - Same input gives same output</li> <li>Uniform Distribution - Minimizes collisions</li> <li>Fast Computation - O(1) time</li> <li>Uses all input data - Avoids clustering</li> </ol> <pre><code># Simple hash functions\ndef hash_string(s, table_size):\n    \"\"\"Polynomial rolling hash\"\"\"\n    hash_value = 0\n    for char in s:\n        hash_value = (hash_value * 31 + ord(char)) % table_size\n    return hash_value\n\ndef hash_number(num, table_size):\n    \"\"\"Modulo hash for integers\"\"\"\n    return num % table_size\n</code></pre>"},{"location":"DSA/06_hash_tables/#applications-of-hash-tables","title":"Applications of Hash Tables","text":"<ol> <li>Caching - Store computed results (LRU cache)</li> <li>Databases - Fast indexing and lookup</li> <li>Symbol Tables - Compilers and interpreters</li> <li>Removing Duplicates - Using set</li> <li>Frequency Counting - Character/word counts</li> <li>Routing - IP address lookup</li> <li>Password Verification - Storing hashed passwords</li> </ol>"},{"location":"DSA/06_hash_tables/#advantages","title":"Advantages","text":"<ol> <li>Fast Operations - O(1) average for insert, delete, search</li> <li>Flexible Keys - Any hashable type</li> <li>Dynamic - Can grow as needed</li> <li>Easy to Implement - Simple interface</li> </ol>"},{"location":"DSA/06_hash_tables/#disadvantages","title":"Disadvantages","text":"<ol> <li>Space Overhead - Extra memory for hash table</li> <li>No Order - Keys are not sorted (use OrderedDict if needed)</li> <li>Hash Function - Quality affects performance</li> <li>Worst Case - O(n) with many collisions</li> </ol> <p>Practice Problems</p> <p>Try solving these hash table problems: - Valid Sudoku - Longest substring without repeating characters - Top K frequent elements - Design LRU Cache - Isomorphic strings</p> <p>When to Use Hash Tables</p> <p>Use hash tables when you need: - Fast lookup by key (O(1)) - Counting occurrences - Removing duplicates - Caching results - Mapping relationships</p> <p>Next Tutorial: Sorting Algorithms</p>"},{"location":"DSA/07_sorting_algorithms/","title":"Sorting Algorithms","text":"<p>Sorting is the process of arranging elements in a specific order (ascending or descending). Sorting algorithms are fundamental to computer science and are used in many applications.</p>"},{"location":"DSA/07_sorting_algorithms/#why-learn-sorting","title":"Why Learn Sorting?","text":"<ol> <li>Foundation for Other Algorithms - Binary search requires sorted data</li> <li>Data Organization - Makes data easier to analyze</li> <li>Interview Questions - Commonly asked in technical interviews</li> <li>Performance Analysis - Understanding time/space complexity</li> <li>Real-World Applications - Databases, file systems, search engines</li> </ol>"},{"location":"DSA/07_sorting_algorithms/#comparison-of-sorting-algorithms","title":"Comparison of Sorting Algorithms","text":"Algorithm Best Average Worst Space Stable Bubble Sort O(n) O(n\u00b2) O(n\u00b2) O(1) Yes Selection Sort O(n\u00b2) O(n\u00b2) O(n\u00b2) O(1) No Insertion Sort O(n) O(n\u00b2) O(n\u00b2) O(1) Yes Merge Sort O(n log n) O(n log n) O(n log n) O(n) Yes Quick Sort O(n log n) O(n log n) O(n\u00b2) O(log n) No Heap Sort O(n log n) O(n log n) O(n log n) O(1) No Counting Sort O(n+k) O(n+k) O(n+k) O(k) Yes Radix Sort O(nk) O(nk) O(nk) O(n+k) Yes <p>Stable: Maintains relative order of equal elements</p>"},{"location":"DSA/07_sorting_algorithms/#1-bubble-sort","title":"1. Bubble Sort","text":"<p>Repeatedly swap adjacent elements if they're in wrong order.</p>"},{"location":"DSA/07_sorting_algorithms/#algorithm","title":"Algorithm:","text":"<ol> <li>Compare adjacent elements</li> <li>Swap if in wrong order</li> <li>Repeat until no swaps needed</li> </ol> <pre><code>def bubble_sort(arr):\n    \"\"\"\n    Bubble Sort\n    Time Complexity: O(n\u00b2)\n    Space Complexity: O(1)\n    \"\"\"\n    n = len(arr)\n\n    for i in range(n):\n        swapped = False\n\n        for j in range(0, n - i - 1):\n            if arr[j] &gt; arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n                swapped = True\n\n        # If no swaps, array is sorted\n        if not swapped:\n            break\n\n    return arr\n\n# Test\narr = [64, 34, 25, 12, 22, 11, 90]\nprint(bubble_sort(arr))  # [11, 12, 22, 25, 34, 64, 90]\n</code></pre>"},{"location":"DSA/07_sorting_algorithms/#when-to-use","title":"When to Use:","text":"<ul> <li>Small datasets</li> <li>Nearly sorted data</li> <li>Educational purposes</li> </ul>"},{"location":"DSA/07_sorting_algorithms/#2-selection-sort","title":"2. Selection Sort","text":"<p>Find minimum element and place it at beginning.</p>"},{"location":"DSA/07_sorting_algorithms/#algorithm_1","title":"Algorithm:","text":"<ol> <li>Find minimum in unsorted portion</li> <li>Swap with first unsorted element</li> <li>Move boundary of sorted portion</li> </ol> <pre><code>def selection_sort(arr):\n    \"\"\"\n    Selection Sort\n    Time Complexity: O(n\u00b2)\n    Space Complexity: O(1)\n    \"\"\"\n    n = len(arr)\n\n    for i in range(n):\n        min_idx = i\n\n        # Find minimum in remaining array\n        for j in range(i + 1, n):\n            if arr[j] &lt; arr[min_idx]:\n                min_idx = j\n\n        # Swap\n        arr[i], arr[min_idx] = arr[min_idx], arr[i]\n\n    return arr\n\n# Test\narr = [64, 25, 12, 22, 11]\nprint(selection_sort(arr))  # [11, 12, 22, 25, 64]\n</code></pre>"},{"location":"DSA/07_sorting_algorithms/#when-to-use_1","title":"When to Use:","text":"<ul> <li>Small datasets</li> <li>Memory is limited (in-place)</li> <li>When number of swaps matters</li> </ul>"},{"location":"DSA/07_sorting_algorithms/#3-insertion-sort","title":"3. Insertion Sort","text":"<p>Build sorted array one element at a time.</p>"},{"location":"DSA/07_sorting_algorithms/#algorithm_2","title":"Algorithm:","text":"<ol> <li>Start with second element</li> <li>Compare with elements before it</li> <li>Insert in correct position</li> <li>Repeat for all elements</li> </ol> <pre><code>def insertion_sort(arr):\n    \"\"\"\n    Insertion Sort\n    Time Complexity: O(n\u00b2) average, O(n) best\n    Space Complexity: O(1)\n    \"\"\"\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n\n        # Move elements greater than key one position ahead\n        while j &gt;= 0 and arr[j] &gt; key:\n            arr[j + 1] = arr[j]\n            j -= 1\n\n        arr[j + 1] = key\n\n    return arr\n\n# Test\narr = [12, 11, 13, 5, 6]\nprint(insertion_sort(arr))  # [5, 6, 11, 12, 13]\n</code></pre>"},{"location":"DSA/07_sorting_algorithms/#when-to-use_2","title":"When to Use:","text":"<ul> <li>Small datasets</li> <li>Nearly sorted data</li> <li>Online sorting (sort as data arrives)</li> <li>Used in hybrid algorithms (Timsort)</li> </ul>"},{"location":"DSA/07_sorting_algorithms/#4-merge-sort","title":"4. Merge Sort","text":"<p>Divide and conquer algorithm that splits array and merges sorted halves.</p>"},{"location":"DSA/07_sorting_algorithms/#algorithm_3","title":"Algorithm:","text":"<ol> <li>Divide array into two halves</li> <li>Recursively sort each half</li> <li>Merge sorted halves</li> </ol> <pre><code>def merge_sort(arr):\n    \"\"\"\n    Merge Sort\n    Time Complexity: O(n log n)\n    Space Complexity: O(n)\n    \"\"\"\n    if len(arr) &lt;= 1:\n        return arr\n\n    # Divide\n    mid = len(arr) // 2\n    left = merge_sort(arr[:mid])\n    right = merge_sort(arr[mid:])\n\n    # Conquer (Merge)\n    return merge(left, right)\n\ndef merge(left, right):\n    \"\"\"Merge two sorted arrays\"\"\"\n    result = []\n    i = j = 0\n\n    while i &lt; len(left) and j &lt; len(right):\n        if left[i] &lt;= right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    result.extend(left[i:])\n    result.extend(right[j:])\n\n    return result\n\n# Test\narr = [38, 27, 43, 3, 9, 82, 10]\nprint(merge_sort(arr))  # [3, 9, 10, 27, 38, 43, 82]\n</code></pre>"},{"location":"DSA/07_sorting_algorithms/#when-to-use_3","title":"When to Use:","text":"<ul> <li>Need guaranteed O(n log n)</li> <li>Stable sort required</li> <li>Sorting linked lists</li> <li>External sorting (large datasets)</li> </ul>"},{"location":"DSA/07_sorting_algorithms/#5-quick-sort","title":"5. Quick Sort","text":"<p>Divide and conquer using pivot element.</p>"},{"location":"DSA/07_sorting_algorithms/#algorithm_4","title":"Algorithm:","text":"<ol> <li>Choose pivot element</li> <li>Partition: elements &lt; pivot on left, &gt; pivot on right</li> <li>Recursively sort left and right partitions</li> </ol> <pre><code>def quick_sort(arr):\n    \"\"\"\n    Quick Sort\n    Time Complexity: O(n log n) average, O(n\u00b2) worst\n    Space Complexity: O(log n)\n    \"\"\"\n    if len(arr) &lt;= 1:\n        return arr\n\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x &lt; pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n\n    return quick_sort(left) + middle + quick_sort(right)\n\n# In-place version\ndef quick_sort_inplace(arr, low, high):\n    \"\"\"Quick Sort (in-place)\"\"\"\n    if low &lt; high:\n        pi = partition(arr, low, high)\n        quick_sort_inplace(arr, low, pi - 1)\n        quick_sort_inplace(arr, pi + 1, high)\n\n    return arr\n\ndef partition(arr, low, high):\n    \"\"\"Partition array around pivot\"\"\"\n    pivot = arr[high]\n    i = low - 1\n\n    for j in range(low, high):\n        if arr[j] &lt;= pivot:\n            i += 1\n            arr[i], arr[j] = arr[j], arr[i]\n\n    arr[i + 1], arr[high] = arr[high], arr[i + 1]\n    return i + 1\n\n# Test\narr = [10, 7, 8, 9, 1, 5]\nprint(quick_sort(arr))  # [1, 5, 7, 8, 9, 10]\n\narr2 = [10, 7, 8, 9, 1, 5]\nprint(quick_sort_inplace(arr2, 0, len(arr2) - 1))  # [1, 5, 7, 8, 9, 10]\n</code></pre>"},{"location":"DSA/07_sorting_algorithms/#when-to-use_4","title":"When to Use:","text":"<ul> <li>General purpose sorting</li> <li>Average case O(n log n)</li> <li>In-place sorting needed</li> <li>Used in many standard libraries</li> </ul>"},{"location":"DSA/07_sorting_algorithms/#6-heap-sort","title":"6. Heap Sort","text":"<p>Uses binary heap data structure.</p>"},{"location":"DSA/07_sorting_algorithms/#algorithm_5","title":"Algorithm:","text":"<ol> <li>Build max heap from array</li> <li>Extract maximum (root) to end</li> <li>Heapify remaining elements</li> <li>Repeat</li> </ol> <pre><code>def heap_sort(arr):\n    \"\"\"\n    Heap Sort\n    Time Complexity: O(n log n)\n    Space Complexity: O(1)\n    \"\"\"\n    n = len(arr)\n\n    # Build max heap\n    for i in range(n // 2 - 1, -1, -1):\n        heapify(arr, n, i)\n\n    # Extract elements from heap\n    for i in range(n - 1, 0, -1):\n        arr[0], arr[i] = arr[i], arr[0]  # Swap\n        heapify(arr, i, 0)\n\n    return arr\n\ndef heapify(arr, n, i):\n    \"\"\"Heapify subtree rooted at index i\"\"\"\n    largest = i\n    left = 2 * i + 1\n    right = 2 * i + 2\n\n    if left &lt; n and arr[left] &gt; arr[largest]:\n        largest = left\n\n    if right &lt; n and arr[right] &gt; arr[largest]:\n        largest = right\n\n    if largest != i:\n        arr[i], arr[largest] = arr[largest], arr[i]\n        heapify(arr, n, largest)\n\n# Test\narr = [12, 11, 13, 5, 6, 7]\nprint(heap_sort(arr))  # [5, 6, 7, 11, 12, 13]\n</code></pre>"},{"location":"DSA/07_sorting_algorithms/#when-to-use_5","title":"When to Use:","text":"<ul> <li>Guaranteed O(n log n)</li> <li>In-place sorting</li> <li>When auxiliary space is limited</li> </ul>"},{"location":"DSA/07_sorting_algorithms/#7-counting-sort","title":"7. Counting Sort","text":"<p>Non-comparison based sorting for integers in specific range.</p>"},{"location":"DSA/07_sorting_algorithms/#algorithm_6","title":"Algorithm:","text":"<ol> <li>Count occurrences of each value</li> <li>Calculate cumulative counts</li> <li>Place elements in sorted order</li> </ol> <pre><code>def counting_sort(arr):\n    \"\"\"\n    Counting Sort\n    Time Complexity: O(n + k) where k is range\n    Space Complexity: O(k)\n    \"\"\"\n    if not arr:\n        return arr\n\n    max_val = max(arr)\n    min_val = min(arr)\n    range_size = max_val - min_val + 1\n\n    # Count occurrences\n    count = [0] * range_size\n    for num in arr:\n        count[num - min_val] += 1\n\n    # Reconstruct sorted array\n    sorted_arr = []\n    for i, cnt in enumerate(count):\n        sorted_arr.extend([i + min_val] * cnt)\n\n    return sorted_arr\n\n# Test\narr = [4, 2, 2, 8, 3, 3, 1]\nprint(counting_sort(arr))  # [1, 2, 2, 3, 3, 4, 8]\n</code></pre>"},{"location":"DSA/07_sorting_algorithms/#when-to-use_6","title":"When to Use:","text":"<ul> <li>Small range of integers</li> <li>Need linear time O(n)</li> <li>Stable sort required</li> </ul>"},{"location":"DSA/07_sorting_algorithms/#8-radix-sort","title":"8. Radix Sort","text":"<p>Sorts by processing digits from least to most significant.</p> <pre><code>def radix_sort(arr):\n    \"\"\"\n    Radix Sort\n    Time Complexity: O(nk) where k is number of digits\n    Space Complexity: O(n + k)\n    \"\"\"\n    if not arr:\n        return arr\n\n    max_val = max(arr)\n    exp = 1\n\n    while max_val // exp &gt; 0:\n        counting_sort_by_digit(arr, exp)\n        exp *= 10\n\n    return arr\n\ndef counting_sort_by_digit(arr, exp):\n    \"\"\"Counting sort by specific digit\"\"\"\n    n = len(arr)\n    output = [0] * n\n    count = [0] * 10\n\n    # Count occurrences\n    for num in arr:\n        digit = (num // exp) % 10\n        count[digit] += 1\n\n    # Cumulative count\n    for i in range(1, 10):\n        count[i] += count[i - 1]\n\n    # Build output\n    for i in range(n - 1, -1, -1):\n        digit = (arr[i] // exp) % 10\n        output[count[digit] - 1] = arr[i]\n        count[digit] -= 1\n\n    # Copy to original\n    for i in range(n):\n        arr[i] = output[i]\n\n# Test\narr = [170, 45, 75, 90, 802, 24, 2, 66]\nprint(radix_sort(arr))  # [2, 24, 45, 66, 75, 90, 170, 802]\n</code></pre>"},{"location":"DSA/07_sorting_algorithms/#when-to-use_7","title":"When to Use:","text":"<ul> <li>Sorting integers or strings</li> <li>Need linear time</li> <li>Fixed number of digits</li> </ul>"},{"location":"DSA/07_sorting_algorithms/#pythons-built-in-sorting","title":"Python's Built-in Sorting","text":"<p>Python uses Timsort, a hybrid of Merge Sort and Insertion Sort.</p> <pre><code># Sort list in-place\narr = [3, 1, 4, 1, 5, 9, 2, 6]\narr.sort()\nprint(arr)  # [1, 1, 2, 3, 4, 5, 6, 9]\n\n# Sort and return new list\narr = [3, 1, 4, 1, 5, 9, 2, 6]\nsorted_arr = sorted(arr)\nprint(sorted_arr)  # [1, 1, 2, 3, 4, 5, 6, 9]\n\n# Reverse sort\narr.sort(reverse=True)\nprint(arr)  # [9, 6, 5, 4, 3, 2, 1, 1]\n\n# Custom key function\nwords = ['banana', 'pie', 'Washington', 'book']\nwords.sort(key=len)\nprint(words)  # ['pie', 'book', 'banana', 'Washington']\n\n# Sort by multiple criteria\nstudents = [('Alice', 85), ('Bob', 75), ('Charlie', 85)]\nstudents.sort(key=lambda x: (-x[1], x[0]))  # By grade desc, then name asc\nprint(students)  # [('Alice', 85), ('Charlie', 85), ('Bob', 75)]\n</code></pre>"},{"location":"DSA/07_sorting_algorithms/#sorting-comparisons","title":"Sorting Comparisons","text":""},{"location":"DSA/07_sorting_algorithms/#stable-vs-unstable","title":"Stable vs Unstable:","text":"<ul> <li>Stable: Maintains relative order of equal elements (Merge, Insertion, Bubble)</li> <li>Unstable: May change relative order (Quick, Heap, Selection)</li> </ul>"},{"location":"DSA/07_sorting_algorithms/#in-place-vs-not-in-place","title":"In-place vs Not In-place:","text":"<ul> <li>In-place: O(1) extra space (Quick, Heap, Insertion)</li> <li>Not in-place: O(n) extra space (Merge, Counting, Radix)</li> </ul>"},{"location":"DSA/07_sorting_algorithms/#common-sorting-problems","title":"Common Sorting Problems","text":""},{"location":"DSA/07_sorting_algorithms/#1-sort-colors-dutch-national-flag","title":"1. Sort Colors (Dutch National Flag)","text":"<pre><code>def sort_colors(nums):\n    \"\"\"\n    Sort array with values 0, 1, 2\n    Time Complexity: O(n)\n    Space Complexity: O(1)\n    \"\"\"\n    low = mid = 0\n    high = len(nums) - 1\n\n    while mid &lt;= high:\n        if nums[mid] == 0:\n            nums[low], nums[mid] = nums[mid], nums[low]\n            low += 1\n            mid += 1\n        elif nums[mid] == 1:\n            mid += 1\n        else:\n            nums[mid], nums[high] = nums[high], nums[mid]\n            high -= 1\n\n    return nums\n\n# Test\nnums = [2, 0, 2, 1, 1, 0]\nprint(sort_colors(nums))  # [0, 0, 1, 1, 2, 2]\n</code></pre>"},{"location":"DSA/07_sorting_algorithms/#2-merge-intervals","title":"2. Merge Intervals","text":"<pre><code>def merge_intervals(intervals):\n    \"\"\"\n    Merge overlapping intervals\n    Time Complexity: O(n log n)\n    \"\"\"\n    if not intervals:\n        return []\n\n    intervals.sort(key=lambda x: x[0])\n    merged = [intervals[0]]\n\n    for current in intervals[1:]:\n        if current[0] &lt;= merged[-1][1]:\n            merged[-1][1] = max(merged[-1][1], current[1])\n        else:\n            merged.append(current)\n\n    return merged\n\n# Test\nintervals = [[1, 3], [2, 6], [8, 10], [15, 18]]\nprint(merge_intervals(intervals))  # [[1, 6], [8, 10], [15, 18]]\n</code></pre>"},{"location":"DSA/07_sorting_algorithms/#3-kth-largest-element","title":"3. Kth Largest Element","text":"<pre><code>def find_kth_largest(nums, k):\n    \"\"\"\n    Find kth largest element using Quick Select\n    Time Complexity: O(n) average\n    \"\"\"\n    def partition(left, right):\n        pivot = nums[right]\n        i = left\n\n        for j in range(left, right):\n            if nums[j] &gt;= pivot:\n                nums[i], nums[j] = nums[j], nums[i]\n                i += 1\n\n        nums[i], nums[right] = nums[right], nums[i]\n        return i\n\n    left, right = 0, len(nums) - 1\n    k_index = k - 1\n\n    while left &lt;= right:\n        pivot_index = partition(left, right)\n\n        if pivot_index == k_index:\n            return nums[pivot_index]\n        elif pivot_index &lt; k_index:\n            left = pivot_index + 1\n        else:\n            right = pivot_index - 1\n\n# Test\nnums = [3, 2, 1, 5, 6, 4]\nprint(find_kth_largest(nums, 2))  # 5\n\n# Using Python's built-in\nimport heapq\nprint(heapq.nlargest(2, nums)[-1])  # 5\n</code></pre> <p>Choosing the Right Algorithm</p> <ul> <li>Small arrays (&lt; 50): Insertion Sort</li> <li>General purpose: Quick Sort or Timsort (Python's default)</li> <li>Guaranteed O(n log n): Merge Sort or Heap Sort</li> <li>Nearly sorted: Insertion Sort</li> <li>Integers in small range: Counting Sort or Radix Sort</li> <li>Need stability: Merge Sort or Timsort</li> </ul> <p>Practice Problems</p> <p>Try solving these sorting problems: - Sort array by parity - Largest number - Sort characters by frequency - Meeting rooms (interval scheduling) - Top K frequent elements</p> <p>Previous Tutorial: Hash Tables</p>"},{"location":"Data%20Collection%20and%20Visulization/21_weather_data/","title":"Weather Data Project","text":"<p>title: Data Cleaning author: Juma Shafara date: \"2024-01\" date-modified: \"2024-07-25\" description: we\u2019ll be using numpy and pandas, to explore some techniques we can use to manipulate and clean a dataset.  keywords: []</p> <p></p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool whereas Numpy is the fundamental package for scientific computing with Python.</p> <p>To continue with this notebook, you must have python, pandas and numpy installed.</p> <pre><code>## Uncomment and run this cell to install pandas and numpy\n#!pip install pandas numpy\n</code></pre> <pre><code># import the libraries\nimport pandas as pd\nimport numpy as np\nfrom dataidea.datasets import loadDataset\n</code></pre> <p>Let's check the versions of python, numpy and pandas we'll be using for this notebook</p> <pre><code># checking python version\nprint('Python Version: ',)\n!python --version\n</code></pre> <pre>\n<code>Python Version: \nPython 3.10.12\n</code>\n</pre> <pre><code># Checking numpy and pandas versions\nprint('Pandas Version: ', pd.__version__)\nprint('Numpy Version: ', np.__version__)\n</code></pre> <pre>\n<code>Pandas Version:  2.2.2\nNumpy Version:  1.26.4\n</code>\n</pre> <p>Let's load the dataset. We'll be using a weather dataset that imagined for learning purposes.</p> <pre><code># load the dataset\nweather_data = loadDataset('weather')\n</code></pre> <p>We can sample out random rows from the dataset using the <code>sample()</code> method, we can use the <code>n</code> parameter to specify the number of rows to sample</p> <pre><code># sample out random values from the dataset\nweather_data.sample(n=5)\n</code></pre> day temperature windspead event 4 07/01/2017 32.0 NaN Rain 1 04/01/2017 NaN 9.0 Sunny 7 10/01/2017 34.0 8.0 Cloudy 6 09/01/2017 NaN NaN NaN 3 06/01/2017 NaN 7.0 NaN <p>From our quick our sample, we can already observe some probles with the data that will need fixing</p> <p>Display some info about the dataset eg number of entries, count of non-null values and variable datatypes using the <code>info()</code> method</p> <pre><code># get quick dataframe info\nweather_data.info()\n</code></pre> <pre>\n<code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9 entries, 0 to 8\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   day          9 non-null      object \n 1   temperature  5 non-null      float64\n 2   windspead    5 non-null      float64\n 3   event        7 non-null      object \ndtypes: float64(2), object(2)\nmemory usage: 416.0+ bytes\n</code>\n</pre> <p>We can count all missing values in each column in our dataframe by using <code>dataframe.isna().sum()</code>, eg</p> <pre><code># count missing values in each column\nweather_data.isna().sum()\n</code></pre> <pre>\n<code>day            0\ntemperature    4\nwindspead      4\nevent          2\ndtype: int64</code>\n</pre> <p>We can use a boolean-indexing like technique to find all rows in a dataset with missing values in a specific column.</p> <pre><code># get rows with missing data in temperature\nweather_data[weather_data.temperature.isna()]\n</code></pre> day temperature windspead event 1 04/01/2017 NaN 9.0 Sunny 3 06/01/2017 NaN 7.0 NaN 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN NaN <pre><code># get rows with missing data in event column\nweather_data[weather_data.event.isna()]\n</code></pre> day temperature windspead event 3 06/01/2017 NaN 7.0 NaN 6 09/01/2017 NaN NaN NaN <p>For the next part, we would like to demonstrate forward fill (<code>ffill()</code>) and  backward fill (<code>bfill</code>), we first create two copies of the dataframe to avoid modifying our original copy in memory. - <code>ffill()</code> fills the missing values with the previous valid value in the column - <code>bfill()</code> fills the missing values with the next valid value in the column</p> <p>Let's create 2 copies of our dataframe and test out each of these concepts on either of the copies</p> <p>For the first copy, let's fill NaN values in the <code>event</code> column with <code>ffill()</code></p> <pre><code># fill with the previous valid value\nweather_data['event'] = weather_data.event.ffill()\nweather_data\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 NaN 7.0 Snow 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN Sunny 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <p>From the returned dataframe we can observe that the <code>NaN</code> values in <code>event</code> have been replaced with their corresponding non-null values orccurring earlier than them ie Snow at row 3 and Sunny at row 6 </p> <p>Exercise: Demonstrate how to replace missing values with the <code>bfill()</code> method</p> <p>We can modify (or fill) a specific value in the dataframe by using the <code>loc[]</code> method. This picks the value by its row (index) and column names. Assigning it a new value modifies it in the dataframe as illustrated below</p> <pre><code># modify a specific value in the dataframe\nweather_data.loc[1, 'temperature'] = 29\nweather_data\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 29.0 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 NaN 7.0 Snow 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN Sunny 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <p>Observe that the missing value in row 1 and <code>temperature</code> has been replaced with 29</p> <p>We can use the <code>fillna()</code> method to replace all missing values in a column with a specific value as demostrated value</p> <pre><code># replace missing values in temperature column with mean\nweather_data['temperature'] = weather_data.temperature.fillna(\n    value=weather_data.temperature.mean()\n)\nweather_data\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 29.0 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 32.5 7.0 Snow 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 32.5 NaN Sunny 6 09/01/2017 32.5 NaN Sunny 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <p>Exercise: Demonstrate some technniques to replace missing data for numeric, and categorical data using the <code>fillna()</code></p> <p>We can also use the <code>fillna()</code> method to fill missing values in multiple columns by passing in the dictionary of key/value pairs of column-name and value to replace. To demonstrate this, let's first reload a fresh dataframe with missing data</p> <pre><code>weather_data = loadDataset('weather')\nweather_data\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 NaN 7.0 NaN 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN NaN 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <pre><code># Replace missing values in temperature, column and event\nweather_data.fillna(value={\n    'temperature': weather_data.temperature.mean(), \n    'windspead': weather_data.windspead.max(), \n    'event': weather_data.event.bfill()\n    }, inplace=True)\n\n# Now let's look at our data\nweather_data\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 29.0 9.0 Sunny 2 05/01/2017 28.0 12.0 Snow 3 06/01/2017 32.5 7.0 Snow 4 07/01/2017 32.0 12.0 Rain 5 08/01/2017 32.5 12.0 Sunny 6 09/01/2017 32.5 12.0 Sunny 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <p>We can optionally drop all rows with missing values using the <code>dropna()</code> method. To demonstrate this, let's first reload a fresh dataframe with missing data</p> <pre><code>weather_data = loadDataset('weather')\nweather_data\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 NaN 7.0 NaN 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN NaN 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <pre><code># Drop all rows with missing values\nweather_data.dropna()\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <p>In the next chapter we'll look at some data visualization tools in Python</p>"},{"location":"Data%20Collection%20and%20Visulization/21_weather_data/#cleaning-the-weather-dataset","title":"Cleaning the weather dataset","text":"<p>In this notebook, we'll be using numpy and pandas, to explore some techniques we can use to manipulate and clean a dataset. We'll be using the <code>weather dataset</code> which I developed specifically for the purpose of this lesson.</p>"},{"location":"Data%20Collection%20and%20Visulization/21_weather_data/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Data%20Collection%20and%20Visulization/21_weather_data/#choosing-between-ffill-and-bfill","title":"Choosing Between <code>ffill</code> and <code>bfill</code>","text":"<ul> <li>Context: Choose based on the context and the logical assumption that fits the nature of your data. If the past influences the present, use <code>ffill</code>. If the future influences the present, use <code>bfill</code>.</li> <li>Data Patterns: Consider the patterns in your data and what makes sense for your specific analysis or model. Ensure that the method you choose maintains the integrity and meaning of your data. </li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/21_weather_data/#fill-with-a-specific-value","title":"Fill with a specific value","text":""},{"location":"Data%20Collection%20and%20Visulization/21_weather_data/#congratulations","title":"Congratulations!","text":"<p>Congratulations on finishing this lesson. In this lesson, you have learned various methods of handling missing data including:</p> <ul> <li> Finding missing values</li> <li> bfill and ffill</li> <li> filling with a specific value</li> <li> min, max and mean values</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/21_weather_data/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Data%20Collection%20and%20Visulization/31_matplotlib_refined/","title":"Matplotlib Crash Course","text":"Meta Data  **title: Matplotlib Crash Course author: Juma Shafara date: \"2024-01\" date-modified: \"2024-09-18\" description: Matplotlib is a powerful plotting library in Python commonly used for data visualization. keywords: [python visualization, matplotlib, bar chart, histogram, scatter plot, line plot, box plot, pie chart, stacked bar chart, quiz, python quiz, dataidea, data science,]**   <p>Matplotlib is a powerful plotting library in Python commonly used for data visualization. </p> <p>When working with datasets, you can use Matplotlib to create various plots to explore and visualize the data. </p> <p>Here are some major plots you can create using Matplotlib with the Titanic dataset:</p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># # Uncomment and run this cell to install the libraries\n# !pip install pandas matplotlib dataidea\n</code></pre> <pre><code># import the libraries, packages and modules\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n</code></pre> <p>Let's demonstrate each of the plots using the Titanic dataset. This dataset can be downloaded from our datasets hub; ; DATAIDEA Datasets for practice. We'll first load the dataset and then create each plot using Matplotlib.</p> <pre><code># Load the Titanic dataset\ntitanic_df = pd.read_csv('titanic.csv')\n</code></pre> <p>We can load this dataset like this because it is inbuilt in the dataidea package</p> <pre><code>titanic_df.head(n=5)\n</code></pre> pclass survived name sex age sibsp parch ticket fare cabin embarked boat body home.dest 0 1.0 1.0 Allen, Miss. Elisabeth Walton female 29.0000 0.0 0.0 24160 211.3375 B5 S 2 NaN St Louis, MO 1 1.0 1.0 Allison, Master. Hudson Trevor male 0.9167 1.0 2.0 113781 151.5500 C22 C26 S 11 NaN Montreal, PQ / Chesterville, ON 2 1.0 0.0 Allison, Miss. Helen Loraine female 2.0000 1.0 2.0 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON 3 1.0 0.0 Allison, Mr. Hudson Joshua Creighton male 30.0000 1.0 2.0 113781 151.5500 C22 C26 S NaN 135.0 Montreal, PQ / Chesterville, ON 4 1.0 0.0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25.0000 1.0 2.0 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON <ol> <li>Bar Plot: You can create a bar plot to visualize categorical data such as the number of passengers in each class (first class, second class, third class), the number of survivors vs. non-survivors, or the number of passengers embarked from each port (Cherbourg, Queenstown, Southampton).</li> </ol> <pre><code># 1. Bar Plot - Number of passengers in each class\nclass_counts = titanic_df.pclass.value_counts()\nclasses = class_counts.index\ncounts = class_counts.values\n\nplt.bar(x=classes, height=counts, color='#008374')\nplt.title('Number of Passengers Per Passenger Class')\nplt.xlabel('Passenger Class')\nplt.ylabel('Number of Passengers')\n\nplt.show()\n</code></pre> <p>It's easy to see from the graph that the 3rd class had the largest number of passengers, followed by the 1st class and 2nd class comes last</p> <ol> <li>Histogram: Histograms are useful for visualizing the distribution of continuous variables such as age or fare. You can create histograms to see the age distribution of passengers or the fare distribution.</li> </ol> <pre><code># 2. Histogram - Age distribution of passengers\nages = titanic_df.age\nplt.hist(x=ages, bins=20, color='#008374', \n         edgecolor='#66FDEE')\nplt.title('Age Distribution of Passengers')\nplt.ylabel('Frequency')\nplt.xlabel('Age')\nplt.show()\n</code></pre> <p>From the histogram we can observe that:</p> <ul> <li> The majority of the people we of ages between 15 and 35</li> <li> Fewer older people(above 60 years) boarded the titanic (below 20)t</li> </ul> <ol> <li>Box Plot: A box plot can be used to show the distribution of a continuous variable across different categories. For example, you can create a box plot to visualize the distribution of age or fare across different passenger classes.</li> </ol> <p>3.1. Age distribution boxplot</p> <pre><code># 3.1 Age distribution boxplot\nages = titanic_df.age.dropna()\nplt.boxplot(x=ages, vert=False,)\nplt.title('Age Distribution of Passengers')\nplt.xlabel('Age')\nplt.show()\n</code></pre> <p>Features of a box plot:</p> <p> Box: The box in a boxplot represents the interquartile range (IQR), which contains the middle 50% of the data. The top and bottom edges of the box are the third quartile (Q3) and the first quartile (Q1), respectively.</p> <p> Median Line: A line inside the box indicates the median (Q2) of the data, which is the middle value of the dataset.</p> <p> Whiskers: The whiskers extend from the edges of the box to the smallest and largest values within 1.5 times the IQR from Q1 and Q3. They represent the range of the bulk of the data.</p> <p> Outliers: Data points that fall outside the whiskers are considered outliers. They are typically plotted as individual points. Outliers can be indicative of variability or errors in the data.</p> <p> Minimum and Maximum: The ends of the whiskers show the minimum and maximum values within the range of 1.5 times the IQR from the first and third quartiles.</p> <p>Meaning: A boxplot provides a visual summary of several important aspects of a dataset:</p> <ul> <li>Central Tendency: The median line shows the central point of the data.</li> <li>Spread: The IQR (the length of the box) shows the spread of the middle 50% of the data.</li> <li>Symmetry and Skewness: The relative position of the median within the box and the length of the whiskers can indicate whether the data is symmetric or skewed.</li> <li>Outliers: Individual points outside the whiskers highlight potential outliers.</li> </ul> <p>Boxplots are particularly useful for comparing distributions between several groups or datasets and identifying outliers and potential anomalies.</p> <p>3.2 Age Distribution Across Passenger Classes</p> <pre><code># 3. Box Plot - Distribution of age across passenger classes\nplt.boxplot([titanic_df[titanic_df['pclass'] == 1]['age'].dropna(),\n             titanic_df[titanic_df['pclass'] == 2]['age'].dropna(),\n             titanic_df[titanic_df['pclass'] == 3]['age'].dropna()],\n            labels=['1st Class', '2nd Class', '3rd Class'])\nplt.xlabel('Passenger Class')\nplt.ylabel('Age')\nplt.title('Distribution of Age Across Passenger Classes')\nplt.show()\n</code></pre> <pre>\n<code>/tmp/ipykernel_16695/4289029800.py:2: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n  plt.boxplot([titanic_df[titanic_df['pclass'] == 1]['age'].dropna(),\n</code>\n</pre> <ol> <li>Scatter Plot: Scatter plots are helpful for visualizing the relationship between two continuous variables. You can create scatter plots to explore relationships such as age vs. fare. Read more about the scatter plot from the Matplotlib documentation</li> </ol> <pre><code># 4. Scatter Plot - Age vs. Fare\nplt.scatter(\n    x=titanic_df['age'], \n    y=titanic_df['fare'], \n    alpha=.5, \n    c=titanic_df['survived'], \n    cmap=ListedColormap(['#008374', '#000000'])\n)\nplt.xlabel('Age')\nplt.ylabel('Fare')\nplt.title('Age vs. Fare')\nplt.colorbar(label='Survived')  \nplt.show()\n</code></pre> <p>I don't about you but for me I don't see a linear relationship between the age and fare of the titanic passengers</p> <ol> <li>Pie Chart: Pie charts can be used to visualize the proportion of different categories within a dataset. For example, you can create a pie chart to show the proportion of male vs. female passengers or the proportion of survivors vs. non-survivors.</li> </ol> <pre><code># 5. Pie Chart - Proportion of male vs. female passengers\ngender_counts = titanic_df['sex'].value_counts()\nplt.pie(x=gender_counts, labels=gender_counts.index, \n        autopct='%1.1f%%', startangle=90, \n        colors=['#008374', '#66FDEE'])\nplt.title('Proportion of Male vs. Female Passengers')\nplt.legend(loc='lower right')\nplt.show()\n</code></pre> <ol> <li>Stacked Bar Plot: Stacked bar plots can be used to compare the composition of different categories across groups. For example, you can create a stacked bar plot to compare the proportion of survivors and non-survivors within each passenger class.</li> </ol> <pre><code># 6. Stacked Bar Plot - Survival status within each passenger class\nsurvival_counts = titanic_df.groupby(['pclass', 'survived']).size().unstack()\nsurvival_counts.plot(kind='bar', stacked=True,  \n                     color=['#008374', '#66FDEE'])\nplt.xlabel('Passenger Class')\nplt.ylabel('Number of Passengers')\nplt.title('Survival Status Within Each Passenger Class')\nplt.legend(['Did not survive', 'Survived'])\nplt.show()\n</code></pre> <pre><code>titanic_df.groupby(['pclass', 'survived']).size().unstack()\n</code></pre> survived 0.0 1.0 pclass 1.0 123 200 2.0 158 119 3.0 528 181 <p>We observe that:</p> <ul> <li> More passengers in class 1 survived than those that did not survive (200 vs 123)</li> <li> Most of the passengers in class 3 did not survive (528 vs 181)</li> <li> Slightly more passengers did not survive as compared to those that survived in class 2 (152 vs 119)</li> </ul> <ol> <li>Line Plot: Line plots can be useful for visualizing trends over time or continuous variables. While the Titanic dataset may not have explicit time data, you can still use line plots to visualize trends such as the change in survival rate with increasing age or fare.</li> </ol> <pre><code># 7. Line Plot - Mean age of passengers by passenger class\nmean_age_by_class = titanic_df.groupby('pclass')['age'].mean()\nplt.plot(mean_age_by_class.index, mean_age_by_class.values, \n         marker='*', color='#008374')\nplt.xlabel('Passenger Class')\nplt.ylabel('Mean Age')\nplt.title('Mean Age of Passengers by Passenger Class')\nplt.show()\n</code></pre> <p>We can quickly see the average ages for each passenger class, ie:</p> <ul> <li> Around 39 for first class</li> <li> Around 30 for second class</li> <li> Around 25 for third class</li> </ul> <p>These are some of the major plots you can create using Matplotlib. Each plot serves a different purpose and can help you gain insights into the data and explore relationships between variables.</p> <pre><code>air_passengers_data = loadDataset('air_passengers')\nair_passengers_data.head()\n</code></pre> Month Passengers 0 1949-01 112 1 1949-02 118 2 1949-03 132 3 1949-04 129 4 1949-05 121 <pre><code>air_passengers_data['Month'] = pd.to_datetime(air_passengers_data.Month)\nplt.plot('Month', 'Passengers', data=air_passengers_data, color='#008374')\nplt.xlabel('Years')\nplt.ylabel('Number of Passengers')\nplt.show()\n</code></pre> <p>We can observe that the number of passengers seems to increase with time</p>"},{"location":"Data%20Collection%20and%20Visulization/31_matplotlib_refined/#what-is-matploblib","title":"What is Matploblib","text":""},{"location":"Data%20Collection%20and%20Visulization/31_matplotlib_refined/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Data%20Collection%20and%20Visulization/31_matplotlib_refined/#review","title":"Review","text":"<p>Congratulations on reaching the end of this tutorial. In this tutorial, we have learned the basic graphs and how to interprete them. ie</p> <ul> <li> Bar chart</li> <li> Histogram</li> <li> Scatter plot</li> <li> Line plot</li> <li> Box plot</li> <li> Pie chart</li> <li> Stacked bar chart</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/31_matplotlib_refined/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Data%20Collection%20and%20Visulization/32_data_exploration_and_cleaning_exercise/","title":"Data Exploration & Cleaning Exercise","text":"<p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <ol> <li>Load demo.xlsx dataset</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Rename the columns as suggested below</li> </ol> Old name New name Age age Gender gender Marital Status marital_status Address address Income income Income Category income_category Job Category job_category <pre><code># your solution\n</code></pre> <ol> <li>Display all the columns in the dataset</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Display some basic statistics about the numeric variables in the dataset</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Display some basic statistics about the categorical variables in the dataset</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>What are the unique observations under gender?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Can you fix any problems observed under the gender, give brief explanations why and how</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>How many observations have 'no answer' for marital status?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write some piece of code to return only numeric variables from the dataset</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Are there any missing values in the dataset?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Are there any outliers in the income variable?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Investigate the relationship between age and income</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>How many people earn more than 300 units?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>What data type is the marital status?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Create dummy variables for gender</li> </ol> <pre><code># your solution\n</code></pre> <p>END</p>"},{"location":"Data%20Collection%20and%20Visulization/32_data_exploration_and_cleaning_exercise/#data-exploration-and-cleaning-exercise","title":"Data Exploration and Cleaning Exercise","text":""},{"location":"Data%20Collection%20and%20Visulization/32_data_exploration_and_cleaning_exercise/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Data%20Collection%20and%20Visulization/32_data_exploration_and_cleaning_exercise/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/","title":"Handling Missing Data","text":"<p>title: Handling Missing Data keywords: [handling missing data, handling missing data in python, python data analysis, dataidea, machine learning, data preprocessing, sklearn data preprocessing] description: n this notebook, we look into the top four missing data imputation methods author: Juma Shafara date: \"2024-03\"</p> <p></p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># install the libraries for this demonstration\n# ! pip install -U dataidea\n</code></pre> <pre><code>import pandas as pd\nimport dataidea as di\n</code></pre> <p><code>loadDataset</code> allows us to load datasets inbuilt in the dataidea library</p> <pre><code>weather = di.loadDataset('weather') \nweather\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 NaN 7.0 NaN 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN NaN 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <pre><code>weather.isna().sum()\n</code></pre> <pre>\n<code>day            0\ntemperature    4\nwindspead      4\nevent          2\ndtype: int64</code>\n</pre> <p>Let's demonstrate how to use the top three missing data imputation methods\u2014SimpleImputer, KNNImputer, and IterativeImputer\u2014using the simple weather dataset.</p> <pre><code># select age from the data\ntemp_wind = weather[['temperature', 'windspead']].copy()\n</code></pre> <pre><code>temp_wind_imputed = temp_wind.copy()\n</code></pre> <p></p> <pre><code>from sklearn.impute import SimpleImputer\n\nsimple_imputer = SimpleImputer(strategy='mean')\ntemp_wind_simple_imputed = simple_imputer.fit_transform(temp_wind)\n\ntemp_wind_simple_imputed_df = pd.DataFrame(temp_wind_simple_imputed, columns=temp_wind.columns)\n</code></pre> <p>Let's have a look at the outcome</p> <pre><code>temp_wind_simple_imputed_df\n</code></pre> temperature windspead 0 32.0 6.0 1 33.2 9.0 2 28.0 8.4 3 33.2 7.0 4 32.0 8.4 5 33.2 8.4 6 33.2 8.4 7 34.0 8.0 8 40.0 12.0 <p></p> <pre><code>from sklearn.impute import KNNImputer\n\nknn_imputer = KNNImputer(n_neighbors=2)\ntemp_wind_knn_imputed = knn_imputer.fit_transform(temp_wind)\n\ntemp_wind_knn_imputed_df = pd.DataFrame(temp_wind_knn_imputed, columns=temp_wind.columns)\n</code></pre> <p>If we take a look at the outcome</p> <pre><code>weather\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 NaN 7.0 NaN 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN NaN 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <p></p> <pre><code>from sklearn.impute import KNNImputer\n\nknn_imputer = KNNImputer(n_neighbors=2)\nwindspead_imputed = knn_imputer.fit_transform(weather[['windspead']].reset_index())\n\nwindspead_imputed\n</code></pre> <pre>\n<code>array([[ 0. ,  6. ],\n       [ 1. ,  9. ],\n       [ 2. ,  8. ],\n       [ 3. ,  7. ],\n       [ 4. ,  8. ],\n       [ 5. ,  7.5],\n       [ 6. , 10. ],\n       [ 7. ,  8. ],\n       [ 8. , 12. ]])</code>\n</pre> <pre><code># we can fill it back in the weather data\nweather['windspead'] = windspead_imputed[:, 1]\n\n# now looking at the data\nweather\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 8.0 Snow 3 06/01/2017 NaN 7.0 NaN 4 07/01/2017 32.0 8.0 Rain 5 08/01/2017 NaN 7.5 Sunny 6 09/01/2017 NaN 10.0 NaN 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <p></p> <pre><code>from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\niterative_imputer = IterativeImputer()\ntemp_wind_iterative_imputed = iterative_imputer.fit_transform(temp_wind)\n\ntemp_wind_iterative_imputed_df = pd.DataFrame(temp_wind_iterative_imputed, columns=temp_wind.columns)\n\ntemp_wind_iterative_imputed_df\n</code></pre> temperature windspead 0 32.000000 6.0 1 33.967053 9.0 2 28.000000 8.0 3 31.410210 7.0 4 32.000000 8.0 5 32.049421 7.5 6 35.245474 10.0 7 34.000000 8.0 8 40.000000 12.0 <p>You can also choose an estimator of your choice, let's try a <code>Linear Regression</code> model</p> <pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# set estimator to an instance of a model\niterative_imputer = IterativeImputer(estimator=LinearRegression())\ntemp_wind_iterative_imputed = iterative_imputer.fit_transform(temp_wind)\n\ntemp_wind_iterative_imputed_df = pd.DataFrame(temp_wind_iterative_imputed, columns=temp_wind.columns)\n\ntemp_wind_iterative_imputed_df\n</code></pre> temperature windspead 0 32.000000 6.0 1 34.125000 9.0 2 28.000000 8.0 3 31.041667 7.0 4 32.000000 8.0 5 31.812500 7.5 6 35.666667 10.0 7 34.000000 8.0 8 40.000000 12.0 <p></p> <pre><code># import datawig\n\n# # Impute missing values\n# df_imputed = datawig.SimpleImputer.complete(weather)\n</code></pre> <p>These top imputation methods offer different trade-offs in terms of computational complexity, handling of missing data patterns, and ease of use. The choice between them depends on the specific characteristics of the dataset and the requirements of the analysis.</p> <p> Don't miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it's easy and safe. </p>"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#introduction","title":"Introduction:","text":"<p>Missing data is a common hurdle in data analysis, impacting the reliability of insights drawn from datasets. Python offers a range of solutions to address this issue, some of which we discussed in the earlier weeks. In this notebook, we look into the top four missing data imputation methods:</p> <ul> <li>SimpleImputer</li> <li>KNNImputer</li> <li>IterativeImputer </li> <li>Datawig</li> </ul> <p>We'll explore these essential techniques, using sklearn and the weather dataset.</p>"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#simpleimputer-from-scikit-learn","title":"SimpleImputer from scikit-learn:","text":"<ul> <li>Usage: SimpleImputer is a straightforward method for imputing missing values by replacing them with a constant, mean, median, or most frequent value along each column.</li> <li>Pros:<ul> <li>Easy to use and understand.</li> <li>Can handle both numerical and categorical data.</li> <li>Offers flexibility with different imputation strategies.</li> </ul> </li> <li>Cons:<ul> <li>It doesn't consider relationships between features.</li> <li>May not be the best choice for datasets with complex patterns of missingness.</li> </ul> </li> <li>Example:</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#exercise","title":"Exercise:","text":"<ol> <li>Try out the SimpleImputer with different imputation strategies like mode, constant </li> <li>Choose and try some imputation techniques on categorical data</li> </ol>"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#knnimputer-from-scikit-learn","title":"KNNImputer from scikit-learn:","text":"<ul> <li>Usage: <ul> <li>KNNImputer imputes missing values using k-nearest neighbors, replacing them with the mean value of the nearest neighbors.</li> <li>You can read more about the KNNImputer from the sklearn official docs site</li> </ul> </li> <li>Pros:<ul> <li>Considers relationships between features, making it suitable for datasets with complex patterns of missingness.</li> <li>Can handle both numerical and categorical data.</li> </ul> </li> <li>Cons:<ul> <li>Computationally expensive for large datasets.</li> <li>Requires careful selection of the number of neighbors (k).</li> </ul> </li> </ul> Note!<p>By default, the KNNImputer uses 'nan' values as missing data and the 'nan_euclidean' metric to calculate the distances between values.</p> <ul> <li>Example:</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#filling-a-single-column-independently-using-the-knnimputer","title":"Filling a single column independently using the <code>KNNImputer</code>","text":"<p>To use the KNNImputer for a single independ column, you can use the index as the other column instead, this will result into equal euclidean distances resulting into the use of the physical neighbors in the data table.</p>"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#exercise_1","title":"Exercise","text":"<ul> <li>Try out the KNNImputer with different numbers of neighbors and compare the results</li> <li>Findo out how to use KNNImputer to fill categorical data</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#iterativeimputer-from-scikit-learn","title":"IterativeImputer from scikit-learn:","text":"<ul> <li>Usage: IterativeImputer models each feature with missing values as a function of other features and uses that estimate for imputation. It iteratively estimates the missing values.</li> <li>Pros:<ul> <li>Takes into account relationships between features, making it suitable for datasets with complex missing patterns.</li> <li>More robust than SimpleImputer for handling missing data.</li> </ul> </li> <li>Cons:<ul> <li>Can be computationally intensive and slower than SimpleImputer.</li> <li>Requires careful tuning of model parameters.</li> </ul> </li> <li>Example:</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#datawig","title":"Datawig:","text":"<p>Datawig is a library specifically designed for imputing missing values in tabular data using deep learning models.</p>"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#homework","title":"Homework","text":"<ul> <li>Try out these techniques for categorical data</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Data%20Collection%20and%20Visulization/kobo_toolbox/","title":"Kobo Toolbox","text":""},{"location":"Data%20Collection%20and%20Visulization/kobo_toolbox/#kobo-toolbox","title":"Kobo Toolbox","text":"<p>Kobo Toolbox is a free, open-source suite of tools for data collection, particularly useful in challenging environments. It's widely used by humanitarian organizations, researchers, and data scientists who need to collect data in the field, often in areas with limited internet connectivity.</p> <p>Kobo Toolbox is a comprehensive platform that allows you to:</p> <ul> <li>Design surveys using a user-friendly web interface</li> <li>Collect data using mobile devices (smartphones, tablets) even offline</li> <li>Manage and export data in various formats (CSV, Excel, JSON) for analysis</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/kobo_toolbox/#key-features","title":"Key Features","text":"<ul> <li>Offline Data Collection: Collect data without internet connection; syncs automatically when connection is restored</li> <li>Multiple Question Types: Text, numbers, dates, GPS coordinates, images, audio, video, barcode scanning</li> <li>User-Friendly Interface: Drag-and-drop form builder with mobile-optimized data entry</li> <li>Data Management: Real-time visualization, data export, validation, and quality checks</li> <li>Free and Open Source: No cost for basic features with community support</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/kobo_toolbox/#video-tutorial","title":"Video Tutorial","text":""},{"location":"Data%20Collection%20and%20Visulization/kobo_toolbox/#integration-with-data-science","title":"Integration with Data Science","text":"<p>Kobo Toolbox fits seamlessly into the data science pipeline:</p> <pre><code>Design Survey \u2192 Collect Data (Offline) \u2192 Export (CSV/Excel) \u2192 \nData Cleaning (Python/Pandas) \u2192 Analysis \u2192 Insights\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/kobo_toolbox/#example-loading-kobo-data-into-python","title":"Example: Loading Kobo Data into Python","text":"<pre><code>import pandas as pd\n\n# Export your data from Kobo Toolbox as CSV\n# Then load it into Python for analysis\ndf = pd.read_csv('kobo_survey_data.csv')\n\n# Explore and analyze\nprint(df.head())\nprint(df.info())\nprint(df.describe())\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/kobo_toolbox/#resources","title":"ResourcesWhat's on your mind? Put it in the comments!","text":"<ul> <li>Official Website: kobotoolbox.org</li> <li>Documentation: support.kobotoolbox.org</li> <li>Community Forum: forum.kobotoolbox.org</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/nobel_prize_laureates_exercise/","title":"Nobel prize laureates exercise","text":"<p>title: Nobel Prize Laureates Exercise author: Juma Shafara date: \"2024-01\" date-modified: \"2024-07-25\" description: Here are ten questions based on the Nobel Prize laureates dataset. Each question involves a mix of data exploration, cleaning, and analysis tasks.  keywords: []</p> <p></p> <p>Here are ten questions based on the Nobel Prize laureates dataset. Each question involves a mix of data exploration, cleaning, and analysis tasks. You can use Python and libraries such as pandas, matplotlib to solve these questions.</p> <p>You can download the Nobel Prize Laureates dataset from opendatasoft</p> <ol> <li>How many Nobel Prize laureates are included in the dataset?</li> </ol> <pre><code># your solution here\n</code></pre> <ol> <li>Which country has the highest number of Nobel laureates?</li> </ol> <pre><code># your solution here\n</code></pre> <p></p> <ol> <li>What is the distribution of Nobel laureates across different prize categories?</li> </ol> <pre><code># your solution here\n</code></pre> <ol> <li>How many Nobel laureates were awarded in each decade?</li> </ol> <pre><code># your solution here\n</code></pre> <ol> <li>Are there any missing values in the dataset? If so, in which columns?</li> </ol> <pre><code># your solution here\n</code></pre> <ol> <li>Perform data cleaning by handling missing values appropriately. Describe your approach.</li> </ol> <pre><code># your solution here\n</code></pre> <p></p> <ol> <li>Visualize the distribution of Nobel laureates' birth countries on a world map.</li> </ol> <pre><code># your solution here\n</code></pre> <ol> <li>Is there any correlation between a laureate's birth year and the year they were awarded the Nobel Prize? Visualize if there's any relationship.</li> </ol> <pre><code># your solution here\n</code></pre> <ol> <li>Perform anomaly detection on the birth years of laureates. Identify and explain any outliers.</li> </ol> <pre><code># your solution here\n</code></pre> <p> Don't miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it's easy and safe. </p> <ol> <li>Based on the dataset, can you identify any interesting trends or patterns regarding Nobel laureates' demographics or the fields in which they were awarded?</li> </ol> <pre><code># your solution here\n</code></pre> <p>These questions cover various aspects of data exploration, cleaning, visualization, and analysis using the Nobel Prize laureates dataset. You can explore and analyze the dataset to find answers to these questions and gain insights into the demographics and distribution of Nobel laureates over time and across different categories. </p> <p></p>"},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/","title":"Pandas puzzles","text":"<p>title: 60 pandas puzzles keywords: [pandas, pandas quiz, dataidea, data science, data analysis, python, machine learning]</p> <p></p> <p>Since pandas is a large library with many different specialist features and functions, these excercises focus mainly on the fundamentals of manipulating data (indexing, grouping, aggregating, cleaning), making use of the core DataFrame and Series objects. </p> <p>Many of the excerises here are stright-forward in that the solutions require no more than a few lines of code (in pandas or NumPy... don't go using pure Python or Cython!). Choosing the right methods and following best practices is the underlying goal.</p> <p>The exercises are loosely divided in sections. Each section has a difficulty rating; these ratings are subjective, of course, but should be a seen as a rough guide as to how inventive the required solution is.</p> <p>If you're just starting out with pandas and you are looking for some other resources, the official documentation  is very extensive. In particular, some good places get a broader overview of pandas are...</p> <ul> <li>10 minutes to pandas</li> <li>pandas basics</li> <li>tutorials</li> <li>cookbook and idioms</li> </ul> <p>Enjoy the puzzles!</p> <pre><code>#| hidden\n\nimport pandas as pd\n</code></pre> <pre><code># solution\n</code></pre> <p>2. Print the version of pandas that has been imported.</p> <pre><code># solution\n</code></pre> <p>3. Print out all the version information of the libraries that are required by the pandas library.</p> <pre><code># solution\n</code></pre> <pre><code>import numpy as np\n\ndata = {'animal': ['cat', 'cat', 'snake', 'dog', 'dog', 'cat', 'snake', 'cat', 'dog', 'dog'],\n        'age': [2.5, 3, 0.5, np.nan, 5, 2, 4.5, np.nan, 7, 3],\n        'visits': [1, 3, 2, 3, 2, 3, 1, 1, 2, 1],\n        'priority': ['yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no']}\n\nlabels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n\n# df =  (complete this line of code)\n</code></pre> <p>5. Display a summary of the basic information about this DataFrame and its data (hint: there is a single method that can be called on the DataFrame).</p> <pre><code># solution\n</code></pre> <p>6. Return the first 3 rows of the DataFrame <code>df</code>.</p> <pre><code># solution\n</code></pre> <p>7. Select just the 'animal' and 'age' columns from the DataFrame <code>df</code>.</p> <pre><code># solution\n</code></pre> <p>8. Select the data in rows <code>[3, 4, 8]</code> and in columns <code>['animal', 'age']</code>.</p> <pre><code># solution\n</code></pre> <p>9. Select only the rows where the number of visits is greater than 3.</p> <pre><code># solution\n</code></pre> <p></p> <p>10. Select the rows where the age is missing, i.e. it is <code>NaN</code>.</p> <pre><code># solution\n</code></pre> <p>11. Select the rows where the animal is a cat and the age is less than 3.</p> <pre><code># solution\n</code></pre> <p>12. Select the rows the age is between 2 and 4 (inclusive).</p> <pre><code># solution\n</code></pre> <p>13. Change the age in row 'f' to 1.5.</p> <pre><code># solution\n</code></pre> <p>14. Calculate the sum of all visits in <code>df</code> (i.e. find the total number of visits).</p> <pre><code># solution\n</code></pre> <p></p> <p>15. Calculate the mean age for each different animal in <code>df</code>.</p> <pre><code># solution\n</code></pre> <p>16. Append a new row 'k' to <code>df</code> with your choice of values for each column. Then delete that row to return the original DataFrame.</p> <pre><code># solution\n</code></pre> <p>17. Count the number of each type of animal in <code>df</code>.</p> <pre><code># solution\n</code></pre> <p>18. Sort <code>df</code> first by the values in the 'age' in decending order, then by the value in the 'visits' column in ascending order (so row <code>i</code> should be first, and row <code>d</code> should be last).</p> <pre><code># solution\n</code></pre> <p>19. The 'priority' column contains the values 'yes' and 'no'. Replace this column with a column of boolean values: 'yes' should be <code>True</code> and 'no' should be <code>False</code>.</p> <pre><code># solution\n</code></pre> <p></p> <p>20. In the 'animal' column, change the 'snake' entries to 'python'.</p> <pre><code># solution\n</code></pre> <p>21. For each animal type and each number of visits, find the mean age. In other words, each row is an animal, each column is a number of visits and the values are the mean ages (hint: use a pivot table).</p> <pre><code># solution\n</code></pre> <p>22. You have a DataFrame <code>df</code> with a column 'A' of integers. For example: <pre><code>df = pd.DataFrame({'A': [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 7]})\n</code></pre></p> <p>How do you filter out rows which contain the same integer as the row immediately above?</p> <p>You should be left with a column containing the following values:</p> <pre><code>1, 2, 3, 4, 5, 6, 7\n</code></pre> <pre><code># solution\n</code></pre> <p>23. Given a DataFrame of numeric values, say <pre><code>df = pd.DataFrame(np.random.random(size=(5, 3))) # a 5x3 frame of float values\n</code></pre></p> <p>how do you subtract the row mean from each element in the row?</p> <pre><code># solution\n</code></pre> <p>24. Suppose you have DataFrame with 10 columns of real numbers, for example:</p> <p><pre><code>df = pd.DataFrame(np.random.random(size=(5, 10)), columns=list('abcdefghij'))\n</code></pre> Which column of numbers has the smallest sum?  Return that column's label.</p> <pre><code># solution\n</code></pre> <p></p> <p>25. How do you count how many unique rows a DataFrame has (i.e. ignore all rows that are duplicates)? As input, use a DataFrame of zeros and ones with 10 rows and 3 columns.</p> <pre><code>df = pd.DataFrame(np.random.randint(0, 2, size=(10, 3)))\n</code></pre> <pre><code># solution\n</code></pre> <p>The next three puzzles are slightly harder.</p> <p>26. In the cell below, you have a DataFrame <code>df</code> that consists of 10 columns of floating-point numbers. Exactly 5 entries in each row are NaN values. </p> <p>For each row of the DataFrame, find the column which contains the third NaN value.</p> <p>You should return a Series of column labels: <code>e, c, d, h, d</code></p> <pre><code>nan = np.nan\n\ndata = [[0.04,  nan,  nan, 0.25,  nan, 0.43, 0.71, 0.51,  nan,  nan],\n        [ nan,  nan,  nan, 0.04, 0.76,  nan,  nan, 0.67, 0.76, 0.16],\n        [ nan,  nan, 0.5 ,  nan, 0.31, 0.4 ,  nan,  nan, 0.24, 0.01],\n        [0.49,  nan,  nan, 0.62, 0.73, 0.26, 0.85,  nan,  nan,  nan],\n        [ nan,  nan, 0.41,  nan, 0.05,  nan, 0.61,  nan, 0.48, 0.68]]\n\ncolumns = list('abcdefghij')\n\ndf = pd.DataFrame(data, columns=columns)\n\n# write a solution to the question here\n</code></pre> <p>27. A DataFrame has a column of groups 'grps' and and column of integer values 'vals': </p> <p><pre><code>df = pd.DataFrame({'grps': list('aaabbcaabcccbbc'), \n                   'vals': [12,345,3,1,45,14,4,52,54,23,235,21,57,3,87]})\n</code></pre> For each group, find the sum of the three greatest values. You should end up with the answer as follows: <pre><code>grps\na    409\nb    156\nc    345\n</code></pre></p> <pre><code>df = pd.DataFrame({'grps': list('aaabbcaabcccbbc'), \n                   'vals': [12,345,3,1,45,14,4,52,54,23,235,21,57,3,87]})\n\n# write a solution to the question here\n</code></pre> <p>28. The DataFrame <code>df</code> constructed below has two integer columns 'A' and 'B'. The values in 'A' are between 1 and 100 (inclusive). </p> <p>For each group of 10 consecutive integers in 'A' (i.e. <code>(0, 10]</code>, <code>(10, 20]</code>, ...), calculate the sum of the corresponding values in column 'B'.</p> <p>The answer should be a Series as follows:</p> <pre><code>A\n(0, 10]      635\n(10, 20]     360\n(20, 30]     315\n(30, 40]     306\n(40, 50]     750\n(50, 60]     284\n(60, 70]     424\n(70, 80]     526\n(80, 90]     835\n(90, 100]    852\n</code></pre> <pre><code>df = pd.DataFrame(np.random.RandomState(8765).randint(1, 101, size=(100, 2)), columns = [\"A\", \"B\"])\n\n# write a solution to the question here\n</code></pre> <p>29. Consider a DataFrame <code>df</code> where there is an integer column 'X': <pre><code>df = pd.DataFrame({'X': [7, 2, 0, 3, 4, 2, 5, 0, 3, 4]})\n</code></pre> For each value, count the difference back to the previous zero (or the start of the Series, whichever is closer). These values should therefore be </p> <pre><code>[1, 2, 0, 1, 2, 3, 4, 0, 1, 2]\n</code></pre> <p>Make this a new column 'Y'.</p> <pre><code># solution\n</code></pre> <p></p> <p>30. Consider the DataFrame constructed below which contains rows and columns of numerical data. </p> <p>Create a list of the column-row index locations of the 3 largest values in this DataFrame. In this case, the answer should be: <pre><code>[(5, 7), (6, 4), (2, 5)]\n</code></pre></p> <pre><code>df = pd.DataFrame(np.random.RandomState(30).randint(1, 101, size=(8, 8)))\n</code></pre> <p>31. You are given the DataFrame below with a column of group IDs, 'grps', and a column of corresponding integer values, 'vals'.</p> <pre><code>df = pd.DataFrame({\"vals\": np.random.RandomState(31).randint(-30, 30, size=15), \n                   \"grps\": np.random.RandomState(31).choice([\"A\", \"B\"], 15)})\n</code></pre> <p>Create a new column 'patched_values' which contains the same values as the 'vals' any negative values in 'vals' with the group mean:</p> <pre><code>    vals grps  patched_vals\n0    -12    A          13.6\n1     -7    B          28.0\n2    -14    A          13.6\n3      4    A           4.0\n4     -7    A          13.6\n5     28    B          28.0\n6     -2    A          13.6\n7     -1    A          13.6\n8      8    A           8.0\n9     -2    B          28.0\n10    28    A          28.0\n11    12    A          12.0\n12    16    A          16.0\n13   -24    A          13.6\n14   -12    A          13.6\n</code></pre> <pre><code># solution\n</code></pre> <p>32. Implement a rolling mean over groups with window size 3, which ignores NaN value. For example consider the following DataFrame:</p> <p><pre><code>&amp;gt;&amp;gt;&amp;gt; df = pd.DataFrame({'group': list('aabbabbbabab'),\n                       'value': [1, 2, 3, np.nan, 2, 3, np.nan, 1, 7, 3, np.nan, 8]})\n&amp;gt;&amp;gt;&amp;gt; df\n   group  value\n0      a    1.0\n1      a    2.0\n2      b    3.0\n3      b    NaN\n4      a    2.0\n5      b    3.0\n6      b    NaN\n7      b    1.0\n8      a    7.0\n9      b    3.0\n10     a    NaN\n11     b    8.0\n</code></pre> The goal is to compute the Series:</p> <p><pre><code>0     1.000000\n1     1.500000\n2     3.000000\n3     3.000000\n4     1.666667\n5     3.000000\n6     3.000000\n7     2.000000\n8     3.666667\n9     2.000000\n10    4.500000\n11    4.000000\n</code></pre> E.g. the first window of size three for group 'b' has values 3.0, NaN and 3.0 and occurs at row index 5. Instead of being NaN the value in the new column at this row index should be 3.0 (just the two non-NaN values are used to compute the mean (3+3)/2)</p> <pre><code># solution\n</code></pre> <p>33. Create a DatetimeIndex that contains each business day of 2015 and use it to index a Series of random numbers. Let's call this Series <code>s</code>.</p> <pre><code># solution\n</code></pre> <p>34. Find the sum of the values in <code>s</code> for every Wednesday.</p> <pre><code># solution\n</code></pre> <p></p> <p>35. For each calendar month in <code>s</code>, find the mean of values.</p> <pre><code># solution\n</code></pre> <p>36. For each group of four consecutive calendar months in <code>s</code>, find the date on which the highest value occurred.</p> <pre><code># solution\n</code></pre> <p>37. Create a DateTimeIndex consisting of the third Thursday in each month for the years 2015 and 2016.</p> <pre><code># solution\n</code></pre> <p>38. Some values in the the FlightNumber column are missing (they are <code>NaN</code>). These numbers are meant to increase by 10 with each row so 10055 and 10075 need to be put in place. Modify <code>df</code> to fill in these missing numbers and make the column an integer column (instead of a float column).</p> <pre><code># solution\n</code></pre> <p>39. The From_To column would be better as two separate columns! Split each string on the underscore delimiter <code>_</code> to give a new temporary DataFrame called 'temp' with the correct values. Assign the correct column names 'From' and 'To' to this temporary DataFrame. </p> <pre><code># solution\n</code></pre> <p></p> <p>40. Notice how the capitalisation of the city names is all mixed up in this temporary DataFrame 'temp'. Standardise the strings so that only the first letter is uppercase (e.g. \"londON\" should become \"London\".)</p> <pre><code># solution\n</code></pre> <p>41. Delete the From_To column from <code>df</code> and attach the temporary DataFrame 'temp' from the previous questions.</p> <pre><code># solution\n</code></pre> <p>42. In the Airline column, you can see some extra puctuation and symbols have appeared around the airline names. Pull out just the airline name. E.g. <code>'(British Airways. )'</code> should become <code>'British Airways'</code>.</p> <pre><code># solution\n</code></pre> <p>43. In the RecentDelays column, the values have been entered into the DataFrame as a list. We would like each first value in its own column, each second value in its own column, and so on. If there isn't an Nth value, the value should be NaN.</p> <p>Expand the Series of lists into a DataFrame named <code>delays</code>, rename the columns <code>delay_1</code>, <code>delay_2</code>, etc. and replace the unwanted RecentDelays column in <code>df</code> with <code>delays</code>.</p> <pre><code># solution\n</code></pre> <p>The DataFrame should look much better now. <pre><code>   FlightNumber          Airline      From         To  delay_1  delay_2  delay_3\n0         10045              KLM    London      Paris     23.0     47.0      NaN\n1         10055       Air France    Madrid      Milan      NaN      NaN      NaN\n2         10065  British Airways    London  Stockholm     24.0     43.0     87.0\n3         10075       Air France  Budapest      Paris     13.0      NaN      NaN\n4         10085        Swiss Air  Brussels     London     67.0     32.0      NaN\n</code></pre></p> <p>44. Given the lists <code>letters = ['A', 'B', 'C']</code> and <code>numbers = list(range(10))</code>, construct a MultiIndex object from the product of the two lists. Use it to index a Series of random numbers. Call this Series <code>s</code>.</p> <pre><code># solution\n</code></pre> <p></p> <p>45. Check the index of <code>s</code> is lexicographically sorted (this is a necessary proprty for indexing to work correctly with a MultiIndex).</p> <pre><code># solution\n</code></pre> <p>46. Select the labels <code>1</code>, <code>3</code> and <code>6</code> from the second level of the MultiIndexed Series.</p> <pre><code># solution\n</code></pre> <p>47. Slice the Series <code>s</code>; slice up to label 'B' for the first level and from label 5 onwards for the second level.</p> <pre><code># solution\n</code></pre> <p>48. Sum the values in <code>s</code> for each label in the first level (you should have Series giving you a total for labels A, B and C).</p> <pre><code># solution\n</code></pre> <p>49. Suppose that <code>sum()</code> (and other methods) did not accept a <code>level</code> keyword argument. How else could you perform the equivalent of <code>s.sum(level=1)</code>?</p> <pre><code># solution\n</code></pre> <p></p> <p>50. Exchange the levels of the MultiIndex so we have an index of the form (letters, numbers). Is this new Series properly lexsorted? If not, sort it.</p> <pre><code># solution\n</code></pre> <p>51. Let's suppose we're playing Minesweeper on a 5 by 4 grid, i.e. <pre><code>X = 5\nY = 4\n</code></pre> To begin, generate a DataFrame <code>df</code> with two columns, <code>'x'</code> and <code>'y'</code> containing every coordinate for this grid. That is, the DataFrame should start: <pre><code>   x  y\n0  0  0\n1  0  1\n2  0  2\n</code></pre></p> <pre><code># solution\n</code></pre> <p>52. For this DataFrame <code>df</code>, create a new column of zeros (safe) and ones (mine). The probability of a mine occuring at each location should be 0.4.</p> <pre><code># solution\n</code></pre> <p>53. Now create a new column for this DataFrame called <code>'adjacent'</code>. This column should contain the number of mines found on adjacent squares in the grid. </p> <p>(E.g. for the first row, which is the entry for the coordinate <code>(0, 0)</code>, count how many mines are found on the coordinates <code>(0, 1)</code>, <code>(1, 0)</code> and <code>(1, 1)</code>.)</p> <pre><code># solution\n</code></pre> <p>54. For rows of the DataFrame that contain a mine, set the value in the <code>'adjacent'</code> column to NaN.</p> <pre><code># solution\n</code></pre> <p></p> <p>55. Finally, convert the DataFrame to grid of the adjacent mine counts: columns are the <code>x</code> coordinate, rows are the <code>y</code> coordinate.</p> <pre><code># solution\n</code></pre> <pre><code># solution\n</code></pre> <p>57. Columns in your DataFrame can also be used to modify colors and sizes.  Bill has been keeping track of his performance at work over time, as well as how good he was feeling that day, and whether he had a cup of coffee in the morning.  Make a plot which incorporates all four features of this DataFrame.</p> <p>(Hint:  If you're having trouble seeing the plot, try multiplying the Series which you choose to represent size by 10 or more)</p> <p>The chart doesn't have to be pretty: this isn't a course in data viz!</p> <pre><code>df = pd.DataFrame({\"productivity\":[5,2,3,1,4,5,6,7,8,3,4,8,9],\n                   \"hours_in\"    :[1,9,6,5,3,9,2,9,1,7,4,2,2],\n                   \"happiness\"   :[2,1,3,2,3,1,2,3,1,2,2,1,3],\n                   \"caffienated\" :[0,0,1,1,0,0,0,0,1,1,0,1,0]})\n</code></pre> <pre><code># solution\n</code></pre> <p>58.  What if we want to plot multiple things?  Pandas allows you to pass in a matplotlib Axis object for plots, and plots will also return an Axis object.</p> <p>Make a bar plot of monthly revenue with a line plot of monthly advertising spending (numbers in millions)</p> <pre><code>df = pd.DataFrame({\"revenue\":[57,68,63,71,72,90,80,62,59,51,47,52],\n                   \"advertising\":[2.1,1.9,2.7,3.0,3.6,3.2,2.7,2.4,1.8,1.6,1.3,1.9],\n                   \"month\":range(12)\n                  })\n</code></pre> <pre><code># solution\n</code></pre> <p>Now we're finally ready to create a candlestick chart, which is a very common tool used to analyze stock price data.  A candlestick chart shows the opening, closing, highest, and lowest price for a stock during a time window.  The color of the \"candle\" (the thick part of the bar) is green if the stock closed above its opening price, or red if below.</p> <p></p> <p>This was initially designed to be a pandas plotting challenge, but it just so happens that this type of plot is just not feasible using pandas' methods.  If you are unfamiliar with matplotlib, we have provided a function that will plot the chart for you so long as you can use pandas to get the data into the correct format.</p> <p>Your first step should be to get the data in the correct format using pandas' time-series grouping function.  We would like each candle to represent an hour's worth of data.  You can write your own aggregation function which returns the open/high/low/close, but pandas has a built-in which also does this.</p> <p>The below cell contains helper functions.  Call <code>day_stock_data()</code> to generate a DataFrame containing the prices a hypothetical stock sold for, and the time the sale occurred.  Call <code>plot_candlestick(df)</code> on your properly aggregated and formatted stock data to print the candlestick chart.</p> <pre><code>import numpy as np\ndef float_to_time(x):\n    return str(int(x)) + \":\" + str(int(x%1 * 60)).zfill(2) + \":\" + str(int(x*60 % 1 * 60)).zfill(2)\n\ndef day_stock_data():\n    #NYSE is open from 9:30 to 4:00\n    time = 9.5\n    price = 100\n    results = [(float_to_time(time), price)]\n    while time &amp;lt; 16:\n        elapsed = np.random.exponential(.001)\n        time += elapsed\n        if time &amp;gt; 16:\n            break\n        price_diff = np.random.uniform(.999, 1.001)\n        price *= price_diff\n        results.append((float_to_time(time), price))\n\n\n    df = pd.DataFrame(results, columns = ['time','price'])\n    df.time = pd.to_datetime(df.time)\n    return df\n\n#Don't read me unless you get stuck!\ndef plot_candlestick(agg):\n    \"\"\"\n    agg is a DataFrame which has a DatetimeIndex and five columns: [\"open\",\"high\",\"low\",\"close\",\"color\"]\n    \"\"\"\n    fig, ax = plt.subplots()\n    for time in agg.index:\n        ax.plot([time.hour] * 2, agg.loc[time, [\"high\",\"low\"]].values, color = \"black\")\n        ax.plot([time.hour] * 2, agg.loc[time, [\"open\",\"close\"]].values, color = agg.loc[time, \"color\"], linewidth = 10)\n\n    ax.set_xlim((8,16))\n    ax.set_ylabel(\"Price\")\n    ax.set_xlabel(\"Hour\")\n    ax.set_title(\"OHLC of Stock Value During Trading Day\")\n    plt.show()\n</code></pre> <p>59. Generate a day's worth of random stock data, and aggregate / reformat it so that it has hourly summaries of the opening, highest, lowest, and closing prices</p> <pre><code># solution\n</code></pre> <p></p> <p>60. Now that you have your properly-formatted data, try to plot it yourself as a candlestick chart.  Use the <code>plot_candlestick(df)</code> function above, or matplotlib's <code>plot</code> documentation if you get stuck.</p> <pre><code># solution\n</code></pre> <p>More exercises to follow soon...</p> <p></p>"},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#importing-pandas","title":"Importing pandas","text":""},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#getting-started-and-checking-your-pandas-setup","title":"Getting started and checking your pandas setup","text":"<p>Difficulty: easy </p> <p>1. Import pandas under the alias <code>pd</code>.</p>"},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#dataframe-basics","title":"DataFrame basics","text":""},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#a-few-of-the-fundamental-routines-for-selecting-sorting-adding-and-aggregating-data-in-dataframes","title":"A few of the fundamental routines for selecting, sorting, adding and aggregating data in DataFrames","text":"<p>Difficulty: easy</p> <p>Note: remember to import numpy using: <pre><code>import numpy as np\n</code></pre></p> <p>Consider the following Python dictionary <code>data</code> and Python list <code>labels</code>:</p> <p><pre><code>data = {'animal': ['cat', 'cat', 'snake', 'dog', 'dog', 'cat', 'snake', 'cat', 'dog', 'dog'],\n        'age': [2.5, 3, 0.5, np.nan, 5, 2, 4.5, np.nan, 7, 3],\n        'visits': [1, 3, 2, 3, 2, 3, 1, 1, 2, 1],\n        'priority': ['yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no']}\n\nlabels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n</code></pre> (This is just some meaningless data I made up with the theme of animals and trips to a vet.)</p> <p>4. Create a DataFrame <code>df</code> from this dictionary <code>data</code> which has the index <code>labels</code>.</p>"},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#dataframes-beyond-the-basics","title":"DataFrames: beyond the basics","text":""},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#slightly-trickier-you-may-need-to-combine-two-or-more-methods-to-get-the-right-answer","title":"Slightly trickier: you may need to combine two or more methods to get the right answer","text":"<p>Difficulty: medium</p> <p>The previous section was tour through some basic but essential DataFrame operations. Below are some ways that you might need to cut your data, but for which there is no single \"out of the box\" method.</p>"},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#dataframes-harder-problems","title":"DataFrames: harder problems","text":""},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#these-might-require-a-bit-of-thinking-outside-the-box","title":"These might require a bit of thinking outside the box...","text":"<p>...but all are solvable using just the usual pandas/NumPy methods (and so avoid using explicit <code>for</code> loops).</p> <p>Difficulty: hard</p>"},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#series-and-datetimeindex","title":"Series and DatetimeIndex","text":""},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#exercises-for-creating-and-manipulating-series-with-datetime-data","title":"Exercises for creating and manipulating Series with datetime data","text":"<p>Difficulty: easy/medium</p> <p>pandas is fantastic for working with dates and times. These puzzles explore some of this functionality.</p>"},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#cleaning-data","title":"Cleaning Data","text":""},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#making-a-dataframe-easier-to-work-with","title":"Making a DataFrame easier to work with","text":"<p>Difficulty: easy/medium</p> <p>It happens all the time: someone gives you data containing malformed strings, Python, lists and missing data. How do you tidy it up so you can get on with the analysis?</p> <p>Take this monstrosity as the DataFrame to use in the following puzzles:</p> <p><pre><code>df = pd.DataFrame({'From_To': ['LoNDon_paris', 'MAdrid_miLAN', 'londON_StockhOlm', \n                               'Budapest_PaRis', 'Brussels_londOn'],\n              'FlightNumber': [10045, np.nan, 10065, np.nan, 10085],\n              'RecentDelays': [[23, 47], [], [24, 43, 87], [13], [67, 32]],\n                   'Airline': ['KLM(!)', '&lt;air france=\"\"&gt; (12)', '(British Airways. )', \n                               '12. Air France', '\"Swiss Air\"']})\n</code></pre> Formatted, it looks like this:</p> <pre><code>            From_To  FlightNumber  RecentDelays              Airline\n0      LoNDon_paris       10045.0      [23, 47]               KLM(!)\n1      MAdrid_miLAN           NaN            []    &lt;air france=\"\"&gt; (12)\n2  londON_StockhOlm       10065.0  [24, 43, 87]  (British Airways. )\n3    Budapest_PaRis           NaN          [13]       12. Air France\n4   Brussels_londOn       10085.0      [67, 32]          \"Swiss Air\"\n</code></pre> <p>(It's some flight data I made up; it's not meant to be accurate in any way.)</p> <p></p>"},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#using-multiindexes","title":"Using MultiIndexes","text":""},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#go-beyond-flat-dataframes-with-additional-index-levels","title":"Go beyond flat DataFrames with additional index levels","text":"<p>Difficulty: medium</p> <p>Previous exercises have seen us analysing data from DataFrames equipped with a single index level. However, pandas also gives you the possibilty of indexing your data using multiple levels. This is very much like adding new dimensions to a Series or a DataFrame. For example, a Series is 1D, but by using a MultiIndex with 2 levels we gain of much the same functionality as a 2D DataFrame.</p> <p>The set of puzzles below explores how you might use multiple index levels to enhance data analysis.</p> <p>To warm up, we'll look make a Series with two index levels. </p>"},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#minesweeper","title":"Minesweeper","text":""},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#generate-the-numbers-for-safe-squares-in-a-minesweeper-grid","title":"Generate the numbers for safe squares in a Minesweeper grid","text":"<p>Difficulty: medium to hard</p> <p>If you've ever used an older version of Windows, there's a good chance you've played with Minesweeper: - https://en.wikipedia.org/wiki/Minesweeper_(video_game)</p> <p>If you're not familiar with the game, imagine a grid of squares: some of these squares conceal a mine. If you click on a mine, you lose instantly. If you click on a safe square, you reveal a number telling you how many mines are found in the squares that are immediately adjacent. The aim of the game is to uncover all squares in the grid that do not contain a mine.</p> <p>In this section, we'll make a DataFrame that contains the necessary data for a game of Minesweeper: coordinates of the squares, whether the square contains a mine and the number of mines found on adjacent squares.</p>"},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#plotting","title":"Plotting","text":""},{"location":"Data%20Collection%20and%20Visulization/pandas-puzzles/#visualize-trends-and-patterns-in-data","title":"Visualize trends and patterns in data","text":"<p>Difficulty: medium</p> <p>To really get a good understanding of the data contained in your DataFrame, it is often essential to create plots: if you're lucky, trends and anomalies will jump right out at you. This functionality is baked into pandas and the puzzles below explore some of what's possible with the library.</p> <p>56. Pandas is highly integrated with the plotting library matplotlib, and makes plotting DataFrames very user-friendly! Plotting in a notebook environment usually makes use of the following boilerplate:</p> <pre><code>import matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\n</code></pre> <p>matplotlib is the plotting library which pandas' plotting functionality is built upon, and it is usually aliased to <code>plt</code>.</p> <p><code>%matplotlib inline</code> tells the notebook to show plots inline, instead of creating them in a separate window.  </p> <p><code>plt.style.use('ggplot')</code> is a style theme that most people find agreeable, based upon the styling of R's ggplot package.</p> <p>For starters, make a scatter plot of this random data, but use black X's instead of the default markers. </p> <p><code>df = pd.DataFrame({\"xs\":[1,5,2,8,1], \"ys\":[4,2,1,9,6]})</code></p> <p>Consult the documentation if you get stuck!</p>"},{"location":"Data%20Collection%20and%20Visulization/visualization_quiz/","title":"Visualization quiz","text":"<p>title: Data Visualization Quiz author: Juma Shafara date: \"2024-01\" date-modified: \"2024-07-25\" keywords: [python visualization, matplotlib, bar chart, histogram, scatter plot, line plot, box plot, pie chart, stacked bar chart, quiz, python quiz, dataidea, data science,]</p> <p></p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Data%20Collection%20and%20Visulization/visualization_quiz/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Data%20Collection%20and%20Visulization/visualization_quiz/#question-1-which-type-of-chart-is-used-to-show-the-relationship-between-two-continuous-variables","title":"Question 1: Which type of chart is used to show the relationship between two continuous variables?","text":"<p>a) Bar chart b) Pie chart c) Scatter plot d) Area chart  </p>"},{"location":"Data%20Collection%20and%20Visulization/visualization_quiz/#question-2-what-is-the-primary-purpose-of-a-radar-chart","title":"Question 2: What is the primary purpose of a radar chart?","text":"<p>a) Comparing multiple categories across a few variables b) Showing hierarchical data c) Displaying trends over time d) Visualizing correlations between two variables  </p>"},{"location":"Data%20Collection%20and%20Visulization/visualization_quiz/#question-3-in-a-stacked-bar-chart-each-bar-is-divided-into-segments-to-represent","title":"Question 3: In a stacked bar chart, each bar is divided into segments to represent:","text":"<p>a) Different categories of a single variable b) Time periods c) Trends over time d) Correlation between variables  </p>"},{"location":"Data%20Collection%20and%20Visulization/visualization_quiz/#question-4-when-should-a-heat-map-be-used","title":"Question 4: When should a heat map be used?","text":"<p>a) Showing proportions within a population b) Comparing individual data points c) Visualizing spatial relationships and correlations d) Displaying data distribution  </p>"},{"location":"Data%20Collection%20and%20Visulization/visualization_quiz/#question-5-which-type-of-chart-is-best-suited-for-comparing-parts-of-a-whole","title":"Question 5: Which type of chart is best suited for comparing parts of a whole?","text":"<p>a) Scatter plot b) Histogram c) Pie chart d) Line chart  </p>"},{"location":"Data%20Collection%20and%20Visulization/visualization_quiz/#question-6-which-chart-type-is-used-to-detect-outliers-and-visualize-the-distribution-of-a-dataset","title":"Question 6: Which chart type is used to detect outliers and visualize the distribution of a dataset?","text":"<p>a) Box plot b) Area chart c) Bubble chart d) Radar chart  </p>"},{"location":"Data%20Collection%20and%20Visulization/visualization_quiz/#question-7-what-does-a-bubble-chart-represent","title":"Question 7: What does a bubble chart represent?","text":"<p>a) Relationship between two continuous variables b) Relationship between two categorical variables c) Relationship between three variables using two continuous variables and one categorical variable d) Relationship between three continuous variables  </p>"},{"location":"Data%20Collection%20and%20Visulization/visualization_quiz/#question-8-which-chart-type-is-suitable-for-showing-hierarchical-data-with-nested-categories","title":"Question 8: Which chart type is suitable for showing hierarchical data with nested categories?","text":"<p>a) Tree map b) Scatter plot c) Line chart d) Histogram  </p>"},{"location":"Data%20Collection%20and%20Visulization/visualization_quiz/#question-9-when-is-a-bar-chart-more-appropriate-than-a-pie-chart","title":"Question 9: When is a bar chart more appropriate than a pie chart?","text":"<p>a) Comparing parts of a whole b) Showing trends over time c) Displaying a single continuous variable d) Visualizing correlations  </p>"},{"location":"Data%20Collection%20and%20Visulization/visualization_quiz/#question-10-what-is-the-main-purpose-of-a-histogram","title":"Question 10: What is the main purpose of a histogram?","text":"<p>a) Showing hierarchical data b) Visualizing correlations c) Displaying the distribution of a single continuous variable d) Comparing multiple categories across variables  </p>"},{"location":"Data%20Collection%20and%20Visulization/visualization_quiz/#answers","title":"Answers:","text":"<ol> <li>c) Scatter plot</li> <li>a) Comparing multiple categories across a few variables</li> <li>a) Different categories of a single variable</li> <li>c) Visualizing spatial relationships and correlations</li> <li>c) Pie chart</li> <li>a) Box plot</li> <li>c) Relationship between three variables using two continuous variables and one categorical variable</li> <li>a) Tree map</li> <li>a) Comparing parts of a whole</li> <li>c) Displaying the distribution of a single continuous variable</li> </ol>"},{"location":"Data%20Collection%20and%20Visulization/visualization_quiz/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Data%20Collection%20and%20Visulization/SQL/01_introduction/","title":"Introduction to SQL","text":"<p>SQL (Structured Query Language) is a standard language for managing and manipulating relational databases. It's essential for data scientists, analysts, and anyone working with data.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/01_introduction/#what-is-sql","title":"What is SQL?","text":"<p>SQL is a domain-specific language used to:</p> <ul> <li>Query data from databases</li> <li>Insert, update, and delete records</li> <li>Create and modify database structures</li> <li>Control access to data</li> <li>Manage transactions</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/SQL/01_introduction/#why-learn-sql","title":"Why Learn SQL?","text":"<ul> <li>Universal: Works across most database systems (MySQL, PostgreSQL, SQL Server, Oracle, SQLite)</li> <li>Essential for Data Science: Most data is stored in relational databases</li> <li>High Demand: SQL skills are among the most sought-after in tech jobs</li> <li>Foundation: Understanding SQL helps you work with modern data tools</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/SQL/01_introduction/#database-concepts","title":"Database Concepts","text":""},{"location":"Data%20Collection%20and%20Visulization/SQL/01_introduction/#what-is-a-database","title":"What is a Database?","text":"<p>A database is an organized collection of structured data stored electronically. Think of it as a digital filing cabinet where data is organized into:</p> <ul> <li>Tables: Similar to spreadsheets, organized in rows and columns</li> <li>Rows: Individual records (like a person, product, or transaction)</li> <li>Columns: Attributes or fields (like name, price, or date)</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/SQL/01_introduction/#example-database-structure","title":"Example Database Structure","text":"<pre><code>Customers Table:\n+----+----------+-------+-------------------+\n| id | name     | age   | email             |\n+----+----------+-------+-------------------+\n| 1  | John Doe | 30    | john@email.com    |\n| 2  | Jane Smith| 25   | jane@email.com    |\n+----+----------+-------+-------------------+\n\nOrders Table:\n+----+-------------+--------+------------+\n| id | customer_id | amount | order_date |\n+----+-------------+--------+------------+\n| 1  | 1           | 150.00 | 2024-01-15 |\n| 2  | 2           | 200.00 | 2024-01-16 |\n+----+-------------+--------+------------+\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/01_introduction/#types-of-sql-commands","title":"Types of SQL Commands","text":"<p>SQL commands are divided into several categories:</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/01_introduction/#1-data-query-language-dql","title":"1. Data Query Language (DQL)","text":"<ul> <li><code>SELECT</code> - Retrieve data from tables</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/SQL/01_introduction/#2-data-definition-language-ddl","title":"2. Data Definition Language (DDL)","text":"<ul> <li><code>CREATE</code> - Create databases, tables, indexes</li> <li><code>ALTER</code> - Modify database structure</li> <li><code>DROP</code> - Delete databases or tables</li> <li><code>TRUNCATE</code> - Remove all records from a table</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/SQL/01_introduction/#3-data-manipulation-language-dml","title":"3. Data Manipulation Language (DML)","text":"<ul> <li><code>INSERT</code> - Add new records</li> <li><code>UPDATE</code> - Modify existing records</li> <li><code>DELETE</code> - Remove records</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/SQL/01_introduction/#4-data-control-language-dcl","title":"4. Data Control Language (DCL)","text":"<ul> <li><code>GRANT</code> - Give user access privileges</li> <li><code>REVOKE</code> - Remove user access privileges</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/SQL/01_introduction/#5-transaction-control-language-tcl","title":"5. Transaction Control Language (TCL)","text":"<ul> <li><code>COMMIT</code> - Save changes permanently</li> <li><code>ROLLBACK</code> - Undo changes</li> <li><code>SAVEPOINT</code> - Set a point to rollback to</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/SQL/01_introduction/#popular-database-systems","title":"Popular Database Systems","text":"Database Description Use Cases MySQL Open-source, widely used Web applications, small to medium businesses PostgreSQL Advanced open-source Complex queries, data integrity SQLite Lightweight, file-based Mobile apps, small applications SQL Server Microsoft's enterprise solution Enterprise applications, Windows environments Oracle Enterprise-grade Large corporations, mission-critical systems"},{"location":"Data%20Collection%20and%20Visulization/SQL/01_introduction/#your-first-sql-query","title":"Your First SQL Query","text":"<p>Here's the most basic SQL query - selecting all data from a table:</p> <pre><code>SELECT * FROM customers;\n</code></pre> <p>This query: - <code>SELECT</code> - tells the database you want to retrieve data - <code>*</code> - means \"all columns\" - <code>FROM customers</code> - specifies the table name</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/01_introduction/#setting-up-your-environment","title":"Setting Up Your Environment","text":"<p>To practice SQL, you can use:</p> <ol> <li>Online SQL Editors:</li> <li>SQLFiddle</li> <li>DB Fiddle</li> <li> <p>SQL Online IDE</p> </li> <li> <p>Local Installation:</p> </li> <li>MySQL Community Server</li> <li>PostgreSQL</li> <li> <p>SQLite (comes with Python)</p> </li> <li> <p>Cloud Platforms:</p> </li> <li>Google BigQuery</li> <li>Amazon RDS</li> <li>Azure SQL Database</li> </ol>"},{"location":"Data%20Collection%20and%20Visulization/SQL/01_introduction/#next-steps","title":"Next Steps","text":"<p>In the following tutorials, you'll learn:</p> <ol> <li>Basic Queries - SELECT, WHERE, ORDER BY</li> <li>Filtering Data - AND, OR, IN, BETWEEN</li> <li>Aggregate Functions - COUNT, SUM, AVG, MAX, MIN</li> <li>Joins - Combining data from multiple tables</li> <li>Subqueries - Queries within queries</li> <li>Database Design - Creating efficient table structures</li> </ol> <p>Practice Makes Perfect</p> <p>The best way to learn SQL is by writing queries. Try to practice with real datasets and experiment with different commands.</p> <p>Next Tutorial: Basic SQL Queries</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/","title":"Basic SQL Queries","text":"<p>Learn the fundamental SQL commands to retrieve and filter data from databases.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#the-select-statement","title":"The SELECT Statement","text":"<p>The <code>SELECT</code> statement is used to retrieve data from a database. It's the most commonly used SQL command.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#basic-syntax","title":"Basic Syntax","text":"<pre><code>SELECT column1, column2, ...\nFROM table_name;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#example-dataset","title":"Example Dataset","text":"<p>Throughout this tutorial, we'll use a <code>students</code> table:</p> <pre><code>+----+-----------+-----+-------+-------+\n| id | name      | age | grade | city  |\n+----+-----------+-----+-------+-------+\n| 1  | Alice     | 20  | A     | NYC   |\n| 2  | Bob       | 22  | B     | LA    |\n| 3  | Charlie   | 21  | A     | NYC   |\n| 4  | Diana     | 23  | C     | Chicago|\n| 5  | Eva       | 20  | B     | LA    |\n+----+-----------+-----+-------+-------+\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#selecting-specific-columns","title":"Selecting Specific Columns","text":""},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#select-one-column","title":"Select One Column","text":"<pre><code>SELECT name FROM students;\n</code></pre> <p>Result: <pre><code>+-----------+\n| name      |\n+-----------+\n| Alice     |\n| Bob       |\n| Charlie   |\n| Diana     |\n| Eva       |\n+-----------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#select-multiple-columns","title":"Select Multiple Columns","text":"<pre><code>SELECT name, age, grade FROM students;\n</code></pre> <p>Result: <pre><code>+-----------+-----+-------+\n| name      | age | grade |\n+-----------+-----+-------+\n| Alice     | 20  | A     |\n| Bob       | 22  | B     |\n| Charlie   | 21  | A     |\n| Diana     | 23  | C     |\n| Eva       | 20  | B     |\n+-----------+-----+-------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#select-all-columns","title":"Select All Columns","text":"<pre><code>SELECT * FROM students;\n</code></pre> <p>The <code>*</code> (asterisk) selects all columns from the table.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#the-where-clause","title":"The WHERE Clause","text":"<p>The <code>WHERE</code> clause filters records based on specified conditions.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#basic-syntax_1","title":"Basic Syntax","text":"<pre><code>SELECT column1, column2\nFROM table_name\nWHERE condition;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#examples","title":"Examples","text":"<p>Filter by exact match: <pre><code>SELECT * FROM students\nWHERE grade = 'A';\n</code></pre></p> <p>Result: <pre><code>+----+-----------+-----+-------+-------+\n| id | name      | age | grade | city  |\n+----+-----------+-----+-------+-------+\n| 1  | Alice     | 20  | A     | NYC   |\n| 3  | Charlie   | 21  | A     | NYC   |\n+----+-----------+-----+-------+-------+\n</code></pre></p> <p>Filter by numeric comparison: <pre><code>SELECT name, age FROM students\nWHERE age &gt; 21;\n</code></pre></p> <p>Result: <pre><code>+-----------+-----+\n| name      | age |\n+-----------+-----+\n| Bob       | 22  |\n| Diana     | 23  |\n+-----------+-----+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#comparison-operators","title":"Comparison Operators","text":"Operator Description Example <code>=</code> Equal to <code>age = 20</code> <code>!=</code> or <code>&lt;&gt;</code> Not equal to <code>grade != 'C'</code> <code>&gt;</code> Greater than <code>age &gt; 21</code> <code>&lt;</code> Less than <code>age &lt; 22</code> <code>&gt;=</code> Greater than or equal <code>age &gt;= 21</code> <code>&lt;=</code> Less than or equal <code>age &lt;= 22</code>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#logical-operators","title":"Logical Operators","text":""},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#and-operator","title":"AND Operator","text":"<p>Both conditions must be true:</p> <pre><code>SELECT * FROM students\nWHERE grade = 'A' AND city = 'NYC';\n</code></pre> <p>Result: <pre><code>+----+-----------+-----+-------+-------+\n| id | name      | age | grade | city  |\n+----+-----------+-----+-------+-------+\n| 1  | Alice     | 20  | A     | NYC   |\n| 3  | Charlie   | 21  | A     | NYC   |\n+----+-----------+-----+-------+-------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#or-operator","title":"OR Operator","text":"<p>At least one condition must be true:</p> <pre><code>SELECT * FROM students\nWHERE city = 'NYC' OR city = 'LA';\n</code></pre> <p>Result: <pre><code>+----+-----------+-----+-------+-------+\n| id | name      | age | grade | city  |\n+----+-----------+-----+-------+-------+\n| 1  | Alice     | 20  | A     | NYC   |\n| 2  | Bob       | 22  | B     | LA    |\n| 3  | Charlie   | 21  | A     | NYC   |\n| 5  | Eva       | 20  | B     | LA    |\n+----+-----------+-----+-------+-------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#not-operator","title":"NOT Operator","text":"<p>Negates a condition:</p> <pre><code>SELECT * FROM students\nWHERE NOT grade = 'C';\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#order-by-clause","title":"ORDER BY Clause","text":"<p>Sort results in ascending or descending order.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#ascending-order-default","title":"Ascending Order (Default)","text":"<pre><code>SELECT * FROM students\nORDER BY age;\n</code></pre> <p>Result: <pre><code>+----+-----------+-----+-------+-------+\n| id | name      | age | grade | city  |\n+----+-----------+-----+-------+-------+\n| 1  | Alice     | 20  | A     | NYC   |\n| 5  | Eva       | 20  | B     | LA    |\n| 3  | Charlie   | 21  | A     | NYC   |\n| 2  | Bob       | 22  | B     | LA    |\n| 4  | Diana     | 23  | C     | Chicago|\n+----+-----------+-----+-------+-------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#descending-order","title":"Descending Order","text":"<pre><code>SELECT * FROM students\nORDER BY age DESC;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#multiple-columns","title":"Multiple Columns","text":"<pre><code>SELECT * FROM students\nORDER BY grade ASC, age DESC;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#limit-clause","title":"LIMIT Clause","text":"<p>Restrict the number of rows returned:</p> <pre><code>SELECT * FROM students\nLIMIT 3;\n</code></pre> <p>Result: <pre><code>+----+-----------+-----+-------+-------+\n| id | name      | age | grade | city  |\n+----+-----------+-----+-------+-------+\n| 1  | Alice     | 20  | A     | NYC   |\n| 2  | Bob       | 22  | B     | LA    |\n| 3  | Charlie   | 21  | A     | NYC   |\n+----+-----------+-----+-------+-------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#distinct-keyword","title":"DISTINCT Keyword","text":"<p>Remove duplicate values:</p> <pre><code>SELECT DISTINCT city FROM students;\n</code></pre> <p>Result: <pre><code>+---------+\n| city    |\n+---------+\n| NYC     |\n| LA      |\n| Chicago |\n+---------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/02_basic_queries/#practice-exercises","title":"Practice Exercises","text":"<p>Try these queries on your own:</p> <ol> <li>Select all students from NYC</li> <li>Find students aged 20 or 21</li> <li>Get names of students with grade A, sorted alphabetically</li> <li>Find the 2 oldest students</li> <li>List all unique grades in the table</li> </ol> Solutions <pre><code>-- 1. Students from NYC\nSELECT * FROM students WHERE city = 'NYC';\n\n-- 2. Students aged 20 or 21\nSELECT * FROM students WHERE age = 20 OR age = 21;\n\n-- 3. Grade A students, sorted by name\nSELECT name FROM students WHERE grade = 'A' ORDER BY name;\n\n-- 4. Two oldest students\nSELECT * FROM students ORDER BY age DESC LIMIT 2;\n\n-- 5. Unique grades\nSELECT DISTINCT grade FROM students;\n</code></pre> <p>SQL is Case-Insensitive</p> <p>SQL keywords are not case-sensitive. <code>SELECT</code>, <code>select</code>, and <code>SeLeCt</code> all work the same. However, it's a good practice to write keywords in UPPERCASE for readability.</p> <p>Previous: Introduction to SQL | Next: Filtering and Pattern Matching</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/","title":"Filtering and Pattern Matching","text":"<p>Learn advanced filtering techniques including pattern matching, ranges, and NULL handling.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#in-operator","title":"IN Operator","text":"<p>The <code>IN</code> operator allows you to specify multiple values in a WHERE clause.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#syntax","title":"Syntax","text":"<pre><code>SELECT column1, column2\nFROM table_name\nWHERE column_name IN (value1, value2, ...);\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#example","title":"Example","text":"<p>Instead of writing: <pre><code>SELECT * FROM students\nWHERE city = 'NYC' OR city = 'LA' OR city = 'Chicago';\n</code></pre></p> <p>You can write: <pre><code>SELECT * FROM students\nWHERE city IN ('NYC', 'LA', 'Chicago');\n</code></pre></p> <p>Using NOT IN: <pre><code>SELECT * FROM students\nWHERE grade NOT IN ('C', 'F');\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#between-operator","title":"BETWEEN Operator","text":"<p>The <code>BETWEEN</code> operator selects values within a given range (inclusive).</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#syntax_1","title":"Syntax","text":"<pre><code>SELECT column1, column2\nFROM table_name\nWHERE column_name BETWEEN value1 AND value2;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#examples","title":"Examples","text":"<p>Numeric ranges: <pre><code>SELECT name, age FROM students\nWHERE age BETWEEN 20 AND 22;\n</code></pre></p> <p>Result: <pre><code>+-----------+-----+\n| name      | age |\n+-----------+-----+\n| Alice     | 20  |\n| Bob       | 22  |\n| Charlie   | 21  |\n| Eva       | 20  |\n+-----------+-----+\n</code></pre></p> <p>Date ranges: <pre><code>SELECT * FROM orders\nWHERE order_date BETWEEN '2024-01-01' AND '2024-12-31';\n</code></pre></p> <p>NOT BETWEEN: <pre><code>SELECT name, age FROM students\nWHERE age NOT BETWEEN 20 AND 21;\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#like-operator","title":"LIKE Operator","text":"<p>The <code>LIKE</code> operator searches for a specified pattern in a column.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#wildcards","title":"Wildcards","text":"Wildcard Description Example <code>%</code> Represents zero or more characters <code>'a%'</code> finds values starting with \"a\" <code>_</code> Represents a single character <code>'a_'</code> finds two-character values starting with \"a\""},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#examples_1","title":"Examples","text":"<p>Starts with 'A': <pre><code>SELECT name FROM students\nWHERE name LIKE 'A%';\n</code></pre></p> <p>Result: <pre><code>+-----------+\n| name      |\n+-----------+\n| Alice     |\n+-----------+\n</code></pre></p> <p>Ends with 'e': <pre><code>SELECT name FROM students\nWHERE name LIKE '%e';\n</code></pre></p> <p>Result: <pre><code>+-----------+\n| name      |\n+-----------+\n| Alice     |\n| Charlie   |\n+-----------+\n</code></pre></p> <p>Contains 'a' anywhere: <pre><code>SELECT name FROM students\nWHERE name LIKE '%a%';\n</code></pre></p> <p>Second letter is 'i': <pre><code>SELECT name FROM students\nWHERE name LIKE '_i%';\n</code></pre></p> <p>Result: <pre><code>+-----------+\n| name      |\n+-----------+\n| Diana     |\n+-----------+\n</code></pre></p> <p>Exactly 3 characters: <pre><code>SELECT name FROM students\nWHERE name LIKE '___';\n</code></pre></p> <p>Result: <pre><code>+-----------+\n| name      |\n+-----------+\n| Bob       |\n| Eva       |\n+-----------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#case-insensitive-searching","title":"Case-Insensitive Searching","text":"<p>Different databases handle case sensitivity differently:</p> <p>MySQL (case-insensitive by default): <pre><code>SELECT * FROM students WHERE name LIKE 'alice';\n</code></pre></p> <p>PostgreSQL (case-sensitive, use ILIKE): <pre><code>SELECT * FROM students WHERE name ILIKE 'alice';\n</code></pre></p> <p>SQL Server (use UPPER or LOWER): <pre><code>SELECT * FROM students WHERE UPPER(name) LIKE 'ALICE';\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#null-values","title":"NULL Values","text":"<p><code>NULL</code> represents missing or unknown data. It's different from zero or empty string.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#is-null","title":"IS NULL","text":"<p>Find records with NULL values:</p> <pre><code>SELECT * FROM students\nWHERE email IS NULL;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#is-not-null","title":"IS NOT NULL","text":"<p>Find records without NULL values:</p> <pre><code>SELECT * FROM students\nWHERE email IS NOT NULL;\n</code></pre> <p>NULL Comparison</p> <p>You cannot use <code>=</code> or <code>!=</code> with NULL. Always use <code>IS NULL</code> or <code>IS NOT NULL</code>.</p> <pre><code>-- WRONG\nWHERE email = NULL\n\n-- CORRECT\nWHERE email IS NULL\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#combining-multiple-conditions","title":"Combining Multiple Conditions","text":"<p>You can combine different filtering techniques:</p> <pre><code>SELECT name, age, city FROM students\nWHERE (grade IN ('A', 'B'))\n  AND (age BETWEEN 20 AND 22)\n  AND (city LIKE '%C%')\n  AND (email IS NOT NULL);\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#practical-examples","title":"Practical Examples","text":""},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#example-1-find-students-whose-name-starts-with-c-or-d-and-are-older-than-20","title":"Example 1: Find students whose name starts with 'C' or 'D' and are older than 20","text":"<pre><code>SELECT * FROM students\nWHERE name LIKE 'C%' OR name LIKE 'D%'\n  AND age &gt; 20;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#example-2-find-students-not-from-nyc-or-la-with-grades-a-or-b","title":"Example 2: Find students not from NYC or LA with grades A or B","text":"<pre><code>SELECT name, city, grade FROM students\nWHERE city NOT IN ('NYC', 'LA')\n  AND grade IN ('A', 'B');\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#example-3-find-students-with-names-containing-ar-and-age-between-18-25","title":"Example 3: Find students with names containing 'ar' and age between 18-25","text":"<pre><code>SELECT * FROM students\nWHERE name LIKE '%ar%'\n  AND age BETWEEN 18 AND 25;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#pattern-matching-best-practices","title":"Pattern Matching Best Practices","text":"<ol> <li>Use specific patterns - <code>name LIKE 'A%'</code> is faster than <code>name LIKE '%A%'</code></li> <li>Avoid leading wildcards - <code>LIKE '%text'</code> can be slow on large tables</li> <li>Consider indexes - Columns frequently used in LIKE should be indexed</li> <li>Use exact matches when possible - <code>=</code> is faster than <code>LIKE</code></li> </ol>"},{"location":"Data%20Collection%20and%20Visulization/SQL/03_filtering/#practice-exercises","title":"Practice Exercises","text":"<p>Using a hypothetical <code>employees</code> table:</p> <pre><code>+----+-----------+-----------+--------+------------+\n| id | name      | department| salary | hire_date  |\n+----+-----------+-----------+--------+------------+\n| 1  | John Doe  | Sales     | 50000  | 2020-01-15 |\n| 2  | Jane Smith| IT        | 75000  | 2019-06-20 |\n| 3  | Bob Wilson| Sales     | 55000  | 2021-03-10 |\n| 4  | Alice Lee | HR        | NULL   | 2022-01-05 |\n+----+-----------+-----------+--------+------------+\n</code></pre> <p>Try these queries:</p> <ol> <li>Find employees in Sales or IT departments</li> <li>Find employees hired between 2020 and 2022</li> <li>Find employees whose name contains 'son'</li> <li>Find employees with no salary information</li> <li>Find employees in Sales with salary above 52000</li> </ol> Solutions <pre><code>-- 1. Employees in Sales or IT\nSELECT * FROM employees\nWHERE department IN ('Sales', 'IT');\n\n-- 2. Hired between 2020 and 2022\nSELECT * FROM employees\nWHERE hire_date BETWEEN '2020-01-01' AND '2022-12-31';\n\n-- 3. Name contains 'son'\nSELECT * FROM employees\nWHERE name LIKE '%son%';\n\n-- 4. No salary information\nSELECT * FROM employees\nWHERE salary IS NULL;\n\n-- 5. Sales employees with salary &gt; 52000\nSELECT * FROM employees\nWHERE department = 'Sales' AND salary &gt; 52000;\n</code></pre> <p>Previous: Basic SQL Queries | Next: Aggregate Functions</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/","title":"Aggregate Functions","text":"<p>Learn how to perform calculations on data sets using SQL's powerful aggregate functions.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#what-are-aggregate-functions","title":"What are Aggregate Functions?","text":"<p>Aggregate functions perform calculations on a set of values and return a single value. They're essential for data analysis and reporting.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#common-aggregate-functions","title":"Common Aggregate Functions","text":"Function Description Example <code>COUNT()</code> Counts the number of rows <code>COUNT(*)</code> <code>SUM()</code> Calculates the sum of values <code>SUM(salary)</code> <code>AVG()</code> Calculates the average <code>AVG(age)</code> <code>MAX()</code> Returns the maximum value <code>MAX(price)</code> <code>MIN()</code> Returns the minimum value <code>MIN(price)</code>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#sample-dataset","title":"Sample Dataset","text":"<p>We'll use this <code>employees</code> table:</p> <pre><code>+----+-----------+-----------+--------+-----+\n| id | name      | department| salary | age |\n+----+-----------+-----------+--------+-----+\n| 1  | Alice     | IT        | 75000  | 28  |\n| 2  | Bob       | Sales     | 50000  | 32  |\n| 3  | Charlie   | IT        | 80000  | 35  |\n| 4  | Diana     | HR        | 60000  | 29  |\n| 5  | Eva       | Sales     | 55000  | 26  |\n| 6  | Frank     | IT        | 70000  | 31  |\n+----+-----------+-----------+--------+-----+\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#count-function","title":"COUNT Function","text":"<p>Counts the number of rows.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#count-all-rows","title":"Count All Rows","text":"<pre><code>SELECT COUNT(*) AS total_employees\nFROM employees;\n</code></pre> <p>Result: <pre><code>+------------------+\n| total_employees  |\n+------------------+\n| 6                |\n+------------------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#count-specific-column","title":"Count Specific Column","text":"<pre><code>SELECT COUNT(department) AS dept_count\nFROM employees;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#count-distinct-values","title":"Count Distinct Values","text":"<pre><code>SELECT COUNT(DISTINCT department) AS unique_departments\nFROM employees;\n</code></pre> <p>Result: <pre><code>+--------------------+\n| unique_departments |\n+--------------------+\n| 3                  |\n+--------------------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#sum-function","title":"SUM Function","text":"<p>Calculates the total sum of a numeric column.</p> <pre><code>SELECT SUM(salary) AS total_payroll\nFROM employees;\n</code></pre> <p>Result: <pre><code>+---------------+\n| total_payroll |\n+---------------+\n| 390000        |\n+---------------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#sum-with-where","title":"SUM with WHERE","text":"<pre><code>SELECT SUM(salary) AS it_payroll\nFROM employees\nWHERE department = 'IT';\n</code></pre> <p>Result: <pre><code>+-------------+\n| it_payroll  |\n+-------------+\n| 225000      |\n+-------------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#avg-function","title":"AVG Function","text":"<p>Calculates the average value.</p> <pre><code>SELECT AVG(salary) AS average_salary\nFROM employees;\n</code></pre> <p>Result: <pre><code>+----------------+\n| average_salary |\n+----------------+\n| 65000.00       |\n+----------------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#rounding-averages","title":"Rounding Averages","text":"<pre><code>SELECT ROUND(AVG(salary), 2) AS average_salary\nFROM employees;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#max-and-min-functions","title":"MAX and MIN Functions","text":"<p>Find the highest and lowest values.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#maximum-value","title":"Maximum Value","text":"<pre><code>SELECT MAX(salary) AS highest_salary\nFROM employees;\n</code></pre> <p>Result: <pre><code>+----------------+\n| highest_salary |\n+----------------+\n| 80000          |\n+----------------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#minimum-value","title":"Minimum Value","text":"<pre><code>SELECT MIN(salary) AS lowest_salary\nFROM employees;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#multiple-aggregates-in-one-query","title":"Multiple Aggregates in One Query","text":"<pre><code>SELECT\n    MAX(salary) AS highest,\n    MIN(salary) AS lowest,\n    AVG(salary) AS average\nFROM employees;\n</code></pre> <p>Result: <pre><code>+---------+--------+----------+\n| highest | lowest | average  |\n+---------+--------+----------+\n| 80000   | 50000  | 65000.00 |\n+---------+--------+----------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#group-by-clause","title":"GROUP BY Clause","text":"<p>Group rows with the same values and apply aggregate functions to each group.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#basic-group-by","title":"Basic GROUP BY","text":"<pre><code>SELECT department, COUNT(*) AS employee_count\nFROM employees\nGROUP BY department;\n</code></pre> <p>Result: <pre><code>+-----------+----------------+\n| department| employee_count |\n+-----------+----------------+\n| IT        | 3              |\n| Sales     | 2              |\n| HR        | 1              |\n+-----------+----------------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#multiple-aggregates-with-group-by","title":"Multiple Aggregates with GROUP BY","text":"<pre><code>SELECT\n    department,\n    COUNT(*) AS num_employees,\n    AVG(salary) AS avg_salary,\n    MAX(salary) AS max_salary,\n    MIN(salary) AS min_salary\nFROM employees\nGROUP BY department;\n</code></pre> <p>Result: <pre><code>+-----------+---------------+------------+------------+------------+\n| department| num_employees | avg_salary | max_salary | min_salary |\n+-----------+---------------+------------+------------+------------+\n| IT        | 3             | 75000.00   | 80000      | 70000      |\n| Sales     | 2             | 52500.00   | 55000      | 50000      |\n| HR        | 1             | 60000.00   | 60000      | 60000      |\n+-----------+---------------+------------+------------+------------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#group-by-multiple-columns","title":"GROUP BY Multiple Columns","text":"<pre><code>SELECT\n    department,\n    CASE\n        WHEN age &lt; 30 THEN 'Under 30'\n        ELSE '30 and over'\n    END AS age_group,\n    COUNT(*) AS count\nFROM employees\nGROUP BY department, age_group;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#having-clause","title":"HAVING Clause","text":"<p>Filter groups based on aggregate values (WHERE filters individual rows, HAVING filters groups).</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#basic-having","title":"Basic HAVING","text":"<pre><code>SELECT department, AVG(salary) AS avg_salary\nFROM employees\nGROUP BY department\nHAVING AVG(salary) &gt; 60000;\n</code></pre> <p>Result: <pre><code>+-----------+------------+\n| department| avg_salary |\n+-----------+------------+\n| IT        | 75000.00   |\n+-----------+------------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#having-with-count","title":"HAVING with COUNT","text":"<pre><code>SELECT department, COUNT(*) AS employee_count\nFROM employees\nGROUP BY department\nHAVING COUNT(*) &gt; 1;\n</code></pre> <p>Result: <pre><code>+-----------+----------------+\n| department| employee_count |\n+-----------+----------------+\n| IT        | 3              |\n| Sales     | 2              |\n+-----------+----------------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#where-vs-having","title":"WHERE vs HAVING","text":"<ul> <li>WHERE: Filters rows BEFORE grouping</li> <li>HAVING: Filters groups AFTER grouping</li> </ul> <pre><code>-- Filter individual rows, then group\nSELECT department, AVG(salary) AS avg_salary\nFROM employees\nWHERE age &gt; 28\nGROUP BY department\nHAVING AVG(salary) &gt; 65000;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#combining-everything","title":"Combining Everything","text":"<p>A comprehensive query using multiple concepts:</p> <pre><code>SELECT\n    department,\n    COUNT(*) AS total_employees,\n    ROUND(AVG(salary), 2) AS avg_salary,\n    MIN(age) AS youngest,\n    MAX(age) AS oldest\nFROM employees\nWHERE salary &gt;= 50000\nGROUP BY department\nHAVING COUNT(*) &gt;= 2\nORDER BY avg_salary DESC;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#common-patterns","title":"Common Patterns","text":""},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#1-find-departments-with-above-average-salaries","title":"1. Find departments with above-average salaries","text":"<pre><code>SELECT department, AVG(salary) AS dept_avg\nFROM employees\nGROUP BY department\nHAVING AVG(salary) &gt; (SELECT AVG(salary) FROM employees);\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#2-count-employees-by-age-range","title":"2. Count employees by age range","text":"<pre><code>SELECT\n    CASE\n        WHEN age &lt; 30 THEN '20-29'\n        WHEN age &lt; 40 THEN '30-39'\n        ELSE '40+'\n    END AS age_range,\n    COUNT(*) AS count,\n    AVG(salary) AS avg_salary\nFROM employees\nGROUP BY age_range\nORDER BY age_range;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#3-top-n-by-group","title":"3. Top N by group","text":"<pre><code>SELECT department, MAX(salary) AS top_salary\nFROM employees\nGROUP BY department\nORDER BY top_salary DESC\nLIMIT 3;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/04_aggregate_functions/#practice-exercises","title":"Practice Exercises","text":"<p>Using this <code>sales</code> table:</p> <pre><code>+----+-----------+--------+--------+------------+\n| id | product   | region | amount | sale_date  |\n+----+-----------+--------+--------+------------+\n| 1  | Laptop    | East   | 1200   | 2024-01-15 |\n| 2  | Mouse     | West   | 25     | 2024-01-16 |\n| 3  | Laptop    | East   | 1200   | 2024-01-17 |\n| 4  | Keyboard  | West   | 75     | 2024-01-18 |\n| 5  | Monitor   | East   | 300    | 2024-01-19 |\n| 6  | Mouse     | East   | 25     | 2024-01-20 |\n+----+-----------+--------+--------+------------+\n</code></pre> <p>Try these queries:</p> <ol> <li>Calculate total sales amount</li> <li>Find average sale amount by region</li> <li>Count number of sales per product</li> <li>Find products with more than 1 sale</li> <li>Find regions with total sales over $1000</li> </ol> Solutions <pre><code>-- 1. Total sales amount\nSELECT SUM(amount) AS total_sales\nFROM sales;\n\n-- 2. Average sale by region\nSELECT region, AVG(amount) AS avg_sale\nFROM sales\nGROUP BY region;\n\n-- 3. Sales count per product\nSELECT product, COUNT(*) AS sale_count\nFROM sales\nGROUP BY product\nORDER BY sale_count DESC;\n\n-- 4. Products with more than 1 sale\nSELECT product, COUNT(*) AS sale_count\nFROM sales\nGROUP BY product\nHAVING COUNT(*) &gt; 1;\n\n-- 5. Regions with sales over $1000\nSELECT region, SUM(amount) AS total_sales\nFROM sales\nGROUP BY region\nHAVING SUM(amount) &gt; 1000;\n</code></pre> <p>Performance Tip</p> <p>Use aggregate functions wisely. On large tables, they can be expensive. Consider using indexes on grouped columns for better performance.</p> <p>Previous: Filtering and Pattern Matching | Next: SQL Joins</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/","title":"SQL Joins","text":"<p>Learn how to combine data from multiple tables using different types of joins.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#what-are-joins","title":"What are Joins?","text":"<p>Joins are used to combine rows from two or more tables based on a related column between them. This is one of the most powerful features of SQL.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#sample-tables","title":"Sample Tables","text":"<p>We'll use these two tables throughout this tutorial:</p> <p>customers table: <pre><code>+----+-----------+-------------+\n| id | name      | city        |\n+----+-----------+-------------+\n| 1  | Alice     | New York    |\n| 2  | Bob       | Los Angeles |\n| 3  | Charlie   | Chicago     |\n| 4  | Diana     | Houston     |\n+----+-----------+-------------+\n</code></pre></p> <p>orders table: <pre><code>+----+-------------+--------+------------+\n| id | customer_id | amount | order_date |\n+----+-------------+--------+------------+\n| 1  | 1           | 250    | 2024-01-15 |\n| 2  | 1           | 180    | 2024-01-20 |\n| 3  | 2           | 320    | 2024-01-18 |\n| 4  | 5           | 150    | 2024-01-22 |\n+----+-------------+--------+------------+\n</code></pre></p> <p>Note: customer_id 5 doesn't exist in customers table, and customers 3 and 4 have no orders.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#inner-join","title":"INNER JOIN","text":"<p>Returns only matching rows from both tables.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#syntax","title":"Syntax","text":"<pre><code>SELECT columns\nFROM table1\nINNER JOIN table2\nON table1.column = table2.column;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#example","title":"Example","text":"<pre><code>SELECT\n    customers.name,\n    customers.city,\n    orders.amount,\n    orders.order_date\nFROM customers\nINNER JOIN orders\nON customers.id = orders.customer_id;\n</code></pre> <p>Result: <pre><code>+-----------+-------------+--------+------------+\n| name      | city        | amount | order_date |\n+-----------+-------------+--------+------------+\n| Alice     | New York    | 250    | 2024-01-15 |\n| Alice     | New York    | 180    | 2024-01-20 |\n| Bob       | Los Angeles | 320    | 2024-01-18 |\n+-----------+-------------+--------+------------+\n</code></pre></p> <p>Note: Charlie and Diana are excluded (no orders), and order with customer_id 5 is excluded (no matching customer).</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#using-table-aliases","title":"Using Table Aliases","text":"<pre><code>SELECT\n    c.name,\n    c.city,\n    o.amount\nFROM customers c\nINNER JOIN orders o\nON c.id = o.customer_id;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#left-join-left-outer-join","title":"LEFT JOIN (LEFT OUTER JOIN)","text":"<p>Returns all rows from the left table and matching rows from the right table. NULL for non-matching rows.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#syntax_1","title":"Syntax","text":"<pre><code>SELECT columns\nFROM table1\nLEFT JOIN table2\nON table1.column = table2.column;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#example_1","title":"Example","text":"<pre><code>SELECT\n    c.name,\n    c.city,\n    o.amount,\n    o.order_date\nFROM customers c\nLEFT JOIN orders o\nON c.id = o.customer_id;\n</code></pre> <p>Result: <pre><code>+-----------+-------------+--------+------------+\n| name      | city        | amount | order_date |\n+-----------+-------------+--------+------------+\n| Alice     | New York    | 250    | 2024-01-15 |\n| Alice     | New York    | 180    | 2024-01-20 |\n| Bob       | Los Angeles | 320    | 2024-01-18 |\n| Charlie   | Chicago     | NULL   | NULL       |\n| Diana     | Houston     | NULL   | NULL       |\n+-----------+-------------+--------+------------+\n</code></pre></p> <p>All customers are included, even those without orders.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#finding-customers-without-orders","title":"Finding Customers Without Orders","text":"<pre><code>SELECT c.name, c.city\nFROM customers c\nLEFT JOIN orders o\nON c.id = o.customer_id\nWHERE o.id IS NULL;\n</code></pre> <p>Result: <pre><code>+-----------+---------+\n| name      | city    |\n+-----------+---------+\n| Charlie   | Chicago |\n| Diana     | Houston |\n+-----------+---------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#right-join-right-outer-join","title":"RIGHT JOIN (RIGHT OUTER JOIN)","text":"<p>Returns all rows from the right table and matching rows from the left table.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#example_2","title":"Example","text":"<pre><code>SELECT\n    c.name,\n    o.amount,\n    o.order_date\nFROM customers c\nRIGHT JOIN orders o\nON c.id = o.customer_id;\n</code></pre> <p>Result: <pre><code>+-----------+--------+------------+\n| name      | amount | order_date |\n+-----------+--------+------------+\n| Alice     | 250    | 2024-01-15 |\n| Alice     | 180    | 2024-01-20 |\n| Bob       | 320    | 2024-01-18 |\n| NULL      | 150    | 2024-01-22 |\n+-----------+--------+------------+\n</code></pre></p> <p>All orders included, even the one with non-existent customer_id 5.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#full-outer-join","title":"FULL OUTER JOIN","text":"<p>Returns all rows when there's a match in either table.</p> <p>Database Support</p> <p>Not all databases support FULL OUTER JOIN (e.g., MySQL doesn't). You can simulate it using UNION of LEFT and RIGHT joins.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#syntax_2","title":"Syntax","text":"<pre><code>SELECT columns\nFROM table1\nFULL OUTER JOIN table2\nON table1.column = table2.column;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#simulating-full-outer-join-in-mysql","title":"Simulating FULL OUTER JOIN in MySQL","text":"<pre><code>SELECT c.name, o.amount\nFROM customers c\nLEFT JOIN orders o ON c.id = o.customer_id\n\nUNION\n\nSELECT c.name, o.amount\nFROM customers c\nRIGHT JOIN orders o ON c.id = o.customer_id;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#cross-join","title":"CROSS JOIN","text":"<p>Returns the Cartesian product of both tables (all possible combinations).</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#example_3","title":"Example","text":"<pre><code>SELECT c.name, o.amount\nFROM customers c\nCROSS JOIN orders o;\n</code></pre> <p>This creates 16 rows (4 customers \u00d7 4 orders).</p> <p>Use with Caution</p> <p>CROSS JOIN can produce very large result sets. Use it only when you need all combinations.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#self-join","title":"SELF JOIN","text":"<p>A table joined with itself.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#example-employee-manager-relationship","title":"Example: Employee-Manager Relationship","text":"<p>employees table: <pre><code>+----+-----------+------------+\n| id | name      | manager_id |\n+----+-----------+------------+\n| 1  | Alice     | NULL       |\n| 2  | Bob       | 1          |\n| 3  | Charlie   | 1          |\n| 4  | Diana     | 2          |\n+----+-----------+------------+\n</code></pre></p> <pre><code>SELECT\n    e.name AS employee,\n    m.name AS manager\nFROM employees e\nLEFT JOIN employees m\nON e.manager_id = m.id;\n</code></pre> <p>Result: <pre><code>+-----------+---------+\n| employee  | manager |\n+-----------+---------+\n| Alice     | NULL    |\n| Bob       | Alice   |\n| Charlie   | Alice   |\n| Diana     | Bob     |\n+-----------+---------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#multiple-joins","title":"Multiple Joins","text":"<p>Join more than two tables together.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#example-with-three-tables","title":"Example with Three Tables","text":"<p>products table: <pre><code>+----+----------+-------+\n| id | name     | price |\n+----+----------+-------+\n| 1  | Laptop   | 1000  |\n| 2  | Mouse    | 25    |\n+----+----------+-------+\n</code></pre></p> <p>order_items table: <pre><code>+----+----------+------------+----------+\n| id | order_id | product_id | quantity |\n+----+----------+------------+----------+\n| 1  | 1        | 1          | 2        |\n| 2  | 1        | 2          | 1        |\n| 3  | 2        | 1          | 1        |\n+----+----------+------------+----------+\n</code></pre></p> <pre><code>SELECT\n    c.name AS customer,\n    p.name AS product,\n    oi.quantity,\n    p.price,\n    (oi.quantity * p.price) AS total\nFROM customers c\nINNER JOIN orders o ON c.id = o.customer_id\nINNER JOIN order_items oi ON o.id = oi.order_id\nINNER JOIN products p ON oi.product_id = p.id;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#join-with-aggregation","title":"Join with Aggregation","text":"<p>Combine joins with aggregate functions.</p> <pre><code>SELECT\n    c.name,\n    COUNT(o.id) AS order_count,\n    COALESCE(SUM(o.amount), 0) AS total_spent\nFROM customers c\nLEFT JOIN orders o ON c.id = o.customer_id\nGROUP BY c.id, c.name\nORDER BY total_spent DESC;\n</code></pre> <p>Result: <pre><code>+-----------+-------------+-------------+\n| name      | order_count | total_spent |\n+-----------+-------------+-------------+\n| Alice     | 2           | 430         |\n| Bob       | 1           | 320         |\n| Charlie   | 0           | 0           |\n| Diana     | 0           | 0           |\n+-----------+-------------+-------------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#practice-exercises","title":"Practice Exercises","text":"<p>Given these tables:</p> <p>authors: <pre><code>+----+-----------+\n| id | name      |\n+----+-----------+\n| 1  | J.K. Rowling |\n| 2  | George Orwell |\n| 3  | Jane Austen |\n+----+-----------+\n</code></pre></p> <p>books: <pre><code>+----+-----------+-------------+------+\n| id | title     | author_id   | year |\n+----+-----------+-------------+------+\n| 1  | Book A    | 1           | 1997 |\n| 2  | Book B    | 1           | 1999 |\n| 3  | Book C    | 2           | 1949 |\n+----+-----------+-------------+------+\n</code></pre></p> <p>Try these queries:</p> <ol> <li>List all books with their author names</li> <li>Find authors who have written more than one book</li> <li>List all authors, including those without books</li> <li>Find authors who haven't written any books</li> <li>Count the number of books per author</li> </ol> Solutions <pre><code>-- 1. Books with author names\nSELECT b.title, a.name AS author, b.year\nFROM books b\nINNER JOIN authors a ON b.author_id = a.id;\n\n-- 2. Authors with more than one book\nSELECT a.name, COUNT(b.id) AS book_count\nFROM authors a\nINNER JOIN books b ON a.id = b.author_id\nGROUP BY a.id, a.name\nHAVING COUNT(b.id) &gt; 1;\n\n-- 3. All authors including those without books\nSELECT a.name, b.title\nFROM authors a\nLEFT JOIN books b ON a.id = b.author_id;\n\n-- 4. Authors without books\nSELECT a.name\nFROM authors a\nLEFT JOIN books b ON a.id = b.author_id\nWHERE b.id IS NULL;\n\n-- 5. Book count per author\nSELECT a.name, COUNT(b.id) AS book_count\nFROM authors a\nLEFT JOIN books b ON a.id = b.author_id\nGROUP BY a.id, a.name\nORDER BY book_count DESC;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/05_joins/#visual-join-guide","title":"Visual Join Guide","text":"<pre><code>INNER JOIN: Only matching rows\n[A] \u2229 [B]\n\nLEFT JOIN: All from left + matching from right\n[A] + [A \u2229 B]\n\nRIGHT JOIN: All from right + matching from left\n[B] + [A \u2229 B]\n\nFULL OUTER JOIN: Everything\n[A] + [B]\n\nCROSS JOIN: All combinations\n[A] \u00d7 [B]\n</code></pre> <p>Join Performance</p> <ul> <li>Always join on indexed columns</li> <li>Use INNER JOIN when possible (faster than OUTER JOINs)</li> <li>Filter early using WHERE clauses</li> <li>Consider using subqueries for complex joins</li> </ul> <p>Previous: Aggregate Functions | Next: Subqueries</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/","title":"Subqueries","text":"<p>Learn how to use queries within queries to solve complex data problems.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#what-is-a-subquery","title":"What is a Subquery?","text":"<p>A subquery (also called an inner query or nested query) is a query within another SQL query. Subqueries can be used in SELECT, FROM, WHERE, and HAVING clauses.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#sample-data","title":"Sample Data","text":"<p>employees table: <pre><code>+----+-----------+-----------+--------+\n| id | name      | department| salary |\n+----+-----------+-----------+--------+\n| 1  | Alice     | IT        | 75000  |\n| 2  | Bob       | Sales     | 50000  |\n| 3  | Charlie   | IT        | 80000  |\n| 4  | Diana     | HR        | 60000  |\n| 5  | Eva       | Sales     | 55000  |\n+----+-----------+-----------+--------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#subquery-in-where-clause","title":"Subquery in WHERE Clause","text":"<p>The most common use of subqueries.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#find-employees-earning-more-than-the-average-salary","title":"Find employees earning more than the average salary","text":"<pre><code>SELECT name, salary\nFROM employees\nWHERE salary &gt; (SELECT AVG(salary) FROM employees);\n</code></pre> <p>Result: <pre><code>+-----------+--------+\n| name      | salary |\n+-----------+--------+\n| Alice     | 75000  |\n| Charlie   | 80000  |\n+-----------+--------+\n</code></pre></p> <p>How it works: 1. Inner query calculates: <code>AVG(salary) = 64000</code> 2. Outer query uses this value: <code>WHERE salary &gt; 64000</code></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#find-the-highest-paid-employee","title":"Find the highest-paid employee","text":"<pre><code>SELECT name, salary\nFROM employees\nWHERE salary = (SELECT MAX(salary) FROM employees);\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#subquery-with-in-operator","title":"Subquery with IN Operator","text":"<p>Use subqueries that return multiple values with IN.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#find-employees-in-departments-that-have-more-than-one-employee","title":"Find employees in departments that have more than one employee","text":"<pre><code>SELECT name, department\nFROM employees\nWHERE department IN (\n    SELECT department\n    FROM employees\n    GROUP BY department\n    HAVING COUNT(*) &gt; 1\n);\n</code></pre> <p>Result: <pre><code>+-----------+-----------+\n| name      | department|\n+-----------+-----------+\n| Alice     | IT        |\n| Bob       | Sales     |\n| Charlie   | IT        |\n| Eva       | Sales     |\n+-----------+-----------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#find-employees-in-it-or-sales-simple-example","title":"Find employees in IT or Sales (simple example)","text":"<pre><code>SELECT name, salary\nFROM employees\nWHERE department IN (\n    SELECT DISTINCT department\n    FROM employees\n    WHERE department IN ('IT', 'Sales')\n);\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#subquery-in-from-clause","title":"Subquery in FROM Clause","text":"<p>Use a subquery as a temporary table (also called a derived table).</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#calculate-department-statistics","title":"Calculate department statistics","text":"<pre><code>SELECT\n    dept_stats.department,\n    dept_stats.avg_salary,\n    dept_stats.employee_count\nFROM (\n    SELECT\n        department,\n        AVG(salary) AS avg_salary,\n        COUNT(*) AS employee_count\n    FROM employees\n    GROUP BY department\n) AS dept_stats\nWHERE dept_stats.avg_salary &gt; 60000;\n</code></pre> <p>Result: <pre><code>+-----------+------------+----------------+\n| department| avg_salary | employee_count |\n+-----------+------------+----------------+\n| IT        | 77500      | 2              |\n+-----------+------------+----------------+\n</code></pre></p> <p>Alias Required</p> <p>Subqueries in FROM clause must have an alias (e.g., <code>AS dept_stats</code>).</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#subquery-in-select-clause","title":"Subquery in SELECT Clause","text":"<p>Return a single value in the SELECT list.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#show-each-employee-with-the-department-average","title":"Show each employee with the department average","text":"<pre><code>SELECT\n    name,\n    salary,\n    department,\n    (SELECT AVG(salary)\n     FROM employees e2\n     WHERE e2.department = e1.department) AS dept_avg\nFROM employees e1;\n</code></pre> <p>Result: <pre><code>+-----------+--------+-----------+----------+\n| name      | salary | department| dept_avg |\n+-----------+--------+-----------+----------+\n| Alice     | 75000  | IT        | 77500    |\n| Bob       | 50000  | Sales     | 52500    |\n| Charlie   | 80000  | IT        | 77500    |\n| Diana     | 60000  | HR        | 60000    |\n| Eva       | 55000  | Sales     | 52500    |\n+-----------+--------+-----------+----------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#correlated-subqueries","title":"Correlated Subqueries","text":"<p>A subquery that references columns from the outer query.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#find-employees-earning-more-than-their-department-average","title":"Find employees earning more than their department average","text":"<pre><code>SELECT name, department, salary\nFROM employees e1\nWHERE salary &gt; (\n    SELECT AVG(salary)\n    FROM employees e2\n    WHERE e2.department = e1.department\n);\n</code></pre> <p>Result: <pre><code>+-----------+-----------+--------+\n| name      | department| salary |\n+-----------+-----------+--------+\n| Charlie   | IT        | 80000  |\n| Eva       | Sales     | 55000  |\n+-----------+-----------+--------+\n</code></pre></p> <p>How it works: - For each row in outer query, inner query executes - Inner query uses the current row's department - Compares the salary with that department's average</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#exists-operator","title":"EXISTS Operator","text":"<p>Tests for the existence of rows in a subquery. Returns TRUE if subquery returns any rows.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#find-departments-that-have-employees","title":"Find departments that have employees","text":"<pre><code>SELECT DISTINCT department\nFROM employees e1\nWHERE EXISTS (\n    SELECT 1\n    FROM employees e2\n    WHERE e2.department = e1.department\n    AND e2.salary &gt; 70000\n);\n</code></pre> <p>Result: <pre><code>+-----------+\n| department|\n+-----------+\n| IT        |\n+-----------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#not-exists-example","title":"NOT EXISTS Example","text":"<p>Find departments with no high earners (salary &gt; 70000):</p> <pre><code>SELECT DISTINCT department\nFROM employees e1\nWHERE NOT EXISTS (\n    SELECT 1\n    FROM employees e2\n    WHERE e2.department = e1.department\n    AND e2.salary &gt; 70000\n);\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#multiple-row-subqueries","title":"Multiple Row Subqueries","text":""},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#any-operator","title":"ANY Operator","text":"<p>Compare with any value returned by the subquery.</p> <pre><code>-- Find employees earning more than ANY Sales employee\nSELECT name, salary\nFROM employees\nWHERE salary &gt; ANY (\n    SELECT salary\n    FROM employees\n    WHERE department = 'Sales'\n);\n</code></pre> <p>This is equivalent to \"greater than the minimum Sales salary.\"</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#all-operator","title":"ALL Operator","text":"<p>Compare with all values returned by the subquery.</p> <pre><code>-- Find employees earning more than ALL Sales employees\nSELECT name, salary\nFROM employees\nWHERE salary &gt; ALL (\n    SELECT salary\n    FROM employees\n    WHERE department = 'Sales'\n);\n</code></pre> <p>This is equivalent to \"greater than the maximum Sales salary.\"</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#nested-subqueries","title":"Nested Subqueries","text":"<p>Subqueries within subqueries.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#find-employees-in-the-highest-paying-department","title":"Find employees in the highest-paying department","text":"<pre><code>SELECT name, department, salary\nFROM employees\nWHERE department = (\n    SELECT department\n    FROM employees\n    GROUP BY department\n    ORDER BY AVG(salary) DESC\n    LIMIT 1\n);\n</code></pre> <p>Result: <pre><code>+-----------+-----------+--------+\n| name      | department| salary |\n+-----------+-----------+--------+\n| Alice     | IT        | 77500  |\n| Charlie   | IT        | 80000  |\n+-----------+-----------+--------+\n</code></pre></p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#common-table-expressions-cte","title":"Common Table Expressions (CTE)","text":"<p>An alternative to subqueries that's often more readable. Uses WITH clause.</p>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#basic-cte","title":"Basic CTE","text":"<pre><code>WITH dept_averages AS (\n    SELECT\n        department,\n        AVG(salary) AS avg_salary\n    FROM employees\n    GROUP BY department\n)\nSELECT\n    e.name,\n    e.salary,\n    e.department,\n    da.avg_salary\nFROM employees e\nJOIN dept_averages da ON e.department = da.department\nWHERE e.salary &gt; da.avg_salary;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#multiple-ctes","title":"Multiple CTEs","text":"<pre><code>WITH\nhigh_earners AS (\n    SELECT *\n    FROM employees\n    WHERE salary &gt; 60000\n),\ndept_stats AS (\n    SELECT\n        department,\n        COUNT(*) AS emp_count\n    FROM high_earners\n    GROUP BY department\n)\nSELECT *\nFROM dept_stats\nWHERE emp_count &gt; 1;\n</code></pre>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#subquery-performance-tips","title":"Subquery Performance Tips","text":"<ol> <li> <p>Use JOINs when possible - Often faster than subqueries    <pre><code>-- Instead of this:\nSELECT name FROM employees\nWHERE department IN (SELECT department FROM departments WHERE location = 'NY');\n\n-- Use this:\nSELECT e.name\nFROM employees e\nJOIN departments d ON e.department = d.department\nWHERE d.location = 'NY';\n</code></pre></p> </li> <li> <p>Use EXISTS instead of IN for large datasets <pre><code>-- Better performance with large subquery results:\nSELECT name FROM employees e\nWHERE EXISTS (\n    SELECT 1 FROM orders o\n    WHERE o.employee_id = e.id\n);\n</code></pre></p> </li> <li> <p>Avoid correlated subqueries when possible - They execute once per row</p> </li> </ol>"},{"location":"Data%20Collection%20and%20Visulization/SQL/06_subqueries/#practice-exercises","title":"Practice Exercises","text":"<p>Using this products and sales tables:</p> <p>products: <pre><code>+----+----------+-------+\n| id | name     | price |\n+----+----------+-------+\n| 1  | Laptop   | 1000  |\n| 2  | Mouse    | 25    |\n| 3  | Keyboard | 75    |\n+----+----------+-------+\n</code></pre></p> <p>sales: <pre><code>+----+------------+----------+\n| id | product_id | quantity |\n+----+------------+----------+\n| 1  | 1          | 5        |\n| 2  | 2          | 20       |\n| 3  | 1          | 3        |\n+----+------------+----------+\n</code></pre></p> <p>Try these queries:</p> <ol> <li>Find products with above-average price</li> <li>Find products that have been sold</li> <li>Find the most expensive product</li> <li>Calculate total revenue per product</li> <li>Find products sold more than once</li> </ol> Solutions <pre><code>-- 1. Above-average price\nSELECT name, price\nFROM products\nWHERE price &gt; (SELECT AVG(price) FROM products);\n\n-- 2. Products that have been sold\nSELECT name\nFROM products\nWHERE id IN (SELECT DISTINCT product_id FROM sales);\n\n-- 3. Most expensive product\nSELECT name, price\nFROM products\nWHERE price = (SELECT MAX(price) FROM products);\n\n-- 4. Total revenue per product (using CTE)\nWITH product_revenue AS (\n    SELECT\n        p.name,\n        SUM(s.quantity * p.price) AS revenue\n    FROM products p\n    JOIN sales s ON p.id = s.product_id\n    GROUP BY p.id, p.name\n)\nSELECT * FROM product_revenue\nORDER BY revenue DESC;\n\n-- 5. Products sold more than once\nSELECT p.name\nFROM products p\nWHERE (\n    SELECT COUNT(*)\n    FROM sales s\n    WHERE s.product_id = p.id\n) &gt; 1;\n</code></pre> <p>When to Use Subqueries</p> <ul> <li>When you need to filter based on aggregate values</li> <li>When the subquery is simple and returns few rows</li> <li>When you need to reference the result multiple times (use CTE)</li> <li>For readability in complex queries</li> </ul> <p>Previous: SQL Joins</p>"},{"location":"Deep%20Learning/outline/","title":"Pytorch Deep Learning Outline","text":"Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Deep%20Learning/outline/#week-1-tensors-and-datasets","title":"Week 1: Tensors and Datasets","text":"<ul> <li>1 Dimension Tensors</li> <li>Two Dimension Tensors</li> <li>Derivatives and Graphs in PyTorch</li> <li>Simple Dataset</li> <li>Pre Built Datasets</li> </ul> <p>Get Started </p>"},{"location":"Deep%20Learning/outline/#week-2-linear-regression","title":"Week 2: Linear Regression","text":"<ul> <li>Linear Regression 1 Dimension</li> <li>Linear Regression with 1 Parameter</li> <li>Training Slope and Bias</li> </ul> <p>Get Started </p>"},{"location":"Deep%20Learning/outline/#week-3-linear-regression-in-pytorch","title":"Week 3: Linear Regression in PyTorch","text":"<ul> <li>Stochastic Gradient Descent</li> <li>Mini-Batch Gradient Descent</li> <li>PyTorch Build-in Functions</li> <li>Training and Validation Sets</li> </ul> <p>Get Started </p>"},{"location":"Deep%20Learning/outline/#week-4-multiple-input-linear-regression","title":"Week 4: Multiple Input Linear Regression","text":"<ul> <li>Making Predictions in Multiple Linear Regression</li> <li>Training a Multiple Linear Regression Models</li> <li>Multi-Target Linear Regression</li> <li>Training Multiple Output Linear Regression Models</li> </ul> <p>Get Started </p>"},{"location":"Deep%20Learning/outline/#week-5-logistic-regression","title":"Week 5: Logistic Regression","text":"<ul> <li>Making Predictions in Multiple Linear Regression</li> <li>Logistic Regression and Bad Initialization Values</li> <li>Cross Entropy Loss Function</li> <li>Sofmax Activation in 1 Dimension</li> </ul> <p>Get Started </p>"},{"location":"Deep%20Learning/outline/#week-6-practice","title":"Week 6: Practice","text":"<ul> <li>Practice</li> </ul>"},{"location":"Deep%20Learning/outline/#week-7-shallow-neural-networks","title":"Week 7: Shallow Neural Networks","text":"<ul> <li>Simple One Hidden Layer</li> <li>Multiple Neurons</li> <li>Noisy XO</li> <li>One Layer Neural Network</li> <li>Activation Functions</li> <li>Test Activation Functions</li> </ul> <p>Get Started </p>"},{"location":"Deep%20Learning/outline/#week-8-deep-neural-networks","title":"Week 8: Deep Neural Networks","text":"<ul> <li>Multiple Linear Regression</li> <li>Deeper Neural Networks with nn.ModuleList()</li> <li>Using Dropout for Classification</li> <li>Neural Networks with Momentum</li> </ul> <p>Get Started </p>"},{"location":"Deep%20Learning/outline/#week-9-convolution-neural-networks","title":"Week 9: Convolution Neural Networks","text":"<ul> <li>What is Convolution</li> <li>Activatioin Function and Max Pooling</li> <li>Multiple Channel Convolutional Neural Network</li> <li>Convolutional Neural Network with Batch Normalization Get Started </li> </ul>"},{"location":"Deep%20Learning/outline/#week-x-capstone-project-and-review","title":"Week X: Capstone Project and Review","text":"<ul> <li>Applying learned concepts to a real-world dataset</li> </ul>"},{"location":"Deep%20Learning/oxdeep-learning-intro/","title":"Oxdeep learning intro","text":"<p>title: Deep Learning Intro author: Juma Shafara date: \"2025-25-02\" keywords: [data science, data analysis, programming, dataidea] description: Programming for Data Science is a subject we\u2019ve designed to explore the various programming components of data science.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#welcome-to-deep-learning","title":"Welcome to Deep Learning!","text":"<p>by Juma Shafara</p> <p>Welcome to DATAIDEA's Introduction to Deep Learning You're about to learn all you need to get started building your own deep neural networks. You'll learn how to:</p> <ul> <li>create a fully-connected neural network architecture</li> <li>apply neural nets to two classic ML problems: regression and classification</li> <li>train neural nets with stochastic gradient descent, and</li> <li>improve performance with dropout, batch normalization, and other techniques</li> </ul> <p>Let's get started!</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#what-is-deep-learning","title":"What is Deep Learning?","text":"<p>Some of the most impressive advances in artificial intelligence in recent years have been in the field of deep learning. Natural language translation, image recognition, and game playing are all tasks where deep learning models have neared or even exceeded human-level performance.</p> <p>So what is deep learning? Deep learning is an approach to machine learning characterized by deep stacks of computations. This depth of computation is what has enabled deep learning models to disentangle the kinds of complex and hierarchical patterns found in the most challenging real-world datasets.</p> <p>Through their power and scalability neural networks have become the defining model of deep learning.  Neural networks are composed of neurons, where each neuron individually performs only a simple computation. The power of a neural network comes instead from the complexity of the connections these neurons can form.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#the-linear-unit","title":"The Linear Unit","text":"<p>So let's begin with the fundamental component of a neural network: the individual neuron. As a diagram, a neuron (or unit) with one input looks like:</p> The Linear Unit: $y = w x + b$  <p>The input is <code>x</code>. Its connection to the neuron has a weight which is <code>w</code>. Whenever a value flows through a connection, you multiply the value by the connection's weight. For the input <code>x</code>, what reaches the neuron is <code>w * x</code>. A neural network \"learns\" by modifying its weights.</p> <p>The <code>b</code> is a special kind of weight we call the bias. The bias doesn't have any input data associated with it; instead, we put a <code>1</code> in the diagram so that the value that reaches the neuron is just <code>b</code> (since <code>1 * b = b</code>). The bias enables the neuron to modify the output independently of its inputs.</p> <p>The <code>y</code> is the value the neuron ultimately outputs. To get the output, the neuron sums up all the values it receives through its connections. This neuron's activation is <code>y = w * x + b</code>, or as a formula \\(y = w x + b\\).</p> Does the formula $y=w x + b$ look familiar? It's an equation of a line! It's the slope-intercept equation, where $w$ is the slope and $b$ is the y-intercept."},{"location":"Deep%20Learning/oxdeep-learning-intro/#example-the-linear-unit-as-a-model","title":"Example - The Linear Unit as a Model","text":"<p>Though individual neurons will usually only function as part of a larger network, it's often useful to start with a single neuron model as a baseline. Single neuron models are linear models. </p> <p>Let's think about how this might work on a dataset like 80 Cereals. Training a model with <code>'sugars'</code> (grams of sugars per serving) as input and <code>'calories'</code> (calories per serving) as output, we might find the bias is <code>b=90</code> and the weight is <code>w=2.5</code>. We could estimate the calorie content of a cereal with 5 grams of sugar per serving like this:</p> Computing with the linear unit.  <p>And, checking against our formula, we have \\(calories = 2.5 \\times 5 + 90 = 102.5\\), just like we expect.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#multiple-inputs","title":"Multiple Inputs","text":"<p>The 80 Cereals dataset has many more features than just <code>'sugars'</code>. What if we wanted to expand our model to include things like fiber or protein content? That's easy enough. We can just add more input connections to the neuron, one for each additional feature. To find the output, we would multiply each input to its connection weight and then add them all together.</p> A linear unit with three inputs.  <p>The formula for this neuron would be \\(y = w_0 x_0 + w_1 x_1 + w_2 x_2 + b\\). A linear unit with two inputs will fit a plane, and a unit with more inputs than that will fit a hyperplane.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#deep-neural-networks-introduction","title":"Deep Neural Networks Introduction","text":"<p>In this lesson we're going to see how we can build neural networks capable of learning the complex kinds of relationships deep neural nets are famous for.</p> <p>The key idea here is modularity, building up a complex network from simpler functional units. We've seen how a linear unit computes a linear function -- now we'll see how to combine and modify these single units to model more complex relationships.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#layers","title":"Layers","text":"<p>Neural networks typically organize their neurons into layers. When we collect together linear units having a common set of inputs we get a dense layer.</p> A dense layer of two linear units receiving two inputs and a bias.  <p>You could think of each layer in a neural network as performing some kind of relatively simple transformation. Through a deep stack of layers, a neural network can transform its inputs in more and more complex ways. In a well-trained neural network, each layer is a transformation getting us a little bit closer to a solution.</p> Many Kinds of Layers A \"layer\" in Keras is a very general kind of thing. A layer can be, essentially, any kind of data transformation. Many layers, like the convolutional and recurrent layers, transform data through use of neurons and differ primarily in the pattern of connections they form. Others though are used for feature engineering or just simple arithmetic. There's a whole world of layers to discover -- check them out!"},{"location":"Deep%20Learning/oxdeep-learning-intro/#the-activation-function","title":"The Activation Function","text":"<p>It turns out, however, that two dense layers with nothing in between are no better than a single dense layer by itself. Dense layers by themselves can never move us out of the world of lines and planes. What we need is something nonlinear. What we need are activation functions.</p> Without activation functions, neural networks can only learn linear relationships. In order to fit curves, we'll need to use activation functions.   <p>An activation function is simply some function we apply to each of a layer's outputs (its activations). The most common is the rectifier function \\(max(0, x)\\).</p> <p>The rectifier function has a graph that's a line with the negative part \"rectified\" to zero. Applying the function to the outputs of a neuron will put a bend in the data, moving us away from simple lines.</p> <p>When we attach the rectifier to a linear unit, we get a rectified linear unit or ReLU. (For this reason, it's common to call the rectifier function the \"ReLU function\".)  Applying a ReLU activation to a linear unit means the output becomes <code>max(0, w * x + b)</code>, which we might draw in a diagram like:</p> A rectified linear unit."},{"location":"Deep%20Learning/oxdeep-learning-intro/#stacking-dense-layers","title":"Stacking Dense Layers","text":"<p>Now that we have some nonlinearity, let's see how we can stack layers to get complex data transformations.</p> A stack of dense layers makes a \"fully-connected\" network.  <p>The layers before the output layer are sometimes called hidden since we never see their outputs directly.</p> <p>Now, notice that the final (output) layer is a linear unit (meaning, no activation function). That makes this network appropriate to a regression task, where we are trying to predict some arbitrary numeric value. Other tasks (like classification) might require an activation function on the output.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#building-sequential-models","title":"Building Sequential Models","text":"<p>The <code>Sequential</code> model we've been using will connect together a list of layers in order from first to last: the first layer gets the input, the last layer produces the output. This creates the model in the figure above:</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#stochastic-gradient-descent","title":"Stochastic Gradient Descent","text":"<p>In the first two lessons, we learned how to build fully-connected networks out of stacks of dense layers. When first created, all of the network's weights are set randomly -- the network doesn't \"know\" anything yet. In this lesson we're going to see how to train a neural network; we're going to see how neural networks learn.</p> <p>As with all machine learning tasks, we begin with a set of training data. Each example in the training data consists of some features (the inputs) together with an expected target (the output). Training the network means adjusting its weights in such a way that it can transform the features into the target. In the 80 Cereals dataset, for instance, we want a network that can take each cereal's <code>'sugar'</code>, <code>'fiber'</code>, and <code>'protein'</code> content and produce a prediction for that cereal's <code>'calories'</code>. If we can successfully train a network to do that, its weights must represent in some way the relationship between those features and that target as expressed in the training data.</p> <p>In addition to the training data, we need two more things: - A \"loss function\" that measures how good the network's predictions are. - An \"optimizer\" that can tell the network how to change its weights.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#the-loss-function","title":"The Loss Function","text":"<p>We've seen how to design an architecture for a network, but we haven't seen how to tell a network what problem to solve. This is the job of the loss function.</p> <p>The loss function measures the disparity between the the target's true value and the value the model predicts. </p> <p>Different problems call for different loss functions. We have been looking at regression problems, where the task is to predict some numerical value -- calories in 80 Cereals, rating in Red Wine Quality. Other regression tasks might be predicting the price of a house or the fuel efficiency of a car.</p> <p>A common loss function for regression problems is the mean absolute error or MAE. For each prediction <code>y_pred</code>, MAE measures the disparity from the true target <code>y_true</code> by an absolute difference <code>abs(y_true - y_pred)</code>.</p> <p>The total MAE loss on a dataset is the mean of all these absolute differences.</p> The mean absolute error is the average length between the fitted curve and the data points.  <p>Besides MAE, other loss functions you might see for regression problems are the mean-squared error (MSE) or the Huber loss (both available in Keras).</p> <p>During training, the model will use the loss function as a guide for finding the correct values of its weights (lower loss is better). In other words, the loss function tells the network its objective.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#the-optimizer-stochastic-gradient-descent","title":"The Optimizer - Stochastic Gradient Descent","text":"<p>We've described the problem we want the network to solve, but now we need to say how to solve it. This is the job of the optimizer. The optimizer is an algorithm that adjusts the weights to minimize the loss.</p> <p>Virtually all of the optimization algorithms used in deep learning belong to a family called stochastic gradient descent. They are iterative algorithms that train a network in steps. One step of training goes like this: 1. Sample some training data and run it through the network to make predictions. 2. Measure the loss between the predictions and the true values. 3. Finally, adjust the weights in a direction that makes the loss smaller.</p> <p>Then just do this over and over until the loss is as small as you like (or until it won't decrease any further.)</p> Training a neural network with Stochastic Gradient Descent.  <p>Each iteration's sample of training data is called a minibatch (or often just \"batch\"), while a complete round of the training data is called an epoch. The number of epochs you train for is how many times the network will see each training example.</p> <p>The animation shows the linear model from Lesson 1 being trained with SGD. The pale red dots depict the entire training set, while the solid red dots are the minibatches. Every time SGD sees a new minibatch, it will shift the weights (<code>w</code> the slope and <code>b</code> the y-intercept) toward their correct values on that batch. Batch after batch, the line eventually converges to its best fit. You can see that the loss gets smaller as the weights get closer to their true values.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#learning-rate-and-batch-size","title":"Learning Rate and Batch Size","text":"<p>Notice that the line only makes a small shift in the direction of each batch (instead of moving all the way). The size of these shifts is determined by the learning rate. A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values.</p> <p>The learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds. Their interaction is often subtle and the right choice for these parameters isn't always obvious. (We'll explore these effects in the exercise.)</p> <p>Fortunately, for most work it won't be necessary to do an extensive hyperparameter search to get satisfactory results. Adam is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is \"self tuning\", in a sense). Adam is a great general-purpose optimizer.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#adding-the-loss-and-optimizer","title":"Adding the Loss and Optimizer","text":"<p>After defining a model, you can add a loss function and optimizer with the model's <code>compile</code> method:</p> <pre><code>model.compile(\n    optimizer=\"adam\",\n    loss=\"mae\",\n)\n</code></pre> <p>Notice that we are able to specify the loss and optimizer with just a string. You can also access these directly through the Keras API -- if you wanted to tune parameters, for instance -- but for us, the defaults will work fine.</p> What's In a Name? The gradient is a vector that tells us in what direction the weights need to go. More precisely, it tells us how to change the weights to make the loss change fastest. We call our process gradient descent because it uses the gradient to descend the loss curve towards a minimum. Stochastic means \"determined by chance.\" Our training is stochastic because the minibatches are random samples from the dataset. And that's why it's called SGD!"},{"location":"Deep%20Learning/oxdeep-learning-intro/#overfitting-and-underfitting","title":"Overfitting and underfitting","text":"<p>Recall from the example in the previous lesson that Keras will keep a history of the training and validation loss over the epochs that it is training the model. In this lesson, we're going to learn how to interpret these learning curves and how we can use them to guide model development. In particular, we'll examine at the learning curves for evidence of underfitting and overfitting and look at a couple of strategies for correcting it.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#interpreting-the-learning-curves","title":"Interpreting the Learning Curves","text":"<p>You might think about the information in the training data as being of two kinds: signal and noise. The signal is the part that generalizes, the part that can help our model make predictions from new data. The noise is that part that is only true of the training data; the noise is all of the random fluctuation that comes from data in the real-world or all of the incidental, non-informative patterns that can't actually help the model make predictions. The noise is the part might look useful but really isn't.</p> <p>We train a model by choosing weights or parameters that minimize the loss on a training set. You might know, however, that to accurately assess a model's performance, we need to evaluate it on a new set of data, the validation data. (You could see our lesson on model validation in Introduction to Machine Learning for a review.)</p> <p>When we train a model we've been plotting the loss on the training set epoch by epoch. To this we'll add a plot the validation data too. These plots we call the learning curves. To train deep learning models effectively, we need to be able to interpret them.</p> The validation loss gives an estimate of the expected error on unseen data.  <p>Now, the training loss will go down either when the model learns signal or when it learns noise. But the validation loss will go down only when the model learns signal. (Whatever noise the model learned from the training set won't generalize to new data.) So, when a model learns signal both curves go down, but when it learns noise a gap is created in the curves. The size of the gap tells you how much noise the model has learned.</p> <p>Ideally, we would create models that learn all of the signal and none of the noise. This will practically never happen. Instead we make a trade. We can get the model to learn more signal at the cost of learning more noise. So long as the trade is in our favor, the validation loss will continue to decrease. After a certain point, however, the trade can turn against us, the cost exceeds the benefit, and the validation loss begins to rise.</p> Underfitting and overfitting.  <p>This trade-off indicates that there can be two problems that occur when training a model: not enough signal or too much noise. Underfitting the training set is when the loss is not as low as it could be because the model hasn't learned enough signal. Overfitting the training set is when the loss is not as low as it could be because the model learned too much noise. The trick to training deep learning models is finding the best balance between the two.</p> <p>We'll look at a couple ways of getting more signal out of the training data while reducing the amount of noise.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#capacity","title":"Capacity","text":"<p>A model's capacity refers to the size and complexity of the patterns it is able to learn. For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting the data, you should try increasing its capacity.</p> <p>You can increase the capacity of a network either by making it wider (more units to existing layers) or by making it deeper (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset.</p> <p>You'll explore how the capacity of a network can affect its performance in the exercise.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#early-stopping","title":"Early Stopping","text":"<p>We mentioned that when a model is too eagerly learning noise, the validation loss may start to increase during training. To prevent this, we can simply stop the training whenever it seems the validation loss isn't decreasing anymore. Interrupting the training this way is called early stopping.</p> We keep the model where the validation loss is at a minimum.  <p>Once we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured. This ensures that the model won't continue to learn noise and overfit the data.</p> <p>Training with early stopping also means we're in less danger of stopping the training too early, before the network has finished learning signal. So besides preventing overfitting from training too long, early stopping can also prevent underfitting from not training long enough. Just set your training epochs to some large number (more than you'll need), and early stopping will take care of the rest.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#adding-early-stopping","title":"Adding Early Stopping","text":"<p>We can include early stopping in our training through a callback. A callback is just a function you want run every so often while the network trains. The early stopping callback will run after every epoch. (Keras has a variety of useful callbacks pre-defined, but you can define your own, too.)</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#drop-out-and-batch-normalization","title":"Drop out and batch normalization","text":"<p>There's more to the world of deep learning than just dense layers. There are dozens of kinds of layers you might add to a model. (Try browsing through the Keras docs for a sample!) Some are like dense layers and define connections between neurons, and others can do preprocessing or transformations of other sorts.</p> <p>In this lesson, we'll learn about a two kinds of special layers, not containing any neurons themselves, but that add some functionality that can sometimes benefit a model in various ways. Both are commonly used in modern architectures.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#dropout","title":"Dropout","text":"<p>The first of these is the \"dropout layer\", which can help correct overfitting.</p> <p>In the last lesson we talked about how overfitting is caused by the network learning spurious patterns in the training data. To recognize these spurious patterns a network will often rely on very a specific combinations of weight, a kind of \"conspiracy\" of weights. Being so specific, they tend to be fragile: remove one and the conspiracy falls apart.</p> <p>This is the idea behind dropout. To break up these conspiracies, we randomly drop out some fraction of a layer's input units every step of training, making it much harder for the network to learn those spurious patterns in the training data. Instead, it has to search for broad, general patterns, whose weight patterns tend to be more robust.</p> Here, 50% dropout has been added between the two hidden layers. <p>You could also think about dropout as creating a kind of ensemble of networks. The predictions will no longer be made by one big network, but instead by a committee of smaller networks. Individuals in the committee tend to make different kinds of mistakes, but be right at the same time, making the committee as a whole better than any individual. (If you're familiar with random forests as an ensemble of decision trees, it's the same idea.)</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#batch-normalization","title":"Batch Normalization","text":"<p>The next special layer we'll look at performs \"batch normalization\" (or \"batchnorm\"), which can help correct training that is slow or unstable.</p> <p>With neural networks, it's generally a good idea to put all of your data on a common scale, perhaps with something like scikit-learn's StandardScaler or MinMaxScaler. The reason is that SGD will shift the network weights in proportion to how large an activation the data produces. Features that tend to produce activations of very different sizes can make for unstable training behavior.</p> <p>Now, if it's good to normalize the data before it goes into the network, maybe also normalizing inside the network would be better! In fact, we have a special kind of layer that can do this, the batch normalization layer. A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs.</p> <p>Most often, batchnorm is added as an aid to the optimization process (though it can sometimes also help prediction performance). Models with batchnorm tend to need fewer epochs to complete training. Moreover, batchnorm can also fix various problems that can cause the training to get \"stuck\". Consider adding batch normalization to your models, especially if you're having trouble during training.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#binary-classification","title":"Binary Classification","text":"<p>So far in this course, we've learned about how neural networks can solve regression problems. Now we're going to apply neural networks to another common machine learning problem: classification. Most everything we've learned up until now still applies. The main difference is in the loss function we use and in what kind of outputs we want the final layer to produce.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#binary-classification_1","title":"Binary Classification","text":"<p>Classification into one of two classes is a common machine learning problem. You might want to predict whether or not a customer is likely to make a purchase, whether or not a credit card transaction was fraudulent, whether deep space signals show evidence of a new planet, or a medical test evidence of a disease. These are all binary classification problems.</p> <p>In your raw data, the classes might be represented by strings like <code>\"Yes\"</code> and <code>\"No\"</code>, or <code>\"Dog\"</code> and <code>\"Cat\"</code>. Before using this data we'll assign a class label: one class will be <code>0</code> and the other will be <code>1</code>. Assigning numeric labels puts the data in a form a neural network can use.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#accuracy-and-cross-entropy","title":"Accuracy and Cross-Entropy","text":"<p>Accuracy is one of the many metrics in use for measuring success on a classification problem. Accuracy is the ratio of correct predictions to total predictions: <code>accuracy = number_correct / total</code>. A model that always predicted correctly would have an accuracy score of <code>1.0</code>. All else being equal, accuracy is a reasonable metric to use whenever the classes in the dataset occur with about the same frequency.</p> <p>The problem with accuracy (and most other classification metrics) is that it can't be used as a loss function. SGD needs a loss function that changes smoothly, but accuracy, being a ratio of counts, changes in \"jumps\". So, we have to choose a substitute to act as the loss function. This substitute is the cross-entropy function.</p> <p>Now, recall that the loss function defines the objective of the network during training. With regression, our goal was to minimize the distance between the expected outcome and the predicted outcome. We chose MAE to measure this distance.</p> <p>For classification, what we want instead is a distance between probabilities, and this is what cross-entropy provides. Cross-entropy is a sort of measure for the distance from one probability distribution to another.</p> Cross-entropy penalizes incorrect probability predictions. <p>The idea is that we want our network to predict the correct class with probability <code>1.0</code>. The further away the predicted probability is from <code>1.0</code>, the greater will be the cross-entropy loss.</p> <p>The technical reasons we use cross-entropy are a bit subtle, but the main thing to take away from this section is just this: use cross-entropy for a classification loss; other metrics you might care about (like accuracy) will tend to improve along with it.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#making-probabilities-with-the-sigmoid-function","title":"Making Probabilities with the Sigmoid Function","text":"<p>The cross-entropy and accuracy functions both require probabilities as inputs, meaning, numbers from 0 to 1. To covert the real-valued outputs produced by a dense layer into probabilities, we attach a new kind of activation function, the sigmoid activation.</p> The sigmoid function maps real numbers into the interval $[0, 1]$. <p>To get the final class prediction, we define a threshold probability. Typically this will be 0.5, so that rounding will give us the correct class: below 0.5 means the class with label 0 and 0.5 or above means the class with label 1. A 0.5 threshold is what Keras uses by default with its accuracy metric.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#about-the-author","title":"About the Author:","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist at Raising The Village and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning. I enjoy developing innovative algorithms and models that can drive insights and value. I regularly share some content that I find useful throughout my work/learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org. Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/","title":"1D Tensors","text":"<p>title: Torch Tensors in 1D author: Juma Shafara date: \"2023-09\" date-modified: \"2024-07-30\" description: In this lab, you will learn the basics of tensor operations. you will compare them to vectors and numpy arrays. keywords: [     tensors,      Types and Shape,     Indexing and Slicing,     Tensor Functions,     Tensor Operations,     Device_Op Operations, ]</p> <p></p> <p>In this lab, you will learn the basics of tensor operations. Tensors are an essential part of PyTorch; there are complex mathematical objects in and of themselves. Fortunately, most of the intricacies are not necessary. In this section, you will compare them to vectors and numpy arrays.</p> <ul> <li>Types and Shape</li> <li>Indexing and Slicing</li> <li>Tensor Functions</li> <li>Tensor Operations</li> <li>Device_Op Operations</li> </ul> <p>Estimated Time Needed: 25 min</p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>Import the following libraries that you'll use for this lab:</p> <pre><code># These are the libraries will be used for this lab.\n\nimport torch \nimport numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline  \n</code></pre> <p>Check PyTorch version:</p> <pre><code>torch.__version__\n</code></pre> <pre>\n<code>'2.4.0+cpu'</code>\n</pre> <p>This is the function for plotting diagrams. You will use this function to plot the vectors in Coordinate system.</p> <pre><code># Plot vecotrs, please keep the parameters in the same length\n# @param: Vectors = [{\"vector\": vector variable, \"name\": name of vector, \"color\": color of the vector on diagram}]\n\ndef plotVec(vectors):\n    ax = plt.axes()\n\n    # For loop to draw the vectors\n    for vec in vectors:\n        ax.arrow(0, 0, *vec[\"vector\"], head_width = 0.05,color = vec[\"color\"], head_length = 0.1)\n        plt.text(*(vec[\"vector\"] + 0.1), vec[\"name\"])\n\n    plt.ylim(-2,2)\n    plt.xlim(-2,2)\n</code></pre> <p>You can find the type of the following list of integers [0, 1, 2, 3, 4] by applying the constructor <code>torch.tensor()</code>:</p> <pre><code># Convert a integer list with length 5 to a tensor\n\nints_to_tensor = torch.tensor([0, 1, 2, 3, 4])\nprint(\"The dtype of tensor object after converting it to tensor: \", ints_to_tensor.dtype)\nprint(\"The type of tensor object after converting it to tensor: \", ints_to_tensor.type())\n</code></pre> <pre>\n<code>The dtype of tensor object after converting it to tensor:  torch.int64\nThe type of tensor object after converting it to tensor:  torch.LongTensor\n</code>\n</pre> <p>As a result, the integer list has been converted to a long tensor.</p> <p>The Python type is still <code>torch.Tensor</code>:</p> <pre><code>type(ints_to_tensor)\n</code></pre> <pre>\n<code>torch.Tensor</code>\n</pre> <p>You can find the type of this float list [0.0, 1.0, 2.0, 3.0, 4.0] by applying the method <code>torch.tensor()</code>:</p> <pre><code># Convert a float list with length 5 to a tensor\n\nfloats_to_tensor = torch.tensor([0.0, 1.0, 2.0, 3.0, 4.0])\nprint(\"The dtype of tensor object after converting it to tensor: \", floats_to_tensor.dtype)\nprint(\"The type of tensor object after converting it to tensor: \", floats_to_tensor.type())\n</code></pre> <pre>\n<code>The dtype of tensor object after converting it to tensor:  torch.float32\nThe type of tensor object after converting it to tensor:  torch.FloatTensor\n</code>\n</pre> <p>The float list is converted to a float tensor.</p> <pre><code>list_floats=[0.0, 1.0, 2.0, 3.0, 4.0]\n\nfloats_int_tensor=torch.tensor(list_floats,dtype=torch.int64)\n</code></pre> <pre><code>print(\"The dtype of tensor object is: \", floats_int_tensor.dtype)\nprint(\"The type of tensor object is: \", floats_int_tensor.type())\n</code></pre> <pre>\n<code>The dtype of tensor object is:  torch.int64\nThe type of tensor object is:  torch.LongTensor\n</code>\n</pre>  Note!<p>The elements in the list that will be converted to tensor must have the same type.</p> <p>From the previous examples, you see that <code>torch.tensor()</code> converts the list to the tensor type, which is similar to the original list type. However, what if you want to convert the list to a certain tensor type? <code>torch</code> contains the methods required to do this conversion. The following code  converts an integer list to float tensor:</p> <pre><code># Convert a integer list with length 5 to float tensor\n\nnew_float_tensor = torch.FloatTensor([0, 1, 2, 3, 4])\nnew_float_tensor.type()\nprint(\"The type of the new_float_tensor:\", new_float_tensor.type())\n</code></pre> <pre>\n<code>The type of the new_float_tensor: torch.FloatTensor\n</code>\n</pre> <pre><code>new_float_tensor = torch.FloatTensor([0, 1, 2, 3, 4])\n</code></pre> <p>You can also convert an existing tensor object (<code>tensor_obj</code>) to another tensor type. Convert the integer tensor to a float tensor:</p> <pre><code># Another method to convert the integer list to float tensor\n\nold_int_tensor = torch.tensor([0, 1, 2, 3, 4])\nnew_float_tensor = old_int_tensor.type(torch.FloatTensor)\nprint(\"The type of the new_float_tensor:\", new_float_tensor.type())\n</code></pre> <pre>\n<code>The type of the new_float_tensor: torch.FloatTensor\n</code>\n</pre> <p>The <code>tensor_obj.size()</code> helps you to find out the size of the <code>tensor_obj</code>. The <code>tensor_obj.ndimension()</code> shows the dimension of the tensor object.</p> <pre><code># Introduce the tensor_obj.size() &amp;amp; tensor_ndimension.size() methods\n\nprint(\"The size of the new_float_tensor: \", new_float_tensor.size())\nprint(\"The dimension of the new_float_tensor: \",new_float_tensor.ndimension())\n</code></pre> <pre>\n<code>The size of the new_float_tensor:  torch.Size([5])\nThe dimension of the new_float_tensor:  1\n</code>\n</pre> <p>The <code>tensor_obj.view(row, column)</code> is used for reshaping a tensor object.</p> <p>What if you have a tensor object with <code>torch.Size([5])</code> as a <code>new_float_tensor</code> as shown in the previous example? After you execute <code>new_float_tensor.view(5, 1)</code>, the size of <code>new_float_tensor</code> will be <code>torch.Size([5, 1])</code>. This means that the tensor object <code>new_float_tensor</code> has been reshaped from a one-dimensional  tensor object with 5 elements to a two-dimensional tensor object with 5 rows and 1 column.</p> <pre><code># Introduce the tensor_obj.view(row, column) method\n\ntwoD_float_tensor = new_float_tensor.view(5, 1)\nprint(\"Original Size: \", new_float_tensor)\nprint(\"Size after view method\", twoD_float_tensor)\n</code></pre> <pre>\n<code>Original Size:  tensor([0., 1., 2., 3., 4.])\nSize after view method tensor([[0.],\n        [1.],\n        [2.],\n        [3.],\n        [4.]])\n</code>\n</pre> <p>Note that the original size is 5. The tensor after reshaping becomes a 5X1 tensor analog to a column vector.</p>  Note!<p>The number of elements in a tensor must remain constant after applying view.</p> <p>What if you have a tensor with dynamic size but you want to reshape it? You can use -1 to do just that.</p> <pre><code># Introduce the use of -1 in tensor_obj.view(row, column) method\n\ntwoD_float_tensor = new_float_tensor.view(-1, 1)\nprint(\"Original Size: \", new_float_tensor)\nprint(\"Size after view method\", twoD_float_tensor)\n</code></pre> <pre>\n<code>Original Size:  tensor([0., 1., 2., 3., 4.])\nSize after view method tensor([[0.],\n        [1.],\n        [2.],\n        [3.],\n        [4.]])\n</code>\n</pre> <p>You get the same result as the previous example. The -1 can represent any size. However, be careful because you can set only one argument as -1.</p> <p>You can also convert a numpy array to a tensor, for example: </p> <pre><code># Convert a numpy array to a tensor\n\nnumpy_array = np.array([0.0, 1.0, 2.0, 3.0, 4.0])\nnew_tensor = torch.from_numpy(numpy_array)\n\nprint(\"The dtype of new tensor: \", new_tensor.dtype)\nprint(\"The type of new tensor: \", new_tensor.type())\n</code></pre> <pre>\n<code>The dtype of new tensor:  torch.float64\nThe type of new tensor:  torch.DoubleTensor\n</code>\n</pre> <p>Converting a tensor to a numpy is also supported in PyTorch. The syntax is shown below:</p> <pre><code># Convert a tensor to a numpy array\n\nback_to_numpy = new_tensor.numpy()\nprint(\"The numpy array from tensor: \", back_to_numpy)\nprint(\"The dtype of numpy array: \", back_to_numpy.dtype)\n</code></pre> <pre>\n<code>The numpy array from tensor:  [0. 1. 2. 3. 4.]\nThe dtype of numpy array:  float64\n</code>\n</pre> <p><code>back_to_numpy</code> and <code>new_tensor</code> still point to <code>numpy_array</code>. As a result if we change <code>numpy_array</code> both <code>back_to_numpy</code> and <code>new_tensor</code> will change. For example if we set all the elements in <code>numpy_array</code> to zeros, <code>back_to_numpy</code> and <code> new_tensor</code> will follow suit.</p> <pre><code># Set all elements in numpy array to zero \nnumpy_array[:] = 0\nprint(\"The new tensor points to numpy_array : \", new_tensor)\nprint(\"and back to numpy array points to the tensor: \", back_to_numpy)\n</code></pre> <pre>\n<code>The new tensor points to numpy_array :  tensor([0., 0., 0., 0., 0.], dtype=torch.float64)\nand back to numpy array points to the tensor:  [0. 0. 0. 0. 0.]\n</code>\n</pre> <p>Pandas Series can also be converted by using the numpy array that is stored in <code>pandas_series.values</code>. Note that <code>pandas_series</code> can be any pandas_series object. </p> <pre><code># Convert a panda series to a tensor\n\npandas_series=pd.Series([0.1, 2, 0.3, 10.1])\nnew_tensor=torch.from_numpy(pandas_series.values)\nprint(\"The new tensor from numpy array: \", new_tensor)\nprint(\"The dtype of new tensor: \", new_tensor.dtype)\nprint(\"The type of new tensor: \", new_tensor.type())\n</code></pre> <pre>\n<code>The new tensor from numpy array:  tensor([ 0.1000,  2.0000,  0.3000, 10.1000], dtype=torch.float64)\nThe dtype of new tensor:  torch.float64\nThe type of new tensor:  torch.DoubleTensor\n</code>\n</pre> <p>consider the following tensor </p> <pre><code>this_tensor=torch.tensor([0,1, 2,3]) \n</code></pre> <p>The method <code>item()</code> returns the value of this tensor as a standard Python number. This only works for one element. </p> <pre><code>this_tensor=torch.tensor([0,1, 2,3]) \n\nprint(\"the first item is given by\",this_tensor[0].item(),\"the first tensor value is given by \",this_tensor[0])\nprint(\"the second item is given by\",this_tensor[1].item(),\"the second tensor value is given by \",this_tensor[1])\nprint(\"the third  item is given by\",this_tensor[2].item(),\"the third tensor value is given by \",this_tensor[2])\n</code></pre> <pre>\n<code>the first item is given by 0 the first tensor value is given by  tensor(0)\nthe second item is given by 1 the second tensor value is given by  tensor(1)\nthe third  item is given by 2 the third tensor value is given by  tensor(2)\n</code>\n</pre> <p>we can use the method <code> tolist()</code> to return a list </p> <pre><code>torch_to_list=this_tensor.tolist()\n\nprint('tensor:', this_tensor,\"\\nlist:\",torch_to_list)\n</code></pre> <pre>\n<code>tensor: tensor([0, 1, 2, 3]) \nlist: [0, 1, 2, 3]\n</code>\n</pre> <p>Try to convert <code>your_tensor</code> to a 1X5 tensor.</p> <pre><code># Practice: convert the following tensor to a tensor object with 1 row and 5 columns\n\nyour_tensor = torch.tensor([1, 2, 3, 4, 5])\n</code></pre> <p>In Python, the index starts with 0. Therefore, the last index will always be 1 less than the length of the tensor object. You can access the value on a certain index by using the square bracket, for example:</p> <pre><code># A tensor for showing how the indexs work on tensors\n\nindex_tensor = torch.tensor([0, 1, 2, 3, 4])\nprint(\"The value on index 0:\",index_tensor[0])\nprint(\"The value on index 1:\",index_tensor[1])\nprint(\"The value on index 2:\",index_tensor[2])\nprint(\"The value on index 3:\",index_tensor[3])\nprint(\"The value on index 4:\",index_tensor[4])\n</code></pre> <pre>\n<code>The value on index 0: tensor(0)\nThe value on index 1: tensor(1)\nThe value on index 2: tensor(2)\nThe value on index 3: tensor(3)\nThe value on index 4: tensor(4)\n</code>\n</pre> <p>Note that the <code>index_tensor[5]</code> will create an error.</p> <p>The index is shown in the following figure: </p> <p></p> <p>Now, you'll see how to change the values on certain indexes.</p> <p>Suppose you have a tensor as shown here: </p> <pre><code># A tensor for showing how to change value according to the index\n\ntensor_sample = torch.tensor([20, 1, 2, 3, 4])\n</code></pre> <p>Assign the value on index 0 as 100:</p> <pre><code># Change the value on the index 0 to 100\n\nprint(\"Inital value on index 0:\", tensor_sample[0])\ntensor_sample[0] = 100\nprint(\"Modified tensor:\", tensor_sample)\n</code></pre> <pre>\n<code>Inital value on index 0: tensor(20)\nModified tensor: tensor([100,   1,   2,   3,   4])\n</code>\n</pre> <p>As you can see, the value on index 0 changes. Change the value on index 4 to 0:</p> <pre><code># Change the value on the index 4 to 0\n\nprint(\"Inital value on index 4:\", tensor_sample[4])\ntensor_sample[4] = 0\nprint(\"Modified tensor:\", tensor_sample)\n</code></pre> <pre>\n<code>Inital value on index 4: tensor(4)\nModified tensor: tensor([100,   1,   2,   3,   0])\n</code>\n</pre> <p>The value on index 4 turns to 0.</p> <p>If you are familiar with Python, you know that there is a feature called slicing on a list. Tensors support the same feature. </p> <p>Get the subset of <code>tensor_sample</code>. The subset should contain the values in <code>tensor_sample</code> from index 1 to index 3.</p> <pre><code># Slice tensor_sample\n\nsubset_tensor_sample = tensor_sample[1:4]\nprint(\"Original tensor sample: \", tensor_sample)\nprint(\"The subset of tensor sample:\", subset_tensor_sample)\n</code></pre> <pre>\n<code>Original tensor sample:  tensor([100,   1,   2,   3,   0])\nThe subset of tensor sample: tensor([1, 2, 3])\n</code>\n</pre> <p>As a result, the <code>subset_tensor_sample</code> returned only the values on index 1, index 2, and index 3. Then, it stored them in a <code>subset_tensor_sample</code>.</p>  Note!<p>The number on the left side of the colon represents the index of the first value. The number on the right side of the colon is always 1 larger than the index of the last value. For example, tensor_sample[1:4]means you get values from the index 1 to index 3 (4-1).</p> <p>As for assigning values to the certain index, you can also assign the value to the slices:</p> <p>Change the value of <code>tensor_sample</code> from index 3 to index 4:</p> <pre><code># Change the values on index 3 and index 4\n\nprint(\"Inital value on index 3 and index 4:\", tensor_sample[3:5])\ntensor_sample[3:5] = torch.tensor([300.0, 400.0])\nprint(\"Modified tensor:\", tensor_sample)\n</code></pre> <pre>\n<code>Inital value on index 3 and index 4: tensor([3, 0])\nModified tensor: tensor([100,   1,   2, 300, 400])\n</code>\n</pre> <p>The values on both index 3 and index 4 were changed. The values on other indexes remain the same.</p> <p>You can also use a variable to contain the selected indexes and pass that variable to a tensor slice operation as a parameter, for example:  </p> <pre><code># Using variable to contain the selected index, and pass it to slice operation\n\nselected_indexes = [3, 4]\nsubset_tensor_sample = tensor_sample[selected_indexes]\nprint(\"The inital tensor_sample\", tensor_sample)\nprint(\"The subset of tensor_sample with the values on index 3 and 4: \", subset_tensor_sample)\n</code></pre> <pre>\n<code>The inital tensor_sample tensor([100,   1,   2, 300, 400])\nThe subset of tensor_sample with the values on index 3 and 4:  tensor([300, 400])\n</code>\n</pre> <p>You can also assign one value to the selected indexes by using the variable. For example, assign 100,000 to all the <code>selected_indexes</code>:</p> <pre><code>#Using variable to assign the value to the selected indexes\n\nprint(\"The inital tensor_sample\", tensor_sample)\nselected_indexes = [1, 3]\ntensor_sample[selected_indexes] = 100000\nprint(\"Modified tensor with one value: \", tensor_sample)\n</code></pre> <pre>\n<code>The inital tensor_sample tensor([100,   1,   2, 300, 400])\nModified tensor with one value:  tensor([   100, 100000,      2, 100000,    400])\n</code>\n</pre> <p>The values on index 1 and index 3 were changed to 100,000. Others remain the same.</p>  Note!<p>You can use only one value for the assignment.</p> <p>Try to change the values on index 3, 4, 7 of the following tensor to 0.</p> <pre><code># Practice: Change the values on index 3, 4, 7 to 0\n\npractice_tensor = torch.tensor([2, 7, 3, 4, 6, 2, 3, 1, 2])\n</code></pre> <p>For this section, you'll work with some methods that you can apply to tensor objects.</p> <p>You'll review the mean and standard deviation methods first. They are two basic statistical methods.</p> <p>Create a tensor with values [1.0, -1, 1, -1]:</p> <pre><code># Sample tensor for mathmatic calculation methods on tensor\n\nmath_tensor = torch.tensor([1.0, -1.0, 1, -1])\nprint(\"Tensor example: \", math_tensor)\n</code></pre> <pre>\n<code>Tensor example:  tensor([ 1., -1.,  1., -1.])\n</code>\n</pre> <p>Here is the mean method:  </p> <pre><code>#Calculate the mean for math_tensor\n\nmean = math_tensor.mean()\nprint(\"The mean of math_tensor: \", mean)\n</code></pre> <pre>\n<code>The mean of math_tensor:  tensor(0.)\n</code>\n</pre> <p>The standard deviation can also be calculated by using <code>tensor_obj.std()</code>:</p> <pre><code>#Calculate the standard deviation for math_tensor\n\nstandard_deviation = math_tensor.std()\nprint(\"The standard deviation of math_tensor: \", standard_deviation)\n</code></pre> <pre>\n<code>The standard deviation of math_tensor:  tensor(1.1547)\n</code>\n</pre> <p>Now, you'll review another two useful methods: <code>tensor_obj.max()</code> and <code>tensor_obj.min()</code>. These two methods are used for finding the maximum value and the minimum value in the tensor.</p> <p>Create a <code>max_min_tensor</code>: </p> <pre><code># Sample for introducing max and min methods\n\nmax_min_tensor = torch.tensor([1, 1, 3, 5, 5])\nprint(\"Tensor example: \", max_min_tensor)\n</code></pre> <pre>\n<code>Tensor example:  tensor([1, 1, 3, 5, 5])\n</code>\n</pre>  Note!<p>There are two minimum numbers as 1 and two maximum numbers as 5 in the tensor. Can you guess how PyTorch is going to deal with the duplicates?</p> <p>Apply <code>tensor_obj.max()</code> on <code>max_min_tensor</code>:</p> <pre><code># Method for finding the maximum value in the tensor\n\nmax_val = max_min_tensor.max()\nprint(\"Maximum number in the tensor: \", max_val)\n</code></pre> <pre>\n<code>Maximum number in the tensor:  tensor(5)\n</code>\n</pre> <p>The answer is <code>tensor(5)</code>. Therefore, the method <code>tensor_obj.max()</code> is grabbing the maximum value but not the elements that contain the maximum value in the tensor.</p> <pre><code> max_min_tensor.max()\n</code></pre> <pre>\n<code>tensor(5)</code>\n</pre> <p>Use <code>tensor_obj.min()</code> on <code>max_min_tensor</code>:</p> <pre><code># Method for finding the minimum value in the tensor\n\nmin_val = max_min_tensor.min()\nprint(\"Minimum number in the tensor: \", min_val)\n</code></pre> <pre>\n<code>Minimum number in the tensor:  tensor(1)\n</code>\n</pre> <p>The answer is <code>tensor(1)</code>. Therefore, the method <code>tensor_obj.min()</code> is grabbing the minimum value but not the elements that contain the minimum value in the tensor.</p> <p>Sin is the trigonometric function of an angle. Again, you will not be introducedvto any mathematic functions. You'll focus on Python.</p> <p>Create a tensor with 0, \u03c0/2 and \u03c0. Then, apply the sin function on the tensor. Notice here that the <code>sin()</code> is not a method of tensor object but is a function of torch:</p> <pre><code># Method for calculating the sin result of each element in the tensor\n\npi_tensor = torch.tensor([0, np.pi/2, np.pi])\nsin = (torch.sin(pi_tensor))\nprint(\"The sin result of pi_tensor: \", sin)\n</code></pre> <pre>\n<code>The sin result of pi_tensor:  tensor([ 0.0000e+00,  1.0000e+00, -8.7423e-08])\n</code>\n</pre> <p>The resultant tensor <code>sin</code> contains the result of the <code>sin</code> function applied to each element in the <code>pi_tensor</code>. This is different from the previous methods. For <code>tensor_obj.mean()</code>, <code>tensor_obj.std()</code>, <code>tensor_obj.max()</code>, and <code>tensor_obj.min()</code>, the result is a tensor with only one number because these are aggregate methods. However, the <code>torch.sin()</code> is not. Therefore, the resultant tensors have the same length as the input tensor.</p> <p>A useful function for plotting mathematical functions is <code>torch.linspace()</code>. <code>torch.linspace()</code> returns evenly spaced numbers over a specified interval. You specify the starting point of the sequence and the ending point of the sequence. The parameter <code>steps</code> indicates the number of samples to generate. Now, you'll work with <code>steps = 5</code>.</p> <pre><code># First try on using linspace to create tensor\n\nlen_5_tensor = torch.linspace(-2, 2, steps = 5)\nprint (\"First Try on linspace\", len_5_tensor)\n</code></pre> <pre>\n<code>First Try on linspace tensor([-2., -1.,  0.,  1.,  2.])\n</code>\n</pre> <p>Assign <code>steps</code> with 9:</p> <pre><code># Second try on using linspace to create tensor\n\nlen_9_tensor = torch.linspace(-2, 2, steps = 9)\nprint (\"Second Try on linspace\", len_9_tensor)\n</code></pre> <pre>\n<code>Second Try on linspace tensor([-2.0000, -1.5000, -1.0000, -0.5000,  0.0000,  0.5000,  1.0000,  1.5000,\n         2.0000])\n</code>\n</pre> <p>Use both <code>torch.linspace()</code> and <code>torch.sin()</code> to construct a tensor that contains the 100 sin result in range from 0 (0 degree) to 2\u03c0 (360 degree): </p> <pre><code># Construct the tensor within 0 to 360 degree\n\npi_tensor = torch.linspace(0, 2*np.pi, 100)\nsin_result = torch.sin(pi_tensor)\n</code></pre> <p>Plot the result to get a clearer picture. You must cast the tensor to a numpy array before plotting it.</p> <pre><code># Plot sin_result\n\nplt.plot(pi_tensor.numpy(), sin_result.numpy())\n</code></pre> <pre>\n<code>[&lt;matplotlib.lines.Line2D at 0x787287da7020&gt;]</code>\n</pre> <p>If you know the trigonometric function, you will notice this is the diagram of the sin result in the range 0 to 360 degrees.</p> <p>Construct a tensor with 25 steps in the range 0 to \u03c0/2. Print out the Maximum and Minimum number. Also, plot  a graph showing the diagram that shows the result.</p> <pre><code># Practice: Create your tensor, print max and min number, plot the sin result diagram\n\npi_tensor = torch.linspace(0, np.pi/2, steps = 25)\nprint(\"Maximum: \", pi_tensor.max())\nprint(\"Minimum: \", pi_tensor.min())\nsin_result = torch.sin(pi_tensor)\nplt.plot(pi_tensor.numpy(), sin_result.numpy())\n# Type your code here\n</code></pre> <pre>\n<code>Maximum:  tensor(1.5708)\nMinimum:  tensor(0.)\n</code>\n</pre> <pre>\n<code>[&lt;matplotlib.lines.Line2D at 0x787287dec5c0&gt;]</code>\n</pre> <p>In the following section, you'll work with operations that you can apply to a tensor.</p> <p>You can perform addition between two tensors.</p> <p>Create a tensor <code>u</code> with 1 dimension and 2 elements. Then, create another tensor <code>v</code> with the same number of dimensions and the same number of elements:</p> <pre><code># Create two sample tensors\n\nu = torch.tensor([1, 0])\nv = torch.tensor([0, 1])\n</code></pre> <p>Add <code>u</code> and <code>v</code> together:</p> <pre><code># Add u and v\n\nw = u + v\nprint(\"The result tensor: \", w)\n</code></pre> <pre>\n<code>The result tensor:  tensor([1, 1])\n</code>\n</pre> <p>The result is <code>tensor([1, 1])</code>. The behavior is [1 + 0, 0 + 1].</p> <p>Plot the result to to get a clearer picture.</p> <pre><code># Plot u, v, w\n\nplotVec([\n    {\"vector\": u.numpy(), \"name\": 'u', \"color\": 'r'},\n    {\"vector\": v.numpy(), \"name\": 'v', \"color\": 'b'},\n    {\"vector\": w.numpy(), \"name\": 'w', \"color\": 'g'}\n])\n</code></pre> <p>Implement the tensor subtraction with <code>u</code> and <code>v</code> as u-v.</p> <pre><code># Try by yourself to get a result of u-v\n\nu = torch.tensor([1, 0])\nv = torch.tensor([0, 1])\nw = u - v\nprint(w)\n</code></pre> <pre>\n<code>tensor([ 1, -1])\n</code>\n</pre> <p>Tensors must be of the same data type to perform addition as well as other operations.If you uncomment the  following code and try to run it you will get an error as the two tensors are of two different data types. NOTE This lab was created on a older PyTorch version so in the current version we are using this is possible and will produce a float64 tensor.</p> <pre><code>torch.tensor([1,2,3],dtype=torch.int64)+torch.tensor([1,2,3],dtype=torch.float64)\n</code></pre> <pre>\n<code>tensor([2., 4., 6.], dtype=torch.float64)</code>\n</pre> <p>You can add a scalar to the tensor. Use <code>u</code> as the sample tensor:</p> <pre><code># tensor + scalar\n\nu = torch.tensor([1, 2, 3, -1])\nv = u + 1\nprint (\"Addition Result: \", v)\n</code></pre> <pre>\n<code>Addition Result:  tensor([2, 3, 4, 0])\n</code>\n</pre> <p>The result is simply adding 1 to each element in tensor <code>u</code> as shown in the following image:</p> <p></p> <p>Now, you'll review the multiplication between a tensor and a scalar.</p> <p>Create a tensor with value <code>[1, 2]</code> and then multiply it by 2:</p> <pre><code># tensor * scalar\n\nu = torch.tensor([1, 2])\nv = 2 * u\nprint(\"The result of 2 * u: \", v)\n</code></pre> <pre>\n<code>The result of 2 * u:  tensor([2, 4])\n</code>\n</pre> <p>The result is <code>tensor([2, 4])</code>, so the code <code>2 * u</code> multiplies each element in the tensor by 2. This is how you get the product between a vector or matrix and a scalar in linear algebra.</p> <p>You can use multiplication between two tensors.</p> <p>Create two tensors <code>u</code> and <code>v</code> and then multiply them together:</p> <pre><code># tensor * tensor\n\nu = torch.tensor([1, 2])\nv = torch.tensor([3, 2])\nw = u * v\nprint (\"The result of u * v\", w)\n</code></pre> <pre>\n<code>The result of u * v tensor([3, 4])\n</code>\n</pre> <p>The result is simply <code>tensor([3, 4])</code>. This result is achieved by multiplying every element in <code>u</code> with the corresponding element in the same position <code>v</code>, which is similar to [1 * 3, 2 * 2].</p> <p>The dot product is a special operation for a vector that you can use in Torch.</p> <p>Here is the dot product of the two tensors <code>u</code> and <code>v</code>:</p> <pre><code># Calculate dot product of u, v\n\nu = torch.tensor([1, 2])\nv = torch.tensor([3, 2])\n\nprint(\"Dot Product of u, v:\", torch.dot(u,v))\n</code></pre> <pre>\n<code>Dot Product of u, v: tensor(7)\n</code>\n</pre> <p>The result is <code>tensor(7)</code>. The function is 1 x 3 + 2 x 2 = 7.</p> <p>Convert the list [-1, 1] and [1, 1] to tensors <code>u</code> and <code>v</code>. Then, plot the tensor <code>u</code> and <code>v</code> as a vector by using the function <code>plotVec</code> and find the dot product:</p> <pre><code># Practice: calculate the dot product of u and v, and plot out two vectors\nu = torch.tensor([-1, 1])\nv = torch.tensor([1, 1])\n\nplotVec([\n    {\"vector\": u.numpy(), \"name\": 'u', \"color\": 'r'},\n    {\"vector\": v.numpy(), \"name\": 'v', \"color\": 'b'},\n    {\"vector\": (u / v).numpy(), \"name\": w, \"color\": 'g'}\n    ])\n\n# Type your code here\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#objective","title":"Objective","text":"<p><ul><li> How tensor operations work in pytorch.</li></ul></p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#table-of-contents","title":"Table of Contents","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#Types_Shape","title":"Types and Shape","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#Index_Slice","title":"Indexing and Slicing","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#practice_1","title":"Practice","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#Tensor_Func","title":"Tensor Functions","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#mean-and-standard-deviation","title":"Mean and Standard Deviation","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#max-and-min","title":"Max and Min","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#sin","title":"Sin","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#create-tensor-by-torchlinspace","title":"Create Tensor by <code>torch.linspace()</code>","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#practice_2","title":"Practice","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#Tensor_Op","title":"Tensor Operations","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#tensor-addition","title":"Tensor Addition","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#try","title":"Try","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#tensor-multiplication","title":"Tensor Multiplication","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#dot-product","title":"Dot Product","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#practice_3","title":"Practice","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#about-the-author","title":"About the Author","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning. I also enjoy developing innovative algorithms and models that can drive insights and value. I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics. Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/","title":"2D Tensors","text":"<p>title: Torch Tensors in 2D author: Juma Shafara date: \"2023-09\" date-modified: \"2024-07-30\" description: In this lab, you will learn the basics of tensor operations on 2D tensors. keywords: []</p> <p></p> <p>In this lab, you will learn the basics of tensor operations on 2D tensors.</p> <ul> <li>Types and Shape </li> <li>Indexing and Slicing</li> <li>Tensor Operations</li> </ul> <p>Estimated Time Needed: 10 min</p> <p>The following are the libraries we are going to use for this lab.</p> <pre><code># These are the libraries will be used for this lab.\n\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport torch\nimport pandas as pd\n</code></pre> <p>The methods and types for 2D tensors is similar to the methods and types for 1D tensors which has been introduced in Previous Lab.</p> <p>Let us see how to convert a 2D list to a 2D tensor. First, let us create a 3X3 2D tensor. Then let us try to use <code>torch.tensor()</code> which we used for converting a 1D list to 1D tensor. Is it going to work?</p> <pre><code># Convert 2D List to 2D Tensor\n\ntwoD_list = [[11, 12, 13], [21, 22, 23], [31, 32, 33]]\ntwoD_tensor = torch.tensor(twoD_list)\nprint(\"The New 2D Tensor: \", twoD_tensor)\n</code></pre> <p>Bravo! The method <code>torch.tensor()</code> works perfectly.Now, let us try other functions we studied in the Previous Lab.</p> <p>Let us try <code>tensor_obj.ndimension()</code> (<code>tensor_obj</code>: This can be any tensor object), <code>tensor_obj.shape</code>, and <code>tensor_obj.size()</code></p> <pre><code># Try tensor_obj.ndimension(), tensor_obj.shape, tensor_obj.size()\n\nprint(\"The dimension of twoD_tensor: \", twoD_tensor.ndimension())\nprint(\"The shape of twoD_tensor: \", twoD_tensor.shape)\nprint(\"The shape of twoD_tensor: \", twoD_tensor.size())\nprint(\"The number of elements in twoD_tensor: \", twoD_tensor.numel())\n</code></pre> <p>Because it is a 2D 3X3 tensor,  the outputs are correct.</p> <p>Now, let us try converting the tensor to a numpy array and convert the numpy array back to a tensor.</p> <pre><code># Convert tensor to numpy array; Convert numpy array to tensor\n\ntwoD_numpy = twoD_tensor.numpy()\nprint(\"Tensor -&amp;gt; Numpy Array:\")\nprint(\"The numpy array after converting: \", twoD_numpy)\nprint(\"Type after converting: \", twoD_numpy.dtype)\n\nprint(\"================================================\")\n\nnew_twoD_tensor = torch.from_numpy(twoD_numpy)\nprint(\"Numpy Array -&amp;gt; Tensor:\")\nprint(\"The tensor after converting:\", new_twoD_tensor)\nprint(\"Type after converting: \", new_twoD_tensor.dtype)\n</code></pre> <p>The result shows the tensor has successfully been converted to a numpy array and then converted back to a tensor.</p> <p>Now let us try to convert a Pandas Dataframe to a tensor. The process is the  Same as the 1D conversion, we can obtain the numpy array via the attribute <code>values</code>. Then, we can use <code>torch.from_numpy()</code> to convert the value of the Pandas Series to a tensor.</p> <pre><code># Try to convert the Panda Dataframe to tensor\n\ndf = pd.DataFrame({'a':[11,21,31],'b':[12,22,312]})\n\nprint(\"Pandas Dataframe to numpy: \", df.values)\nprint(\"Type BEFORE converting: \", df.values.dtype)\n\nprint(\"================================================\")\n\nnew_tensor = torch.from_numpy(df.values)\nprint(\"Tensor AFTER converting: \", new_tensor)\nprint(\"Type AFTER converting: \", new_tensor.dtype)\n</code></pre> <p>Try to convert the following Pandas Dataframe  to a tensor</p> <pre><code># Practice: try to convert Pandas Series to tensor\n\ndf = pd.DataFrame({'A':[11, 33, 22],'B':[3, 3, 2]})\n</code></pre> <p>Double-click here for the solution.</p> <p>You can use rectangular brackets to access the different elements of the tensor. The correspondence between the rectangular brackets and the list and the rectangular representation is shown in the following figure for a 3X3 tensor:  </p> <p></p> <p>You can access the 2nd-row 3rd-column as shown in the following figure:</p> <p></p> <p>You simply use the square brackets and the indices corresponding to the element that you want.</p> <p>Now, let us try to access the value on position 2nd-row 3rd-column. Remember that the index is always 1 less than how we count rows and columns. There are two ways to access the certain value of a tensor. The example in code will be the same as the example picture above.</p> <pre><code># Use tensor_obj[row, column] and tensor_obj[row][column] to access certain position\n\ntensor_example = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\nprint(\"What is the value on 2nd-row 3rd-column? \", tensor_example[1, 2])\nprint(\"What is the value on 2nd-row 3rd-column? \", tensor_example[1][2])\n</code></pre> <p>As we can see, both methods return the true value (the same value as the picture above). Therefore, both of the methods work.</p> <p>Consider the elements shown in the following figure: </p> <p></p> <p>Use the method above, we can access the 1st-row 1st-column by <code>tensor_example[0][0]</code></p> <pre><code>tensor_example[0][0]\n</code></pre> <p>But what if we want to get the value on both 1st-row 1st-column and 1st-row 2nd-column?</p> <p>You can also use slicing in a tensor. Consider the following figure. You want to obtain the 1st two columns in the 1st row:  </p> <p></p> <pre><code># Use tensor_obj[begin_row_number: end_row_number, begin_column_number: end_column number] \n# and tensor_obj[row][begin_column_number: end_column number] to do the slicing\n\ntensor_example = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\nprint(\"What is the value on 1st-row first two columns? \", tensor_example[0, 0:2])\nprint(\"What is the value on 1st-row first two columns? \", tensor_example[0][0:2])\n</code></pre> <p>We get the result as <code>tensor([11, 12])</code> successfully.</p> <p>But we can't combine using slicing on row and pick one column by using the code <code>tensor_obj[begin_row_number: end_row_number][begin_column_number: end_column number]</code>. The reason is that the slicing will be applied on the tensor first. The result type will be a two dimension again. The second bracket will no longer represent the index of the column it will be the index of the row at that time. Let us see an example. </p> <pre><code># Give an idea on tensor_obj[number: number][number]\n\ntensor_example = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\nsliced_tensor_example = tensor_example[1:3]\nprint(\"1. Slicing step on tensor_example: \")\nprint(\"Result after tensor_example[1:3]: \", sliced_tensor_example)\nprint(\"Dimension after tensor_example[1:3]: \", sliced_tensor_example.ndimension())\nprint(\"================================================\")\nprint(\"2. Pick an index on sliced_tensor_example: \")\nprint(\"Result after sliced_tensor_example[1]: \", sliced_tensor_example[1])\nprint(\"Dimension after sliced_tensor_example[1]: \", sliced_tensor_example[1].ndimension())\nprint(\"================================================\")\nprint(\"3. Combine these step together:\")\nprint(\"Result: \", tensor_example[1:3][1])\nprint(\"Dimension: \", tensor_example[1:3][1].ndimension())\n</code></pre> <p>See the results and dimensions in 2 and 3 are the same. Both of them contains the 3rd row in the <code>tensor_example</code>, but not the last two values in the 3rd column.</p> <p>So how can we get the elements in the 3rd column with the last two rows? As the below picture.</p> <p></p> <p>Let's see the code below.</p> <pre><code># Use tensor_obj[begin_row_number: end_row_number, begin_column_number: end_column number] \n\ntensor_example = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\nprint(\"What is the value on 3rd-column last two rows? \", tensor_example[1:3, 2])\n</code></pre> <p>Fortunately, the code <code>tensor_obj[begin_row_number: end_row_number, begin_column_number: end_column number]</code> is still works.</p> <p>Try to change the values on the second column and the last two rows to 0. Basically, change the values on <code>tensor_ques[1][1]</code> and <code>tensor_ques[2][1]</code> to 0.</p> <pre><code># Practice: Use slice and index to change the values on the matrix tensor_ques.\n\ntensor_ques = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\n</code></pre> <p>Double-click here for the solution.</p> <p>We can also do some calculations on 2D tensors.</p> <p>You can also add tensors; the process is identical to matrix addition. Matrix addition of X and Y is shown in the following figure:</p> <p></p> <p>Let us see how tensor addition works with <code>X</code> and <code>Y</code>.</p> <pre><code># Calculate [[1, 0], [0, 1]] + [[2, 1], [1, 2]]\n\nX = torch.tensor([[1, 0],[0, 1]]) \nY = torch.tensor([[2, 1],[1, 2]])\nX_plus_Y = X + Y\nprint(\"The result of X + Y: \", X_plus_Y)\n</code></pre> <p>Like the result shown in the picture above. The result is <code>[[3, 1], [1, 3]]</code></p> <p>Multiplying a tensor by a scalar is identical to multiplying a matrix by a scaler. If you multiply the matrix Y by the scalar 2, you simply multiply every element in the matrix by 2 as shown in the figure:</p> <p></p> <p>Let us try to calculate the product of 2Y.</p> <pre><code># Calculate 2 * [[2, 1], [1, 2]]\n\nY = torch.tensor([[2, 1], [1, 2]]) \ntwo_Y = 2 * Y\nprint(\"The result of 2Y: \", two_Y)\n</code></pre> <p>Multiplication of two tensors corresponds to an element-wise product or Hadamard product.  Consider matrix the X and Y with the same size. The Hadamard product corresponds to multiplying each of the elements at the same position, that is, multiplying elements with the same color together. The result is a new matrix that is the same size as matrix X and Y as shown in the following figure:</p> <p> </p> <p>The code below calculates the element-wise product of the tensor X and Y:</p> <pre><code># Calculate [[1, 0], [0, 1]] * [[2, 1], [1, 2]]\n\nX = torch.tensor([[1, 0], [0, 1]])\nY = torch.tensor([[2, 1], [1, 2]]) \nX_times_Y = X * Y\nprint(\"The result of X * Y: \", X_times_Y)\n</code></pre> <p>This is a simple calculation. The result from the code matches the result shown in the picture.</p> <p>We can also apply matrix multiplication to two tensors, if you have learned linear algebra, you should know that in the multiplication of two matrices order matters. This means if X * Y is valid, it does not mean Y * X is valid. The number of columns of the matrix on the left side of the multiplication sign must equal to the number of rows of the matrix on the right side.</p> <p>First, let us create a tensor <code>X</code> with size 2X3. Then, let us create another tensor <code>Y</code> with size 3X2. Since the number of columns of <code>X</code> is equal to the number of rows of <code>Y</code>. We are able to perform the multiplication.</p> <p>We use <code>torch.mm()</code> for calculating the multiplication between tensors with different sizes.</p> <pre><code># Calculate [[0, 1, 1], [1, 0, 1]] * [[1, 1], [1, 1], [-1, 1]]\n\nA = torch.tensor([[0, 1, 1], [1, 0, 1]])\nB = torch.tensor([[1, 1], [1, 1], [-1, 1]])\nA_times_B = torch.mm(A,B)\nprint(\"The result of A * B: \", A_times_B)\n</code></pre> <p>Try to create your own two tensors (<code>X</code> and <code>Y</code>) with different sizes, and multiply them.</p> <pre><code># Practice: Calculate the product of two tensors (X and Y) with different sizes \n\n# Type your code here\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#two-dimensional-tensors","title":"Two-Dimensional Tensors","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#objective","title":"Objective","text":"<p><ul><li> How to perform tensor operations on 2D tensors.</li></ul></p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#table-of-contents","title":"Table of Contents","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#Types_Shape","title":"Types and Shape","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#Index_Slice","title":"Indexing and Slicing","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#let-us-see-how-we-use-slicing-with-2d-tensors-to-get-the-values-in-the-above-picture","title":"Let us see how  we use slicing with 2D tensors to get the values in the above picture.","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#practice_1","title":"Practice","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#Tensor_Op","title":"Tensor Operations","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#tensor-addition","title":"Tensor Addition","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#scalar-multiplication","title":"Scalar Multiplication","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#element-wise-producthadamard-product","title":"Element-wise Product/Hadamard Product","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#matrix-multiplication","title":"Matrix Multiplication","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#practice_2","title":"Practice","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.2derivativesandGraphsinPytorch_v2/","title":"Derivatives & Graphs","text":"<p>title: Differentiation in PyTorch author: Juma Shafara date: \"2023-09\" date-modified: \"2024-07-30\" description: In this lab, you will learn the basics of tensor operations on 2D tensors. keywords: []</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>The following are the libraries we are going to use for this lab.</p> <pre><code># These are the libraries will be useing for this lab.\n\nimport torch \nimport matplotlib.pylab as plt\n</code></pre> <p>Let us create the tensor <code>x</code> and set the parameter <code>requires_grad</code> to true because you are going to take the derivative of the tensor.</p> <pre><code># Create a tensor x\n\nx = torch.tensor(2.0, requires_grad = True)\nprint(\"The tensor x: \", x)\n</code></pre> <pre>\n<code>The tensor x:  tensor(2., requires_grad=True)\n</code>\n</pre> <p>Then let us create a tensor according to the equation \\(y=x^2\\).</p> <pre><code># Create a tensor y according to y = x^2\n\ny = x ** 2\nprint(\"The result of y = x^2: \", y)\n</code></pre> <pre>\n<code>The result of y = x^2:  tensor(4., grad_fn=&lt;PowBackward0&gt;)\n</code>\n</pre> <p>Then let us take the derivative with respect x at x = 2</p> <pre><code># Take the derivative. Try to print out the derivative at the value x = 2\n\ny.backward()\nprint(\"The dervative at x = 2: \", x.grad)\n</code></pre> <pre>\n<code>The dervative at x = 2:  tensor(4.)\n</code>\n</pre> <p>The preceding lines perform the following operation: </p> <p>\\(\\frac{\\mathrm{dy(x)}}{\\mathrm{dx}}=2x\\)</p> <p>\\(\\frac{\\mathrm{dy(x=2)}}{\\mathrm{dx}}=2(2)=4\\)</p> <pre><code>print('data:',x.data)\nprint('grad_fn:',x.grad_fn)\nprint('grad:',x.grad)\nprint(\"is_leaf:\",x.is_leaf)\nprint(\"requires_grad:\",x.requires_grad)\n</code></pre> <pre>\n<code>data: tensor(2.)\ngrad_fn: None\ngrad: tensor(4.)\nis_leaf: True\nrequires_grad: True\n</code>\n</pre> <pre><code>print('data:',y.data)\nprint('grad_fn:',y.grad_fn)\nprint('grad:',y.grad)\nprint(\"is_leaf:\",y.is_leaf)\nprint(\"requires_grad:\",y.requires_grad)\n</code></pre> <pre>\n<code>data: tensor(4.)\ngrad_fn: &lt;PowBackward0 object at 0x7873c29f8070&gt;\ngrad: None\nis_leaf: False\nrequires_grad: True\n</code>\n</pre> <pre>\n<code>/tmp/ipykernel_17856/1355624623.py:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n  print('grad:',y.grad)\n</code>\n</pre> <p>Let us try to calculate the derivative for a more complicated function. </p> <pre><code># Calculate the y = x^2 + 2x + 1, then find the derivative \n\nx = torch.tensor(2.0, requires_grad = True)\ny = x ** 2 + 2 * x + 1\nprint(\"The result of y = x^2 + 2x + 1: \", y)\ny.backward()\nprint(\"The dervative at x = 2: \", x.grad)\n</code></pre> <pre>\n<code>The result of y = x^2 + 2x + 1:  tensor(9., grad_fn=&lt;AddBackward0&gt;)\nThe dervative at x = 2:  tensor(6.)\n</code>\n</pre> <p>The function is in the following form: \\(y=x^{2}+2x+1\\)</p> <p>The derivative is given by:</p> <p>\\(\\frac{\\mathrm{dy(x)}}{\\mathrm{dx}}=2x+2\\)</p> <p>\\(\\frac{\\mathrm{dy(x=2)}}{\\mathrm{dx}}=2(2)+2=6\\)</p> <p>Determine the derivative of \\(y = 2x^3+x\\) at \\(x=1\\)</p> <pre><code># Practice: Calculate the derivative of y = 2x^3 + x at x = 1\n\n# Type your code here\n</code></pre> <p>Double-click here for the solution.</p> <p>We can implement our own custom autograd Functions by subclassing     torch.autograd.Function and implementing the forward and backward passes     which operate on Tensors</p> <pre><code>class SQ(torch.autograd.Function):\n\n\n    @staticmethod\n    def forward(ctx,i):\n        \"\"\"\n        In the forward pass we receive a Tensor containing the input and return\n        a Tensor containing the output. ctx is a context object that can be used\n        to stash information for backward computation. You can cache arbitrary\n        objects for use in the backward pass using the ctx.save_for_backward method.\n        \"\"\"\n        result=i**2\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"\n        In the backward pass we receive a Tensor containing the gradient of the loss\n        with respect to the output, and we need to compute the gradient of the loss\n        with respect to the input.\n        \"\"\"\n        i, = ctx.saved_tensors\n        grad_output = 2*i\n        return grad_output\n</code></pre> <p>We can apply it the function  </p> <pre><code>x=torch.tensor(2.0,requires_grad=True )\nsq=SQ.apply\n\ny=sq(x)\ny\nprint(y.grad_fn)\ny.backward()\nx.grad\n</code></pre> <pre>\n<code>&lt;torch.autograd.function.SQBackward object at 0x7873c29c7df0&gt;\n</code>\n</pre> <pre>\n<code>tensor(4.)</code>\n</pre> <p>We can also calculate Partial Derivatives. Consider the function: \\(f(u,v)=vu+u^{2}\\)</p> <p>Let us create <code>u</code> tensor, <code>v</code> tensor and  <code>f</code> tensor</p> <pre><code># Calculate f(u, v) = v * u + u^2 at u = 1, v = 2\n\nu = torch.tensor(1.0,requires_grad=True)\nv = torch.tensor(2.0,requires_grad=True)\nf = u * v + u ** 2\nprint(\"The result of v * u + u^2: \", f)\n</code></pre> <pre>\n<code>The result of v * u + u^2:  tensor(3., grad_fn=&lt;AddBackward0&gt;)\n</code>\n</pre> <p>This is equivalent to the following: </p> <p>\\(f(u=1,v=2)=(2)(1)+1^{2}=3\\)</p> <p>Now let us take the derivative with respect to <code>u</code>:</p> <pre><code># Calculate the derivative with respect to u\n\nf.backward()\nprint(\"The partial derivative with respect to u: \", u.grad)\n</code></pre> <pre>\n<code>The partial derivative with respect to u:  tensor(4.)\n</code>\n</pre> <p>the expression is given by:</p> <p>\\(\\frac{\\mathrm{\\partial f(u,v)}}{\\partial {u}}=v+2u\\)</p> <p>\\(\\frac{\\mathrm{\\partial f(u=1,v=2)}}{\\partial {u}}=2+2(1)=4\\)</p> <p>Now, take the derivative with respect to <code>v</code>:</p> <pre><code># Calculate the derivative with respect to v\n\nprint(\"The partial derivative with respect to u: \", v.grad)\n</code></pre> <pre>\n<code>The partial derivative with respect to u:  tensor(1.)\n</code>\n</pre> <p>The equation is given by:</p> <p>\\(\\frac{\\mathrm{\\partial f(u,v)}}{\\partial {v}}=u\\)</p> <p>\\(\\frac{\\mathrm{\\partial f(u=1,v=2)}}{\\partial {v}}=1\\)</p> <p>Calculate the derivative with respect to a function with multiple values as follows. You use the sum trick to produce a scalar valued function and then take the gradient: </p> <pre><code># Calculate the derivative with multiple values\n\nx = torch.linspace(-10, 10, 10, requires_grad = True)\nY = x ** 2\ny = torch.sum(x ** 2)\n</code></pre> <p>We can plot the function  and its derivative </p> <pre><code># Take the derivative with respect to multiple value. Plot out the function and its derivative\n\ny.backward()\n\nplt.plot(x.detach().numpy(), Y.detach().numpy(), label = 'function')\nplt.plot(x.detach().numpy(), x.grad.detach().numpy(), label = 'derivative')\nplt.xlabel('x')\nplt.legend()\nplt.show()\n</code></pre> <p>The orange line is the slope of the blue line at the intersection point, which is the derivative of the blue line.</p> <p>The  method <code> detach()</code>  excludes further tracking of operations in the graph, and therefore the subgraph will not record operations. This allows us to then convert the tensor to a numpy array. To understand the sum operation  Click Here</p> <p>The relu activation function is an essential function in neural networks. We can take the derivative as follows: </p> <pre><code># Take the derivative of Relu with respect to multiple value. Plot out the function and its derivative\n\nx = torch.linspace(-10, 10, 1000, requires_grad = True)\nY = torch.relu(x)\ny = Y.sum()\ny.backward()\nplt.plot(x.detach().numpy(), Y.detach().numpy(), label = 'function')\nplt.plot(x.detach().numpy(), x.grad.detach().numpy(), label = 'derivative')\nplt.xlabel('x')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>y.grad_fn\n</code></pre> <pre>\n<code>&lt;SumBackward0 at 0x7873545dfc40&gt;</code>\n</pre> <p>Try to determine partial derivative  \\(u\\) of the following function where \\(u=2\\) and \\(v=1\\): $ f=uv+(uv)^2$</p> <pre><code># Practice: Calculate the derivative of f = u * v + (u * v) ** 2 at u = 2, v = 1\n\n# Type the code here\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.2derivativesandGraphsinPytorch_v2/#objective","title":"Objective","text":"<p><ul><li> How to perform differentiation in pytorch.</li></ul></p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.2derivativesandGraphsinPytorch_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will learn the basics of differentiation.</p> <ul> <li>Derivatives</li> <li>Partial Derivatives</li> </ul> <p>Estimated Time Needed: 25 min</p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.2derivativesandGraphsinPytorch_v2/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.2derivativesandGraphsinPytorch_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.2derivativesandGraphsinPytorch_v2/#Derivative","title":"Derivatives","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.2derivativesandGraphsinPytorch_v2/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.2derivativesandGraphsinPytorch_v2/#Partial_Derivative","title":"Partial Derivatives","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.2derivativesandGraphsinPytorch_v2/#practice_1","title":"Practice","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.2derivativesandGraphsinPytorch_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.1_simple_data_set_v2/","title":"Simple Datasets","text":"<p>title: Simple Dataset author: Juma Shafara date: \"2023-09\" date-modified: \"2024-09-02\" description: In this lab, you will construct a basic dataset by using PyTorch and learn how to apply basic transformations to it. keywords: [     custom dataset classes in pytorch,     Simple dataset,     Transforms,     Compose, ]</p> <p></p> <p>In this lab, you will construct a basic dataset by using PyTorch and learn how to apply basic transformations to it.</p> <ul> <li>Simple dataset</li> <li>Transforms</li> <li>Compose</li> </ul> <p>Estimated Time Needed: 30 min</p> <p>The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.</p> <pre><code># These are the libraries will be used for this lab.\n\nimport torch\nfrom torch.utils.data import Dataset\ntorch.manual_seed(1)\n</code></pre> <pre>\n<code>&lt;torch._C.Generator at 0x71849b542f70&gt;</code>\n</pre> <p>Let us try to create our own dataset class.</p> <pre><code># Define class for dataset\n\nclass toy_set(Dataset):\n\n    # Constructor with defult values \n    def __init__(self, length = 10, transform = None):\n        self.len = length\n        self.x = 2 * torch.ones(length, 2)\n        self.y = torch.ones(length, 1)\n        self.transform = transform\n\n    # Getter\n    def __getitem__(self, index):\n        sample = self.x[index], self.y[index]\n        if self.transform:\n            sample = self.transform(sample)     \n        return sample\n\n    # Get Length\n    def __len__(self):\n        return self.len\n</code></pre> <p>Now, let us create our <code>toy_set</code> object, and find out the value on index 1 and the length of the inital dataset</p> <pre><code># Create Dataset Object. Find out the value on index 1. Find out the length of Dataset Object.\n\nour_dataset = toy_set()\nprint(\"Our toy_set object: \", our_dataset)\nprint(\"Value on index 0 of our toy_set object: \", our_dataset[0])\nprint(\"Our toy_set length: \", len(our_dataset))\n</code></pre> <pre>\n<code>Our toy_set object:  &lt;__main__.toy_set object at 0x7184a1b0ec60&gt;\nValue on index 0 of our toy_set object:  (tensor([2., 2.]), tensor([1.]))\nOur toy_set length:  10\n</code>\n</pre> <p>As a result, we can apply the same indexing convention as a <code>list</code>, and apply the fuction <code>len</code> on the <code>toy_set</code> object. We are able to customize the indexing and length method by <code>def getitem(self, index)</code> and <code>def len(self)</code>.</p> <p>Now, let us print out the first 3 elements and assign them to x and y:</p> <pre><code># Use loop to print out first 3 elements in dataset\n\nfor i in range(3):\n    x, y=our_dataset[i]\n    print(\"index: \", i, '; x:', x, '; y:', y)\n</code></pre> <pre>\n<code>index:  0 ; x: tensor([2., 2.]) ; y: tensor([1.])\nindex:  1 ; x: tensor([2., 2.]) ; y: tensor([1.])\nindex:  2 ; x: tensor([2., 2.]) ; y: tensor([1.])\n</code>\n</pre> <p>The dataset object is an Iterable; as a result, we  apply the loop directly on the dataset object </p> <pre><code>for x,y in our_dataset:\n    print(' x:', x, 'y:', y)\n</code></pre> <pre>\n<code> x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n</code>\n</pre> <pre><code>import dataidea\n\n# load dataset\nmusic_data = dataidea.loadDataset('music')\n\n# get features and target\nfeatures = music_data.drop('genre', axis=1)\ntarget = music_data['genre']\n\n# display sample\nmusic_data.sample(n=3)\n</code></pre> age gender genre 4 29 1 Jazz 5 30 1 Jazz 18 35 1 Classical <pre><code># lets encode the target\nfrom sklearn.preprocessing import LabelEncoder\n\ntarget = LabelEncoder().fit_transform(target)\nprint(f'Encoded Genre: {target}')\n</code></pre> <pre>\n<code>Encoded Genre: [3 3 3 4 4 4 1 1 1 2 2 2 0 0 0 1 1 1 1 1]\n</code>\n</pre> <p>We can create a custom Class for this dataset as demonstrated below</p> <pre><code>class MusicDataset(Dataset):\n\n    def __init__(self):\n        self.features = torch.tensor(features.values, dtype=torch.float32)\n        self.target = torch.from_numpy(target)\n\n    def __getitem__(self, index):\n        x = self.features[index]\n        y = self.target[index]\n        return x, y\n\n    def __len__(self):\n        return len(self.features)\n</code></pre> <p>Now let's create a MusicDataset object and get use the methods to access some data and info</p> <pre><code>music_torch_dataset = MusicDataset()\n\n# Row 0 data\nprint(f\"Row 0: {music_torch_dataset[0]}\")\n\n# display no of rows\nprint(f\"Number of rows: {len(music_torch_dataset)}\")\n</code></pre> <pre>\n<code>Row 0: (tensor([20.,  1.]), tensor(3))\nNumber of rows: 20\n</code>\n</pre> <p>Let's have a look at the first 5 rows</p> <pre><code>for sample in range(5):\n    print(f\"Row {sample}: {music_torch_dataset[0]}\")\n</code></pre> <pre>\n<code>Row 0: (tensor([20.,  1.]), tensor(3))\nRow 1: (tensor([20.,  1.]), tensor(3))\nRow 2: (tensor([20.,  1.]), tensor(3))\nRow 3: (tensor([20.,  1.]), tensor(3))\nRow 4: (tensor([20.,  1.]), tensor(3))\n</code>\n</pre> <p>Try to create an <code>toy_set</code> object with length 50. Print out the length of your object.</p> <pre><code># Practice: Create a new object with length 50, and print the length of object out.\n\n# Type your code here\n</code></pre> <p>Double-click here for the solution.</p> <p>You can also create a class for transforming the data. In this case, we will try to add 1 to x and multiply y by 2:</p> <pre><code># Create tranform class add_mult\n\nclass add_mult(object):\n\n    # Constructor\n    def __init__(self, addx = 1, muly = 2):\n        self.addx = addx\n        self.muly = muly\n\n    # Executor\n    def __call__(self, sample):\n        x = sample[0]\n        y = sample[1]\n        x = x + self.addx\n        y = y * self.muly\n        sample = x, y\n        return sample\n</code></pre> <p>Now, create a transform object:.</p> <pre><code># Create an add_mult transform object, and an toy_set object\n\na_m = add_mult()\ndata_set = toy_set()\n</code></pre> <p>Assign the outputs of the original dataset to <code>x</code> and <code>y</code>. Then, apply the transform <code>add_mult</code> to the dataset and output the values as <code>x_</code> and <code>y_</code>, respectively: </p> <pre><code># Use loop to print out first 10 elements in dataset\n\nfor i in range(10):\n    x, y = data_set[i]\n    print('Index: ', i, 'Original x: ', x, 'Original y: ', y)\n    x_, y_ = a_m(data_set[i])\n    print('Index: ', i, 'Transformed x_:', x_, 'Transformed y_:', y_)\n</code></pre> <p>As the result, <code>x</code> has been added by 1 and y has been multiplied by 2, as [2, 2] + 1 = [3, 3] and [1] x 2 = [2]</p> <p>We can apply the transform object every time we create a new <code>toy_set object</code>? Remember, we have the constructor in toy_set class with the parameter <code>transform = None</code>. When we create a new object using the constructor, we can assign the transform object to the parameter transform, as the following code demonstrates.</p> <pre><code># Create a new data_set object with add_mult object as transform\n\ncust_data_set = toy_set(transform = a_m)\n</code></pre> <p>This applied <code>a_m</code> object (a transform method) to every element in <code>cust_data_set</code> as initialized. Let us print out the first 10 elements in <code>cust_data_set</code> in order to see whether the <code>a_m</code> applied on <code>cust_data_set</code></p> <pre><code># Use loop to print out first 10 elements in dataset\n\nfor i in range(10):\n    x, y = data_set[i]\n    print('Index: ', i, 'Original x: ', x, 'Original y: ', y)\n    x_, y_ = cust_data_set[i]\n    print('Index: ', i, 'Transformed x_:', x_, 'Transformed y_:', y_)\n</code></pre> <p>The result is the same as the previous method.</p> <pre><code># Practice: Construct your own my_add_mult transform. Apply my_add_mult on a new toy_set object. Print out the first three elements from the transformed dataset.\n\n# Type your code here.\n</code></pre> <p>Double-click here for the solution.</p> <p>You can compose multiple transforms on the dataset object. First, import <code>transforms</code> from <code>torchvision</code>:</p> <pre><code># Run the command below when you do not have torchvision installed\n# !mamba install -y torchvision\n\nfrom torchvision import transforms\n</code></pre> <p>Then, create a new transform class that multiplies each of the elements by 100: </p> <pre><code># Create tranform class mult\n\nclass mult(object):\n\n    # Constructor\n    def __init__(self, mult = 100):\n        self.mult = mult\n\n    # Executor\n    def __call__(self, sample):\n        x = sample[0]\n        y = sample[1]\n        x = x * self.mult\n        y = y * self.mult\n        sample = x, y\n        return sample\n</code></pre> <p>Now let us try to combine the transforms <code>add_mult</code> and <code>mult</code></p> <pre><code># Combine the add_mult() and mult()\n\ndata_transform = transforms.Compose([add_mult(), mult()])\nprint(\"The combination of transforms (Compose): \", data_transform)\n</code></pre> <p>The new <code>Compose</code> object will perform each transform concurrently as shown in this figure:</p> <p></p> <pre><code>data_transform(data_set[0])\n</code></pre> <pre><code>x,y=data_set[0]\nx_,y_=data_transform(data_set[0])\nprint( 'Original x: ', x, 'Original y: ', y)\n\nprint( 'Transformed x_:', x_, 'Transformed y_:', y_)\n</code></pre> <p>Now we can pass the new <code>Compose</code> object (The combination of methods <code>add_mult()</code> and <code>mult</code>) to the constructor for creating <code>toy_set</code> object.</p> <pre><code># Create a new toy_set object with compose object as transform\n\ncompose_data_set = toy_set(transform = data_transform)\n</code></pre> <p>Let us print out the first 3 elements in different <code>toy_set</code> datasets in order to compare the output after different transforms have been applied: </p> <pre><code># Use loop to print out first 3 elements in dataset\n\nfor i in range(3):\n    x, y = data_set[i]\n    print('Index: ', i, 'Original x: ', x, 'Original y: ', y)\n    x_, y_ = cust_data_set[i]\n    print('Index: ', i, 'Transformed x_:', x_, 'Transformed y_:', y_)\n    x_co, y_co = compose_data_set[i]\n    print('Index: ', i, 'Compose Transformed x_co: ', x_co ,'Compose Transformed y_co: ',y_co)\n</code></pre> <p>Let us see what happened on index 0. The original value of <code>x</code> is [2, 2], and the original value of <code>y</code> is [1]. If we only applied <code>add_mult()</code> on the original dataset, then the <code>x</code> became [3, 3] and y became [2]. Now let us see what is the value after applied both <code>add_mult()</code> and <code>mult()</code>. The result of x is [300, 300] and y is [200]. The calculation which is equavalent to the compose is  x = ([2, 2] + 1) x 100 = [300, 300], y = ([1] x 2) x 100 = 200</p> <p>Try to combine the <code>mult()</code> and <code>add_mult()</code> as <code>mult()</code> to be executed first. And apply this on a new <code>toy_set</code> dataset. Print out the first 3 elements in the transformed dataset.</p> <pre><code># Practice: Make a compose as mult() execute first and then add_mult(). Apply the compose on toy_set dataset. Print out the first 3 elements in the transformed dataset.\n\n# Type your code here.\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.1_simple_data_set_v2/#objective","title":"Objective","text":"<p><ul><li> How to create a dataset in pytorch.</li><li> How to perform transformations on the dataset.</li></ul></p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.1_simple_data_set_v2/#table-of-contents","title":"Table of Contents","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.1_simple_data_set_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.1_simple_data_set_v2/#simple-dataset","title":"Simple dataset","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.1_simple_data_set_v2/#an-existing-dataset","title":"An existing dataset","text":"<p>For purposes of learning, we will use a simple dataset from the <code>dataidea</code> package called music. It's made up of two features, <code>age</code> and <code>gender</code> and outcome variable as <code>genre</code></p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.1_simple_data_set_v2/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.1_simple_data_set_v2/#transforms","title":"Transforms","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.1_simple_data_set_v2/#compose","title":"Compose","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.1_simple_data_set_v2/#practice_1","title":"Practice","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.3_pre-Built%20Datasets_and_transforms_v2/","title":"Pre-Built Datasets & Transforms","text":"<p>title: Prebuilt Datasets and Transforms author: Juma Shafara date: \"2023-09\" date-modified: \"2024-07-30\" description: In this lab, you will use a prebuilt dataset and then use some prebuilt dataset transforms. keywords: []</p> <p></p> <p>The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.</p> <pre><code># These are the libraries will be used for this lab.\n\n# !pip install torchvision==0.9.1 torch==1.8.1 \nimport torch \nimport matplotlib.pylab as plt\nimport numpy as np\ntorch.manual_seed(0)\n</code></pre> <p>This is the function for displaying images.</p> <pre><code># Show data by diagram\n\ndef show_data(data_sample, shape = (28, 28)):\n    plt.imshow(data_sample[0].numpy().reshape(shape), cmap='gray')\n    plt.title('y = ' + str(data_sample[1]))\n</code></pre> <p>You will focus on the following libraries: </p> <pre><code># Run the command below when you do not have torchvision installed\n# !mamba install -y torchvision\n\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n</code></pre> <p>We can import a prebuilt dataset. In this case, use MNIST. You'll work with several of these parameters later by placing a transform object in the argument <code>transform</code>.</p> <pre><code># Import the prebuilt dataset into variable dataset\n\n\ndataset = dsets.MNIST(\n    root = './data',  \n    download = True, \n    transform = transforms.ToTensor()\n)\n</code></pre> <p>Each element of the dataset object contains a tuple. Let us see whether the first element in the dataset is a tuple and what is in it.</p> <pre><code># Examine whether the elements in dataset MNIST are tuples, and what is in the tuple?\n\nprint(\"Type of the first element: \", type(dataset[0]))\nprint(\"The length of the tuple: \", len(dataset[0]))\nprint(\"The shape of the first element in the tuple: \", dataset[0][0].shape)\nprint(\"The type of the first element in the tuple\", type(dataset[0][0]))\nprint(\"The second element in the tuple: \", dataset[0][1])\nprint(\"The type of the second element in the tuple: \", type(dataset[0][1]))\nprint(\"As the result, the structure of the first element in the dataset is (tensor([1, 28, 28]), tensor(7)).\")\n</code></pre> <p>As shown in the output, the first element in the tuple is a cuboid tensor. As you can see, there is a dimension with only size 1, so basically, it is a rectangular tensor. The second element in the tuple is a number tensor, which indicate the real number the image shows. As the second element in the tuple is <code>tensor(7)</code>, the image should show a hand-written 7.</p> <p>Let us plot the first element in the dataset:</p> <pre><code># Plot the first element in the dataset\n\nshow_data(dataset[0])\n</code></pre> <p>As we can see, it is a 7.</p> <p>Plot the second sample:   </p> <pre><code># Plot the second element in the dataset\n\nshow_data(dataset[1])\n</code></pre> <p>We can apply some image transform functions on the MNIST dataset.</p> <p>As an example, the images in the MNIST dataset can be cropped and converted to a tensor. We can use <code>transform.Compose</code> we learned from the previous lab to combine the two transform functions.</p> <pre><code># Combine two transforms: crop and convert to tensor. Apply the compose to MNIST dataset\n\ncroptensor_data_transform = transforms.Compose([transforms.CenterCrop(20), transforms.ToTensor()])\ndataset = dsets.MNIST(root = './data', download = True, transform = croptensor_data_transform)\nprint(\"The shape of the first element in the first tuple: \", dataset[0][0].shape)\n</code></pre> <pre>\n<code>The shape of the first element in the first tuple:  torch.Size([1, 20, 20])\n</code>\n</pre> <p>We can see the image is now 20 x 20 instead of 28 x 28.</p> <p>Let us plot the first image again. Notice that the black space around the 7 become less apparent.</p> <pre><code># Plot the first element in the dataset\n\nshow_data(dataset[0],shape = (20, 20))\n</code></pre> <pre><code># Plot the second element in the dataset\n\nshow_data(dataset[1],shape = (20, 20))\n</code></pre> <p>In the below example, we horizontally flip the image, and then convert it to a tensor. Use <code>transforms.Compose()</code> to combine these two transform functions. Plot the flipped image.</p> <pre><code># Construct the compose. Apply it on MNIST dataset. Plot the image out.\n\nfliptensor_data_transform = transforms.Compose([transforms.RandomHorizontalFlip(p = 1),transforms.ToTensor()])\ndataset = dsets.MNIST(root = './data', download = True, transform = fliptensor_data_transform)\n# show_data(dataset[1])\n</code></pre> <p>Try to use the <code>RandomVerticalFlip</code> (vertically flip the image) with horizontally flip and convert to tensor as a compose. Apply the compose on image. Use <code>show_data()</code> to plot the second image (the image as 2).</p> <pre><code># Practice: Combine vertical flip, horizontal flip and convert to tensor as a compose. Apply the compose on image. Then plot the image\nrandom_vertical_flip = transforms.Compose([transforms.RandomVerticalFlip(), transforms.ToTensor()])\ndataset = dsets.MNIST(root= \".data\", download=True, transform=fliptensor_data_transform)\nshow_data(dataset[0])\n# Type your code here\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.3_pre-Built%20Datasets_and_transforms_v2/#prebuilt-datasets-and-transforms","title":"Prebuilt Datasets and Transforms","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.3_pre-Built%20Datasets_and_transforms_v2/#objective","title":"Objective","text":"<p><ul><li> How to use MNIST prebuilt dataset in pytorch.</li></ul></p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.3_pre-Built%20Datasets_and_transforms_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will use a prebuilt dataset and then use some prebuilt dataset transforms.</p> <ul> <li>Prebuilt Datasets</li> <li>Torchvision Transforms</li> </ul> <p>Estimated Time Needed: 10 min</p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.3_pre-Built%20Datasets_and_transforms_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.3_pre-Built%20Datasets_and_transforms_v2/#Prebuilt_Dataset","title":"Prebuilt Datasets","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.3_pre-Built%20Datasets_and_transforms_v2/#Torchvision","title":"Torchvision Transforms","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.3_pre-Built%20Datasets_and_transforms_v2/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/tensors_and_datasets_exercise/","title":"Exercise","text":"<p>title: Tensors &amp; Datasets Exercise author: Juma Shafara date: \"2024-08-29\" keywords: [data science, data analysis, programming, dataidea] description: Programming for Data Science is a subject we\u2019ve designed to explore the various programming components of data science.</p> <p></p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/tensors_and_datasets_exercise/#week-1-tensors-and-datasets-5-questions","title":"Week 1: Tensors and Datasets (5 Questions)","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/tensors_and_datasets_exercise/#1-1-dimension-tensors","title":"1. 1 Dimension Tensors:","text":"<ul> <li>Exercise: Create a 1D tensor with 10 elements ranging from 0 to 9. Perform the following operations: find the mean, sum, and standard deviation of the tensor. Print the results.</li> </ul>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/tensors_and_datasets_exercise/#2-two-dimension-tensors","title":"2. Two Dimension Tensors:","text":"<ul> <li>Exercise: Generate a 2D tensor with shape (3, 4) filled with random numbers. Perform matrix multiplication with another 2D tensor of shape (4, 2) filled with ones. Print the resulting tensor and its shape.</li> </ul>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/tensors_and_datasets_exercise/#3-derivatives-and-graphs-in-pytorch","title":"3. Derivatives and Graphs in PyTorch:","text":"<ul> <li>Exercise: Define a simple function \\(f(x) = x^2\\) in PyTorch and compute its derivative at \\(x = 3\\). Use PyTorch\u2019s autograd to compute the gradient.</li> </ul> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/tensors_and_datasets_exercise/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/tensors_and_datasets_exercise/#4-simple-dataset","title":"4. Simple Dataset:","text":"<ul> <li>Exercise: Create a custom dataset of 100 samples with 1 feature and a corresponding label using PyTorch\u2019s <code>Dataset</code> class. Implement the <code>__getitem__</code> and <code>__len__</code> methods. Print the first 5 samples.</li> </ul>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/tensors_and_datasets_exercise/#5-pre-built-datasets","title":"5. Pre Built Datasets:","text":"<ul> <li>Exercise: Load the MNIST dataset using PyTorch\u2019s <code>torchvision.datasets</code>. Display the first image in the dataset along with its label.</li> </ul>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/tensors_and_datasets_exercise/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.1Prediction1Dregression_v3/","title":"1D Regression","text":"<p>title: Linear Regression 1D, Prediction author: Juma Shafara date: \"2023-09\" date-modified: \"2024-07-30\" description: In this lab, we will review how to make a prediction in several different ways by using PyTorch. keywords: []</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>The following are the libraries we are going to use for this lab.</p> <pre><code># These are the libraries will be used for this lab.\n\nimport torch\n</code></pre> <p>Let us create the following expressions:</p> <p>\\(b=-1,w=2\\)</p> <p>\\(\\hat{y}=-1+2x\\)</p> <p>First, define the parameters:</p> <pre><code># Define w = 2 and b = -1 for y = wx + b\n\nw = torch.tensor(2.0, requires_grad = True)\nb = torch.tensor(-1.0, requires_grad = True)\n</code></pre> <p>Then, define the function <code>forward(x, w, b)</code> makes the prediction: </p> <pre><code># Function forward(x) for prediction\n\ndef forward(x):\n    yhat = w * x + b\n    return yhat\n</code></pre> <p>Let's make the following prediction at x = 1</p> <p>\\(\\hat{y}=-1+2x\\)</p> <p>\\(\\hat{y}=-1+2(1)\\)</p> <pre><code>m = torch.tensor([2])\nprint(m)\n\nforward(m)\n</code></pre> <pre>\n<code>tensor([2])\n</code>\n</pre> <pre>\n<code>tensor([3.], grad_fn=&lt;AddBackward0&gt;)</code>\n</pre> <pre><code># Predict y = 2x - 1 at x = 1\n\nx = torch.tensor([[1.0]])\nyhat = forward(x)\nprint(\"The prediction: \", yhat)\n</code></pre> <pre>\n<code>The prediction:  tensor([[1.]], grad_fn=&lt;AddBackward0&gt;)\n</code>\n</pre> <p>Now, let us try to make the prediction for multiple inputs:</p> <p></p> <p>Let us construct the <code>x</code> tensor first. Check the shape of <code>x</code>.</p> <pre><code># Create x Tensor and check the shape of x tensor\n\nx = torch.tensor([[1.0], [2.0]])\nprint(\"The shape of x: \", x.shape)\n</code></pre> <pre>\n<code>The shape of x:  torch.Size([2, 1])\n</code>\n</pre> <p>Now make the prediction: </p> <pre><code># Make the prediction of y = 2x - 1 at x = [1, 2]\n\nyhat = forward(x)\nprint(\"The prediction: \", yhat)\n</code></pre> <pre>\n<code>The prediction:  tensor([[1.],\n        [3.]], grad_fn=&lt;AddBackward0&gt;)\n</code>\n</pre> <p>The result is the same as what it is in the image above.</p> <p>Make a prediction of the following <code>x</code> tensor using the <code>w</code> and <code>b</code> from above.</p> <pre><code># Practice: Make a prediction of y = 2x - 1 at x = [[1.0], [2.0], [3.0]]\n\nx = torch.tensor([[1.0], [2.0], [3.0]])\nyhat = forward(x)\nprint(\"The prediction: \", yhat)\n</code></pre> <pre>\n<code>The prediction:  tensor([[1.],\n        [3.],\n        [5.]], grad_fn=&lt;AddBackward0&gt;)\n</code>\n</pre> <p>Double-click here for the solution.</p> <p>The linear class can be used to make a prediction. We can also use the linear class to build more complex models. Let's import the module:</p> <pre><code># Import Class Linear\n\nfrom torch.nn import Linear\n</code></pre> <p>Set the random seed because the parameters are randomly initialized:</p> <pre><code># Set random seed\n\ntorch.manual_seed(1)\n</code></pre> <pre>\n<code>&lt;torch._C.Generator at 0x7c780c384eb0&gt;</code>\n</pre> <p>Let us create the linear object by using the constructor. The parameters are randomly created. Let us print out to see what w and b. The parameters of an <code>torch.nn.Module</code> model are contained in the model\u2019s parameters accessed with <code>lr.parameters()</code>:</p> <pre><code># Create Linear Regression Model, and print out the parameters\n\nlr = Linear(in_features=1, out_features=1, bias=True)\nprint(\"Parameters w and b: \", list(lr.parameters()))\n</code></pre> <pre>\n<code>Parameters w and b:  [Parameter containing:\ntensor([[0.2772]], requires_grad=True), Parameter containing:\ntensor([-0.3058], requires_grad=True)]\n</code>\n</pre> <p>This is equivalent to the following expression:  </p> <p>\\(b=-0.44, w=0.5153\\)</p> <p>\\(\\hat{y}=-0.44+0.5153x\\)</p> <p>A method  <code>state_dict()</code> Returns a Python dictionary object corresponding to the layers of each parameter  tensor. </p> <pre><code>print(\"Python dictionary: \",lr.state_dict())\nprint(\"keys: \",lr.state_dict().keys())\nprint(\"values: \",lr.state_dict().values())\n</code></pre> <pre>\n<code>Python dictionary:  OrderedDict({'weight': tensor([[0.3652]]), 'bias': tensor([-0.3897])})\nkeys:  odict_keys(['weight', 'bias'])\nvalues:  odict_values([tensor([[0.3652]]), tensor([-0.3897])])\n</code>\n</pre> <p>The keys correspond to the name of the attributes and the values correspond to the parameter value.</p> <pre><code>print(\"weight:\",lr.weight)\nprint(\"bias:\",lr.bias)\n</code></pre> <pre>\n<code>weight: Parameter containing:\ntensor([[0.3652]], requires_grad=True)\nbias: Parameter containing:\ntensor([-0.3897], requires_grad=True)\n</code>\n</pre> <p>Now let us make a single prediction at x = [[1.0]].</p> <pre><code># Make the prediction at x = [[1.0]]\n\nx = torch.tensor([[1.0]])\nyhat = lr(x)\nprint(\"The prediction: \", yhat)\n</code></pre> <pre>\n<code>The prediction:  tensor([[-0.0245]], grad_fn=&lt;AddmmBackward0&gt;)\n</code>\n</pre> <p>Similarly, you can make multiple predictions:</p> <p></p> <p>Use model <code>lr(x)</code> to predict the result.</p> <pre><code># Create the prediction using linear model\n\nx = torch.tensor([[1.0], [2.0]])\nyhat = lr(x)\nprint(\"The prediction: \", yhat)\n</code></pre> <pre>\n<code>The prediction:  tensor([[-0.0286],\n        [ 0.2487]], grad_fn=&lt;AddmmBackward0&gt;)\n</code>\n</pre> <p>Make a prediction of the following <code>x</code> tensor using the linear regression model <code>lr</code>.</p> <pre><code># Practice: Use the linear regression model object lr to make the prediction.\n\nx = torch.tensor([[1.0],[2.0],[3.0]])\nyhat = lr(x)\n\nprint(\"The prediction: \", yhat)\n</code></pre> <pre>\n<code>The prediction:  tensor([[-0.0286],\n        [ 0.2487],\n        [ 0.5259]], grad_fn=&lt;AddmmBackward0&gt;)\n</code>\n</pre> <p>Double-click here for the solution.</p> <p>Now, let's build a custom module. We can make more complex models by using this method later on. </p> <p>First, import the following library.</p> <pre><code># Library for this section\n\nfrom torch import nn\n</code></pre> <p>Now, let us define the class: </p> <pre><code># Customize Linear Regression Class\n\nclass LR(nn.Module):\n\n    # Constructor\n    def __init__(self, input_size, output_size):\n\n        # Inherit from parent\n        super(LR, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n\n    # Prediction function\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n</code></pre> <p>Create an object by using the constructor. Print out the parameters we get and the model.</p> <pre><code># Create the linear regression model. Print out the parameters.\n\nlr = LR(1, 1)\nprint(\"The parameters: \", list(lr.parameters()))\nprint(\"Linear model: \", lr.linear)\n</code></pre> <pre>\n<code>The parameters:  [Parameter containing:\ntensor([[-0.0729]], requires_grad=True), Parameter containing:\ntensor([-0.0900], requires_grad=True)]\nLinear model:  Linear(in_features=1, out_features=1, bias=True)\n</code>\n</pre> <p>Let us try to make a prediction of a single input sample.</p> <pre><code># Try our customize linear regression model with single input\n\nx = torch.tensor([[1.0]])\nyhat = lr(x)\nprint(\"The prediction: \", yhat)\n</code></pre> <pre>\n<code>The prediction:  tensor([[-0.1629]], grad_fn=&lt;AddmmBackward0&gt;)\n</code>\n</pre> <p>Now, let us try another example with multiple samples.</p> <pre><code># Try our customize linear regression model with multiple input\n\nx = torch.tensor([[1.0], [2.0]])\nyhat = lr(x)\nprint(\"The prediction: \", yhat)\n</code></pre> <pre>\n<code>The prediction:  tensor([[-0.1629],\n        [-0.2358]], grad_fn=&lt;AddmmBackward0&gt;)\n</code>\n</pre> <p>the parameters are also stored in an ordered dictionary :</p> <pre><code>print(\"Python dictionary: \", lr.state_dict())\nprint(\"keys: \",lr.state_dict().keys())\nprint(\"values: \",lr.state_dict().values())\n</code></pre> <pre>\n<code>Python dictionary:  OrderedDict([('weight', tensor([[0.5153]])), ('bias', tensor([-0.4414]))])\nkeys:  odict_keys(['weight', 'bias'])\nvalues:  odict_values([tensor([[0.5153]]), tensor([-0.4414])])\n</code>\n</pre> <p>Create an object <code>lr1</code> from the class we created before and make a prediction by using the following tensor: </p> <pre><code># Practice: Use the LR class to create a model and make a prediction of the following tensor.\n\nx = torch.tensor([[1.0], [2.0], [3.0]])\nlr1 = LR(1, 1)\nyhat = lr(x)\nyhat\n# print(\"The prediction: \", yhat)\n</code></pre> <pre>\n<code>tensor([[-0.1629],\n        [-0.2358],\n        [-0.3088]], grad_fn=&lt;AddmmBackward0&gt;)</code>\n</pre>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.1Prediction1Dregression_v3/#objective","title":"Objective","text":"<p><ul><li> How to make the prediction for multiple inputs.</li><li> How to use linear class to build more complex models.</li><li> How to build a custom module.</li></ul></p>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.1Prediction1Dregression_v3/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, we will  review how to make a prediction in several different ways by using PyTorch. </p> <ul> <li>Prediction</li> <li>Class Linear</li> <li>Build Custom Modules</li> </ul> <p>Estimated Time Needed: 15 min</p>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.1Prediction1Dregression_v3/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.1Prediction1Dregression_v3/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.1Prediction1Dregression_v3/#Prediction","title":"Prediction","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.1Prediction1Dregression_v3/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.1Prediction1Dregression_v3/#Linear","title":"Class Linear","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.1Prediction1Dregression_v3/#practice_1","title":"Practice","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.1Prediction1Dregression_v3/#Cust","title":"Build Custom Modules","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.1Prediction1Dregression_v3/#practice_2","title":"Practice","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.1Prediction1Dregression_v3/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.2_linear_regression_one_parameter_v3/","title":"One Parameter","text":"<p>title: Linear Regression 1D, Training One Parameter keywords: [] description: In this lab, you will train a model with PyTorch by using data that you created author: Juma Shafara date: \"2024-08-05\"</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>The following are the libraries we are going to use for this lab.</p> <pre><code># These are the libraries will be used for this lab.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n</code></pre> <p>The class <code>plot_diagram</code> helps us to visualize the data space and the parameter space during training and has nothing to do with PyTorch.</p> <pre><code># The class for plotting\n\nclass plot_diagram():\n\n    # Constructor\n    def __init__(self, X, Y, w, stop, go = False):\n        start = w.data\n        self.error = []\n        self.parameter = []\n        self.X = X.numpy()\n        self.Y = Y.numpy()\n        self.parameter_values = torch.arange(start, stop)\n        self.Loss_function = [criterion(forward(X), Y) for w.data in self.parameter_values] \n        w.data = start\n\n    # Executor\n    def __call__(self, Yhat, w, error, n):\n        self.error.append(error)\n        self.parameter.append(w.data)\n        plt.subplot(212)\n        plt.plot(self.X, Yhat.detach().numpy())\n        plt.plot(self.X, self.Y,'ro')\n        plt.xlabel(\"A\")\n        plt.ylim(-20, 20)\n        plt.subplot(211)\n        plt.title(\"Data Space (top) Estimated Line (bottom) Iteration \" + str(n))\n        plt.plot(self.parameter_values.numpy(), self.Loss_function)   \n        plt.plot(self.parameter, self.error, 'ro')\n        plt.xlabel(\"B\")\n        plt.figure()\n\n    # Destructor\n    def __del__(self):\n        plt.close('all')\n</code></pre> <p>Import PyTorch library:</p> <pre><code># Import the library PyTorch\n\nimport torch\n</code></pre> <p>Generate values from -3 to 3 that create a line with a slope of -3. This is the line you will estimate.</p> <pre><code># Create the f(X) with a slope of -3\n\nX = torch.arange(-3, 3, 0.1).view(-1, 1)\nf = -3 * X\n</code></pre> <p>Let us plot the line.</p> <pre><code># Plot the line with blue\n\nplt.plot(X.numpy(), f.numpy(), label = 'f')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n</code></pre> <p>Let us add some noise to the data in order to simulate the real data. Use <code>torch.randn(X.size())</code> to generate Gaussian noise that is the same size as <code>X</code> and has a standard deviation opf 0.1.</p> <pre><code># Add some noise to f(X) and save it in Y\n\nY = f + 0.1 * torch.randn(X.size())\n</code></pre> <p>Plot the <code>Y</code>:</p> <pre><code># Plot the data points\n\nplt.plot(X.numpy(), Y.numpy(), 'rx', label = 'Y')\n\nplt.plot(X.numpy(), f.numpy(), label = 'f')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n</code></pre> <p>In this section, let us create the model and the cost function (total loss) we are going to use to train the model and evaluate the result.</p> <p>First, define the <code>forward</code> function \\(y=w*x\\). (We will add the bias in the next lab.)</p> <pre><code># Create forward function for prediction\n\ndef forward(x):\n    return w * x\n</code></pre> <p>Define the cost or criterion function using MSE (Mean Square Error):</p> <pre><code># Create the MSE function for evaluate the result.\n\ndef criterion(yhat, y):\n    return torch.mean((yhat - y) ** 2)\n</code></pre> <p>Define the learning rate <code>lr</code> and an empty list <code>LOSS</code> to record the loss for each iteration:</p> <pre><code># Create Learning Rate and an empty list to record the loss for each iteration\n\nlr = 0.1\nLOSS = []\n</code></pre> <p>Now, we create a model parameter by setting the argument <code>requires_grad</code> to <code> True</code> because the system must learn it.</p> <pre><code>w = torch.tensor(-10.0, requires_grad = True)\n</code></pre> <p>Create a <code>plot_diagram</code> object to visualize the data space and the parameter space for each iteration during training:</p> <pre><code>gradient_plot = plot_diagram(X, Y, w, stop = 5)\n</code></pre> <p>Let us define a function for training the model. The steps will be described in the comments.</p> <pre><code># Define a function for train the model\n\ndef train_model(iter):\n    for epoch in range (iter):\n\n        # make the prediction as we learned in the last lab\n        Yhat = forward(X)\n\n        # calculate the iteration\n        loss = criterion(Yhat,Y)\n\n        # plot the diagram for us to have a better idea\n        # gradient_plot(Yhat, w, loss.item(), epoch)\n\n        # store the loss into list\n        LOSS.append(loss.item())\n\n        # backward pass: compute gradient of the loss with respect to all the learnable parameters\n        loss.backward()\n\n        # updata parameters\n        w.data = w.data - lr * w.grad.data\n\n        # zero the gradients before running the backward pass\n        w.grad.data.zero_()\n</code></pre> <p>Let us try to run 4 iterations of gradient descent:</p> <pre><code># Give 4 iterations for training the model here.\n\ntrain_model(4)\n</code></pre> <p>Plot the cost for each iteration:</p> <pre><code># Plot the loss for each iteration\n\nplt.plot(LOSS)\nplt.tight_layout()\nplt.xlabel(\"Epoch/Iterations\")\nplt.ylabel(\"Cost\")\n</code></pre> <pre>\n<code>Text(38.347222222222214, 0.5, 'Cost')</code>\n</pre>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.2_linear_regression_one_parameter_v3/#linear-regression-1d-training-one-parameter","title":"Linear Regression 1D: Training One Parameter","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.2_linear_regression_one_parameter_v3/#objective","title":"Objective","text":"<p><ul><li> How to create cost or criterion function using MSE (Mean Square Error).</li></ul></p>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.2_linear_regression_one_parameter_v3/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will train a model with PyTorch by using data that you created. The model only has one parameter: the slope.</p> <ul> <li>Make Some Data</li> <li>Create the Model and Cost Function (Total Loss)</li> <li>Train the Model</li> </ul> <p>Estimated Time Needed: 20 min</p>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.2_linear_regression_one_parameter_v3/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.2_linear_regression_one_parameter_v3/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.2_linear_regression_one_parameter_v3/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.2_linear_regression_one_parameter_v3/#Model_Cost","title":"Create the Model and Cost Function (Total Loss)","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.2_linear_regression_one_parameter_v3/#Train","title":"Train the Model","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.2_linear_regression_one_parameter_v3/#about-the-author","title":"About the Author:","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning. I also enjoy developing innovative algorithms and models that can drive insights and value. I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org. Besides these //technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.2_linear_regression_one_parameter_v3/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.3_training_slope_and_bias_v3/","title":"Slope & Bias","text":"<p>author: Juma Shafara date: \"2024-08-06\" title: Linear regression 1D, Training Two Parameters keywords: [] description: In this lab, you will train a model with PyTorch by using the data that we created. The model will have the slope and bias.</p> <p></p> <pre><code>#| hide\n#| default_exp models\n</code></pre> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>We'll need the following libraries:  </p> <pre><code># These are the libraries we are going to use in the lab.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\n</code></pre> <p>The class <code>plot_error_surfaces</code> is just to help you visualize the data space and the parameter space during training and has nothing to do with PyTorch. </p> <pre><code>#| hide\n#| export\n# The class for plot the diagram\n\nclass plot_error_surfaces(object):\n\n    # Constructor\n    def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):\n        W = np.linspace(-w_range, w_range, n_samples)\n        B = np.linspace(-b_range, b_range, n_samples)\n        w, b = np.meshgrid(W, B)    \n        Z = np.zeros((30,30))\n        count1 = 0\n        self.y = Y.numpy()\n        self.x = X.numpy()\n        for w1, b1 in zip(w, b):\n            count2 = 0\n            for w2, b2 in zip(w1, b1):\n                Z[count1, count2] = np.mean((self.y - w2 * self.x + b2) ** 2)\n                count2 += 1\n            count1 += 1\n        self.Z = Z\n        self.w = w\n        self.b = b\n        self.W = []\n        self.B = []\n        self.LOSS = []\n        self.n = 0\n        if go == True:\n            plt.figure()\n            plt.figure(figsize = (7.5, 5))\n            plt.axes(projection='3d').plot_surface(self.w, self.b, self.Z, rstride = 1, cstride = 1,cmap = 'viridis', edgecolor = 'none')\n            plt.title('Cost/Total Loss Surface')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.show()\n            plt.figure()\n            plt.title('Cost/Total Loss Surface Contour')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.contour(self.w, self.b, self.Z)\n            plt.show()\n\n    # Setter\n    def set_para_loss(self, W, B, loss):\n        self.n = self.n + 1\n        self.W.append(W)\n        self.B.append(B)\n        self.LOSS.append(loss)\n\n    # Plot diagram\n    def final_plot(self): \n        ax = plt.axes(projection = '3d')\n        ax.plot_wireframe(self.w, self.b, self.Z)\n        ax.scatter(self.W,self.B, self.LOSS, c = 'r', marker = 'x', s = 200, alpha = 1)\n        plt.figure()\n        plt.contour(self.w,self.b, self.Z)\n        plt.scatter(self.W, self.B, c = 'r', marker = 'x')\n        plt.xlabel('w')\n        plt.ylabel('b')\n        plt.show()\n\n    # Plot diagram\n    def plot_ps(self):\n        plt.subplot(121)\n        plt.ylim\n        plt.plot(self.x, self.y, 'ro', label=\"training points\")\n        plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label = \"estimated line\")\n        plt.xlabel('x')\n        plt.ylabel('y')\n        plt.ylim((-10, 15))\n        plt.title('Data Space Iteration: ' + str(self.n))\n\n        plt.subplot(122)\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c = 'r', marker = 'x')\n        plt.title('Total Loss Surface Contour Iteration' + str(self.n))\n        plt.xlabel('w')\n        plt.ylabel('b')\n        plt.show()\n</code></pre> <p>Import PyTorch: </p> <pre><code># Import PyTorch library\n\nimport torch\n</code></pre> <p>Start with generating values from -3 to 3 that create a line with a slope of 1 and a bias of -1. This is the line that you need to estimate.</p> <pre><code># Create f(X) with a slope of 1 and a bias of -1\n\nX = torch.arange(-3, 3, 0.1).view(-1, 1)\nf = 1 * X - 1\n</code></pre> <p>Now, add some noise to the data:</p> <pre><code># Add noise\n\nY = f + 0.1 * torch.randn(X.size())\n</code></pre> <p>Plot the line and <code>Y</code> with noise:</p> <pre><code># Plot out the line and the points with noise\n\nplt.plot(X.numpy(), Y.numpy(), 'rx', label = 'y')\nplt.plot(X.numpy(), f.numpy(), label = 'f')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\n</code></pre> <pre>\n<code>&lt;matplotlib.legend.Legend at 0x708d3b7fbc80&gt;</code>\n</pre> <p>Define the <code>forward</code> function: </p> <pre><code># Define the forward function\n\ndef forward(x):\n    return w * x + b\n</code></pre> <p>Define the cost or criterion function (MSE): </p> <pre><code># Define the MSE Loss function\n\ndef criterion(yhat,y):\n    return torch.mean((yhat-y)**2)\n</code></pre> <p>Create a <code> plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> <pre><code># Create plot_error_surfaces for viewing the data\n\nget_surface = plot_error_surfaces(15, 15, X, Y, 30)\n</code></pre> <pre>\n<code>&lt;Figure size 640x480 with 0 Axes&gt;</code>\n</pre> <p>Create model parameters <code>w</code>, <code>b</code> by setting the argument <code>requires_grad</code> to True because we must learn it using the data.</p> <pre><code># Define the parameters w, b for y = wx + b\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\n</code></pre> <p>Set the learning rate to 0.1 and create an empty list <code>LOSS</code> for storing the loss for each iteration.</p> <pre><code># Define learning rate and create an empty list for containing the loss for each iteration.\n\nlr = 0.1\nLOSS = []\n</code></pre> <p>Define <code>train_model</code> function for train the model.</p> <pre><code># The function for training the model\n\ndef train_model(iter):\n\n    # Loop\n    for epoch in range(iter):\n\n        # make a prediction\n        Yhat = forward(X)\n\n        # calculate the loss \n        loss = criterion(Yhat, Y)\n\n        # Section for plotting\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n        if epoch % 3 == 0:\n            get_surface.plot_ps()\n\n        # store the loss in the list LOSS\n        LOSS.append(loss.item())\n\n        # backward pass: compute gradient of the loss with respect to all the learnable parameters\n        loss.backward()\n\n        # update parameters slope and bias\n        w.data = w.data - lr * w.grad.data\n        b.data = b.data - lr * b.grad.data\n\n        # zero the gradients before running the backward pass\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n</code></pre> <p>Run 15 iterations of gradient descent: bug data space is 1 iteration ahead of parameter space </p> <pre><code># Train the model with 15 iterations\n\ntrain_model(15)\n</code></pre> <p>Plot total loss/cost surface with loss values for different parameters in red:</p> <pre><code># Plot out the Loss Result\n\n\nget_surface.final_plot()\nplt.plot(LOSS)\nplt.tight_layout()\nplt.xlabel(\"Epoch/Iterations\")\nplt.ylabel(\"Cost\")\n</code></pre> <pre>\n<code>Text(38.347222222222214, 0.5, 'Cost')</code>\n</pre> <p>Experiment using s learning rates 0.2 and width the following parameters. Run 15 iterations.</p> <pre><code># Practice: train and plot the result with lr = 0.2 and the following parameters\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\nlr = 0.2\nLOSS2 = []\n</code></pre> <p>Double-click here for the solution.</p> <p>Plot the <code>LOSS</code> and <code>LOSS2</code></p> <pre><code># Practice: Plot the LOSS and LOSS2 in order to compare the Total Loss\n\n# Type your code here\nplt.plot(LOSS, label = \"LOSS\")\nplt.plot(LOSS2, label = \"LOSS2\")\nplt.tight_layout()\nplt.xlabel(\"Epoch/Iterations\")\nplt.ylabel(\"Cost\")\nplt.legend()\n</code></pre> <pre>\n<code>&lt;matplotlib.legend.Legend at 0x7ad8e1e85c40&gt;</code>\n</pre> <pre><code>#| hide\nimport nbdev; nbdev.nbdev_export()\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.3_training_slope_and_bias_v3/#objective","title":"Objective","text":"<p><ul><li> How to train the model and visualize the loss results.</li></ul></p>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.3_training_slope_and_bias_v3/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will train a model with PyTorch by using the data that we created. The model will have the slope and bias. And we will review how to make a prediction in several different ways by using PyTorch.</p> <ul> <li>Make Some Data</li> <li>Create the Model and Cost Function (Total Loss) </li> <li>Train the Model </li> </ul> <p>Estimated Time Needed: 20 min </p>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.3_training_slope_and_bias_v3/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.3_training_slope_and_bias_v3/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.3_training_slope_and_bias_v3/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.3_training_slope_and_bias_v3/#Model_Cost","title":"Create the Model and Cost Function (Total Loss)","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.3_training_slope_and_bias_v3/#Train","title":"Train the Model","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.3_training_slope_and_bias_v3/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.3_training_slope_and_bias_v3/#about-the-author","title":"About the Author:","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning. I also enjoy developing innovative algorithms and models that can drive insights and value. I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org. Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.3_training_slope_and_bias_v3/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/linear_regression_exercise/","title":"Exercise","text":"<p>title: Linear Regression Exercise author: Juma Shafara date: \"2024-08-29\" keywords: [data science, data analysis, programming, dataidea] description: Programming for Data Science is a subject we\u2019ve designed to explore the various programming components of data science.</p> <p></p>"},{"location":"Deep%20Learning/Week2-Linear-Regression/linear_regression_exercise/#week-2-linear-regression-3-questions","title":"Week 2: Linear Regression (3 Questions)","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/linear_regression_exercise/#1-linear-regression-1-dimension","title":"1. Linear Regression 1 Dimension","text":"<ul> <li>Exercise: Implement a simple linear regression model to fit a line to 1D data points generated as <code>(x, y) = (x, 3x + 7 + noise)</code>. Visualize the data and the fitted line.</li> </ul>"},{"location":"Deep%20Learning/Week2-Linear-Regression/linear_regression_exercise/#2-linear-regression-with-1-parameter","title":"2. Linear Regression with 1 Parameter","text":"<ul> <li>Exercise: Create a linear regression model with a single parameter. Train the model on a dataset with a known linear relationship, and evaluate the model\u2019s performance by plotting the predicted vs. actual values.</li> </ul> <p> Before the last question, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Deep%20Learning/Week2-Linear-Regression/linear_regression_exercise/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/linear_regression_exercise/#3-training-slope-and-bias","title":"3. Training Slope and Bias","text":"<ul> <li>Exercise: Implement a linear regression model from scratch to learn the slope and bias for the dataset <code>(x, y) = (x, 4x + 10 + noise)</code>. Compare the learned parameters with the true values.</li> </ul>"},{"location":"Deep%20Learning/Week2-Linear-Regression/linear_regression_exercise/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/","title":"SGD","text":"<p>author: Juma Shafara date: \"2024-08-07\" title: Training Two Parameter Stochastic Gradient Descent keywords: [] description: In this Lab, you will practice training a model by using Stochastic Gradient</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>We'll need the following libraries:  </p> <pre><code># These are the libraries we are going to use in the lab.\n\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits import mplot3d\nfrom dataidea_science.plots import plot_error_surfaces\n</code></pre> <p>The class <code>plot_error_surfaces</code> is just to help you visualize the data space and the parameter space during training and has nothing to do with PyTorch.</p> <p>Set random seed: </p> <pre><code># Set random seed\n\ntorch.manual_seed(1)\n</code></pre> <pre>\n<code>&lt;torch._C.Generator at 0x79c1633e7c10&gt;</code>\n</pre> <p>Generate values from -3 to 3 that create a line with a slope of 1 and a bias of -1. This is the line that you need to estimate. Add some noise to the data:</p> <pre><code># Setup the actual data and simulated data\n\nX = torch.arange(-3, 3, 0.1).view(-1, 1)\nf = 1 * X - 1\nY = f + 0.1 * torch.randn(X.size())\n</code></pre> <p>Plot the results:</p> <pre><code># Plot out the data dots and line\nfrom dataidea.models import *\n</code></pre> <p>Define the <code>forward</code> function:</p> <pre><code># Define the forward function\n\ndef forward(x):\n    return w * x + b\n</code></pre> <p>Define the cost or criterion function (MSE): </p> <pre><code># Define the MSE Loss function\n\ndef criterion(yhat, y):\n    return torch.mean((yhat - y) ** 2)\n</code></pre> <p>Create a <code> plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> <pre><code># Create plot_error_surfaces for viewing the data\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30)\n</code></pre> <pre>\n<code>&lt;Figure size 640x480 with 0 Axes&gt;</code>\n</pre> <p>Create model parameters <code>w</code>, <code>b</code> by setting the argument <code>requires_grad</code> to True because the system must learn it.</p> <pre><code># Define the parameters w, b for y = wx + b\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\n</code></pre> <p>Set the learning rate to  0.1 and create an empty list <code>LOSS</code> for storing the loss for each iteration.</p> <pre><code># Define learning rate and create an empty list for containing the loss for each iteration.\n\nlr = 0.1\nLOSS_BGD = []\n</code></pre> <p>Define <code>train_model</code> function for train the model.</p> <pre><code># The function for training the model\n\ndef train_model(iter):\n\n    # Loop\n    for epoch in range(iter):\n\n        # make a prediction\n        Yhat = forward(X)\n\n        # calculate the loss \n        loss = criterion(Yhat, Y)\n\n        # Section for plotting\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n        get_surface.plot_ps()\n\n        # store the loss in the list LOSS_BGD\n        LOSS_BGD.append(loss)\n\n        # backward pass: compute gradient of the loss with respect to all the learnable parameters\n        loss.backward()\n\n        # update parameters slope and bias\n        w.data = w.data - lr * w.grad.data\n        b.data = b.data - lr * b.grad.data\n\n        # zero the gradients before running the backward pass\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n</code></pre> <p>Run 10 epochs of batch gradient descent: bug data space is 1 iteration ahead of parameter space. </p> <pre><code># Train the model with 10 iterations\n\ntrain_model(10)\n</code></pre> <p>Create a <code>plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> <pre><code># Create plot_error_surfaces for viewing the data\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30, go = False)\n</code></pre> <p>Define <code>train_model_SGD</code> function for training the model.</p> <pre><code># The function for training the model\n\nLOSS_SGD = []\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\n\ndef train_model_SGD(iter):\n\n    # Loop\n    for epoch in range(iter):\n\n        # SGD is an approximation of out true total loss/cost, in this line of code we calculate our true loss/cost and store it\n        Yhat = forward(X)\n\n        # store the loss \n        LOSS_SGD.append(criterion(Yhat, Y).tolist())\n\n        for x, y in zip(X, Y):\n\n            # make a pridiction\n            yhat = forward(x)\n\n            # calculate the loss \n            loss = criterion(yhat, y)\n\n            # Section for plotting\n            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n\n            # backward pass: compute gradient of the loss with respect to all the learnable parameters\n            loss.backward()\n\n            # update parameters slope and bias\n            w.data = w.data - lr * w.grad.data\n            b.data = b.data - lr * b.grad.data\n\n            # zero the gradients before running the backward pass\n            w.grad.data.zero_()\n            b.grad.data.zero_()\n\n        #plot surface and data space after each epoch    \n        get_surface.plot_ps()\n</code></pre> <p>Run 10 epochs of stochastic gradient descent: bug data space is 1 iteration ahead of parameter space. </p> <pre><code># Train the model with 10 iterations\n\ntrain_model_SGD(10)\n</code></pre> <p>Compare the loss of both batch gradient descent as SGD.</p> <pre><code>LOSS_BGD_ = [loss.item() for loss in LOSS_BGD]\n</code></pre> <pre><code># Plot out the LOSS_BGD and LOSS_SGD\n\nplt.plot(LOSS_BGD_,label = \"Batch Gradient Descent\")\nplt.plot(LOSS_SGD,label = \"Stochastic Gradient Descent\")\nplt.xlabel('epoch')\nplt.ylabel('Cost/ total loss')\nplt.legend()\nplt.show()\n</code></pre> <p>Import the module for building a dataset class: </p> <pre><code># Import the library for DataLoader\n\nfrom torch.utils.data import Dataset, DataLoader\n</code></pre> <p>Create a dataset class:</p> <pre><code># Dataset Class\n\nclass Data(Dataset):\n\n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n        self.y = 1 * self.x - 1\n        self.len = self.x.shape[0]\n\n    # Getter\n    def __getitem__(self,index):    \n        return self.x[index], self.y[index]\n\n    # Return the length\n    def __len__(self):\n        return self.len\n</code></pre> <p>Create a dataset object and check the length of the dataset.</p> <pre><code># Create the dataset and check the length\n\ndataset = Data()\nprint(\"The length of dataset: \", len(dataset))\n</code></pre> <pre>\n<code>The length of dataset:  60\n</code>\n</pre> <p>Obtain the first training point:  </p> <pre><code># Print the first point\n\nx, y = dataset[0]\nprint(\"(\", x, \", \", y, \")\")\n</code></pre> <pre>\n<code>( tensor([-3.]) ,  tensor([-4.]) )\n</code>\n</pre> <p>Similarly, obtain the first three training points:  </p> <pre><code># Print the first 3 point\n\nx, y = dataset[0:3]\nprint(\"The first 3 x: \", x)\nprint(\"The first 3 y: \", y)\n</code></pre> <pre>\n<code>The first 3 x:  tensor([[-3.0000],\n        [-2.9000],\n        [-2.8000]])\nThe first 3 y:  tensor([[-4.0000],\n        [-3.9000],\n        [-3.8000]])\n</code>\n</pre> <p>Create a <code>plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> <pre><code># Create plot_error_surfaces for viewing the data\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30, go = False)\n</code></pre> <p>Create a <code>DataLoader</code> object by using the constructor: </p> <pre><code># Create DataLoader\n\ntrainloader = DataLoader(dataset = dataset, batch_size = 1)\n</code></pre> <p>Define <code>train_model_DataLoader</code> function for training the model.</p> <pre><code># The function for training the model\n\nw = torch.tensor(-15.0,requires_grad=True)\nb = torch.tensor(-10.0,requires_grad=True)\nLOSS_Loader = []\n\ndef train_model_DataLoader(epochs):\n\n    # Loop\n    for epoch in range(epochs):\n\n        # SGD is an approximation of out true total loss/cost, in this line of code we calculate our true loss/cost and store it\n        Yhat = forward(X)\n\n        # store the loss \n        LOSS_Loader.append(criterion(Yhat, Y).tolist())\n\n        for x, y in trainloader:\n\n            # make a prediction\n            yhat = forward(x)\n\n            # calculate the loss\n            loss = criterion(yhat, y)\n\n            # Section for plotting\n            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n\n            # Backward pass: compute gradient of the loss with respect to all the learnable parameters\n            loss.backward()\n\n            # Updata parameters slope\n            w.data = w.data - lr * w.grad.data\n            b.data = b.data - lr* b.grad.data\n\n            # Clear gradients \n            w.grad.data.zero_()\n            b.grad.data.zero_()\n\n        #plot surface and data space after each epoch    \n        get_surface.plot_ps()\n</code></pre> <p>Run 10 epochs of stochastic gradient descent: bug data space is 1 iteration ahead of parameter space. </p> <pre><code># Run 10 iterations\n\ntrain_model_DataLoader(10)\n</code></pre> <p>Compare the loss of both batch gradient decent as SGD. Note that SGD converges to a minimum faster, that is, it decreases faster. </p> <pre><code># Plot the LOSS_BGD and LOSS_Loader\n\nplt.plot(LOSS_BGD_,label=\"Batch Gradient Descent\")\nplt.plot(LOSS_Loader,label=\"Stochastic Gradient Descent with DataLoader\")\nplt.xlabel('epoch')\nplt.ylabel('Cost/ total loss')\nplt.legend()\nplt.show()\n</code></pre> <p>For practice, try to use SGD with DataLoader to train model with 10 iterations. Store the total loss in <code>LOSS</code>. We are going to use it in the next question.</p> <pre><code># Practice: Use SGD with trainloader to train model and store the total loss in LOSS\n\nLOSS = []\nw = torch.tensor(-12.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\n</code></pre> <p>Double-click here for the solution.</p> <p>Plot the total loss</p> <pre><code># Practice: Plot the total loss using LOSS\n\n# Type your code here\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#linear-regression-1d-training-two-parameter-stochastic-gradient-descent-sgd","title":"Linear regression 1D: Training Two Parameter Stochastic Gradient Descent (SGD)","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#objective","title":"Objective","text":"<p><ul><li> How to use SGD(Stochastic Gradient Descent) to train the model.</li></ul></p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#table-of-contents","title":"Table of Contents","text":"<p>In this Lab, you will practice training a model by using Stochastic Gradient descent.</p> <ul> <li>Make Some Data</li> <li>Create the Model and Cost Function (Total Loss)</li> <li>Train the Model:Batch Gradient Descent</li> <li>Train the Model:Stochastic gradient descent</li> <li>Train the Model:Stochastic gradient descent with Data Loader</li> </ul> <p>Estimated Time Needed: 30 min</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#Model_Cost","title":"Create the Model and Cost Function (Total Loss)","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#BGD","title":"Train the Model: Batch Gradient Descent","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#SGD","title":"Train the Model: Stochastic Gradient Descent","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#SGD_Loader","title":"SGD with Dataset DataLoader","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#about-the-author","title":"About the Author:","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning. </p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value. </p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org. </p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/","title":"Mini-Batch Gradient Descent","text":"<p>author: Juma Shafara date: \"2024-08-08\" title: Training Two Parameter Mini-Batch Gradient Decent keywords: Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this Lab, you will practice training a model by using Mini-Batch Gradient Descent.</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># Import the libraries we need for this lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\nfrom dataidea_science.plots import plot_error_surfaces\n</code></pre> <p>The class <code>plot_error_surfaces</code> is just to help you visualize the data space and the parameter space during training and has nothing to do with PyTorch.</p> <p>Import PyTorch and set random seed:</p> <pre><code># Import PyTorch library\n\nimport torch\ntorch.manual_seed(1)\n</code></pre> <pre>\n<code>&lt;torch._C.Generator at 0x7dc5dfe66290&gt;</code>\n</pre> <p>Generate values from -3 to 3 that create a line with a slope of 1 and a bias of -1. This is the line that you need to estimate. Add some noise to the data:</p> <pre><code># Generate the data with noise and the line\n\nX = torch.arange(-3, 3, 0.1).view(-1, 1)\nf = 1 * X - 1\nY = f + 0.1 * torch.randn(X.size())\n</code></pre> <p>Plot the results:</p> <pre><code># Plot the line and the data\n\nplt.plot(X.numpy(), Y.numpy(), 'o', label = 'y', c='g')\nplt.plot(X.numpy(), f.numpy(), label = 'f', c='b')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n</code></pre> <p>Define the <code>forward</code> function:</p> <pre><code># Define the prediction function\n\ndef forward(x):\n    return w * x + b\n</code></pre> <p>Define the cost or criterion function:</p> <pre><code># Define the cost function\n\ndef criterion(yhat, y):\n    return torch.mean((yhat - y) ** 2)\n</code></pre> <p>Create a <code> plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> <pre><code># Create a plot_error_surfaces object.\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30)\n</code></pre> <pre>\n<code>&lt;Figure size 640x480 with 0 Axes&gt;</code>\n</pre> <p>Define <code>train_model_BGD</code> function.</p> <pre><code># Define the function for training model\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\nlr = 0.1\nLOSS_BGD = []\n\ndef train_model_BGD(epochs):\n    for epoch in range(epochs):\n        Yhat = forward(X)\n        loss = criterion(Yhat, Y)\n        LOSS_BGD.append(loss)\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n        get_surface.plot_ps()\n        loss.backward()\n        w.data = w.data - lr * w.grad.data\n        b.data = b.data - lr * b.grad.data\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n</code></pre> <p>Run 10 epochs of batch gradient descent: bug data space is 1 iteration ahead of parameter space.</p> <pre><code># Run train_model_BGD with 10 iterations\n\ntrain_model_BGD(10)\n</code></pre> <p>Create a <code>plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> <pre><code># Create a plot_error_surfaces object.\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30, go = False)\n</code></pre> <p>Import <code>Dataset</code> and <code>DataLoader</code> libraries</p> <pre><code># Import libraries\n\nfrom torch.utils.data import Dataset, DataLoader\n</code></pre> <p>Create <code>Data</code> class</p> <pre><code># Create class Data\n\nclass Data(Dataset):\n\n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n        self.y = 1 * X - 1\n        self.len = self.x.shape[0]\n\n    # Getter\n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]\n\n    # Get length\n    def __len__(self):\n        return self.len\n</code></pre> <p>Create a dataset object and a dataloader object:</p> <pre><code># Create Data object and DataLoader object\n\ndataset = Data()\ntrainloader = DataLoader(dataset = dataset, batch_size = 1)\n</code></pre> <p>Define <code>train_model_SGD</code> function for training the model.</p> <pre><code># Define train_model_SGD function\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\nLOSS_SGD = []\nlr = 0.1\ndef train_model_SGD(epochs):\n    for epoch in range(epochs):\n        Yhat = forward(X)\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), criterion(Yhat, Y).tolist())\n        get_surface.plot_ps()\n        LOSS_SGD.append(criterion(forward(X), Y).tolist())\n        for x, y in trainloader:\n            yhat = forward(x)\n            loss = criterion(yhat, y)\n            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n            loss.backward()\n            w.data = w.data - lr * w.grad.data\n            b.data = b.data - lr * b.grad.data\n            w.grad.data.zero_()\n            b.grad.data.zero_()\n        get_surface.plot_ps()\n</code></pre> <p>Run 10 epochs of stochastic gradient descent: bug data space is 1 iteration ahead of parameter space.</p> <pre><code># Run train_model_SGD(iter) with 10 iterations\n\ntrain_model_SGD(10)\n</code></pre> <p>Create a <code> plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> <pre><code># Create a plot_error_surfaces object.\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30, go = False)\nget_surface\n</code></pre> <pre>\n<code>&lt;__main__.plot_error_surfaces at 0x77fc19f16660&gt;</code>\n</pre> <p>Create <code>Data</code> object and create a <code>Dataloader</code> object where the batch size equals 5:</p> <pre><code># Create DataLoader object and Data object\n\ndataset = Data()\ntrainloader = DataLoader(dataset = dataset, batch_size = 5)\n</code></pre> <p>Define <code>train_model_Mini5</code> function to train the model.</p> <pre><code># Define train_model_Mini5 function\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\nLOSS_MINI5 = []\nlr = 0.1\n\ndef train_model_Mini5(epochs):\n    for epoch in range(epochs):\n        Yhat = forward(X)\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), criterion(Yhat, Y).tolist())\n        get_surface.plot_ps()\n        LOSS_MINI5.append(criterion(forward(X), Y).tolist())\n        for x, y in trainloader:\n            yhat = forward(x)\n            loss = criterion(yhat, y)\n            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n            loss.backward()\n            w.data = w.data - lr * w.grad.data\n            b.data = b.data - lr * b.grad.data\n            w.grad.data.zero_()\n            b.grad.data.zero_()\n</code></pre> <p>Run 10 epochs of mini-batch gradient descent: bug data space is 1 iteration ahead of parameter space.</p> <pre><code># Run train_model_Mini5 with 10 iterations.\n\ntrain_model_Mini5(10)\n</code></pre> <p>Create a <code> plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> <pre><code># Create a plot_error_surfaces object.\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30, go = False)\n</code></pre> <p>Create <code>Data</code> object and create a <code>Dataloader</code> object batch size equals 10</p> <pre><code># Create DataLoader object\n\ndataset = Data()\ntrainloader = DataLoader(dataset = dataset, batch_size = 10)\n</code></pre> <p>Define <code>train_model_Mini10</code> function for training the model.</p> <pre><code># Define train_model_Mini5 function\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\nLOSS_MINI10 = []\nlr = 0.1\n\ndef train_model_Mini10(epochs):\n    for epoch in range(epochs):\n        Yhat = forward(X)\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), criterion(Yhat, Y).tolist())\n        get_surface.plot_ps()\n        LOSS_MINI10.append(criterion(forward(X),Y).tolist())\n        for x, y in trainloader:\n            yhat = forward(x)\n            loss = criterion(yhat, y)\n            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n            loss.backward()\n            w.data = w.data - lr * w.grad.data\n            b.data = b.data - lr * b.grad.data\n            w.grad.data.zero_()\n            b.grad.data.zero_()\n</code></pre> <p>Run 10 epochs of mini-batch gradient descent: bug data space is 1 iteration ahead of parameter space.</p> <pre><code># Run train_model_Mini5 with 10 iterations.\n\ntrain_model_Mini10(10)\n</code></pre> <p>Plot the loss for each epoch:</p> <pre><code>LOSS_BGD_ = [loss.item() for loss in LOSS_BGD]\n</code></pre> <pre><code># Plot out the LOSS for each method\n\nplt.plot(LOSS_BGD_,label = \"Batch Gradient Descent\")\nplt.plot(LOSS_SGD,label = \"Stochastic Gradient Descent\")\nplt.plot(LOSS_MINI5,label = \"Mini-Batch Gradient Descent, Batch size: 5\")\nplt.plot(LOSS_MINI10,label = \"Mini-Batch Gradient Descent, Batch size: 10\")\nplt.legend()\n</code></pre> <pre>\n<code>&lt;matplotlib.legend.Legend at 0x77fc3583c5f0&gt;</code>\n</pre> <p>Perform mini batch gradient descent with a batch size of 20. Store the total loss for each epoch in the list LOSS20.</p> <pre><code># Practice: Perform mini batch gradient descent with a batch size of 20.\n\ndataset = Data()\ntrainloader = DataLoader(dataset = dataset, batch_size = 20)\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\n\nLOSS_MINI20 = []\nlr = 0.1\n\ndef my_train_model(epochs):\n    for epoc in range(epochs):\n        Yhat = forward(X)\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), criterion(Yhat, Y).tolist())\n        get_surface.plot_ps()\n        LOSS_MINI20.append(criterion(forward(X), Y).tolist())\n        for x, y in trainloader:\n            yhat = forward(x)\n            loss = criterion(yhat, y)\n            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n            loss.backward()\n            w.data = w.data - lr * w.grad.data\n            b.data = b.data - lr * b.grad.data\n            w.grad.data.zero_()\n            b.grad.data.zero_()\n</code></pre> <p>Double-click here for the solution.</p> <p>Plot a graph that shows the LOSS results for all the methods.</p> <pre><code># Practice: Plot a graph to show all the LOSS functions\n\n# Type your code here\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#linear-regression-1d-training-two-parameter-mini-batch-gradient-decent","title":"Linear Regression 1D: Training Two Parameter Mini-Batch Gradient Decent","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#objective","title":"Objective","text":"<p><ul><li> How to use Mini-Batch Gradient Descent to train model.</li></ul></p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#table-of-contents","title":"Table of Contents","text":"<p>In this Lab, you will practice training a model by using Mini-Batch Gradient Descent.</p> <ul> <li> Make Some Data</li> <li> Create the Model and Cost Function (Total Loss)</li> <li> Train the Model: Batch Gradient Descent</li> <li> Train the Model: Stochastic Gradient Descent with Dataset DataLoader</li> <li> Train the Model: Mini Batch Gradient Decent: Batch Size Equals 5</li> <li> Train the Model: Mini Batch Gradient Decent: Batch Size Equals 10</li> </ul> <p>Estimated Time Needed: 30 min</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#preparation","title":"Preparation","text":"<p>We'll need the following libraries:</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#Model_Cost","title":"Create the Model and Cost Function (Total Loss)","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#train-the-model-batch-gradient-descent-bgd","title":"Train the Model: Batch Gradient Descent (BGD)","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#SGD","title":"Stochastic Gradient Descent (SGD) with Dataset DataLoader","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#Mini5","title":"Mini Batch Gradient Descent: Batch Size Equals 5","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#Mini10","title":"Mini Batch Gradient Descent: Batch Size Equals 10","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#about-the-author","title":"About the Author:","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/","title":"PyTorch Way","text":"<p>author: Juma Shafara date: \"2024-08-08\" date-modified: \"2024-08-21\" title: PyTorch build-in functions keywords: [     Create the Model and Total Loss Function,      Train the Model via Batch Gradient Descent     ] description: In this lab, you will create a model the PyTroch way, this will help you as models get more complicated</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>We'll need the following libraries:  </p> <pre><code># These are the libraries we are going to use in the lab.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\nfrom dataidea_science.plots import plot_error_surfaces\n</code></pre> <p>The class <code>plot_error_surfaces</code> is just to help you visualize the data space and the parameter space during training and has nothing to do with PyTorch. </p> <p>Import libraries and set random seed.</p> <pre><code># Import libraries and set random seed\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\ntorch.manual_seed(1)\n</code></pre> <pre>\n<code>&lt;torch._C.Generator at 0x7f5c8438a2b0&gt;</code>\n</pre> <p>Generate values from -3 to 3 that create a line with a slope of 1 and a bias of -1. This is the line that you need to estimate. Add some noise to the data:</p> <pre><code># Create Data Class\n\nclass Data(Dataset):\n\n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n        self.f = 1 * self.x - 1\n        self.y = self.f + 0.1 * torch.randn(self.x.size())\n        self.len = self.x.shape[0]\n\n    # Getter\n    def __getitem__(self,index):    \n        return self.x[index],self.y[index]\n\n    # Get Length\n    def __len__(self):\n        return self.len\n</code></pre> <p>Create a dataset object: </p> <pre><code># Create dataset object\n\ndataset = Data()\n</code></pre> <p>Plot out the data and the line.</p> <pre><code># Plot the data\n\nplt.plot(dataset.x.numpy(), dataset.y.numpy(), 'rx', label = 'y')\nplt.plot(dataset.x.numpy(), dataset.f.numpy(), label = 'f')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\n</code></pre> <pre>\n<code>&lt;matplotlib.legend.Legend at 0x7f5c615e7d40&gt;</code>\n</pre> <p>Create a linear regression class </p> <pre><code># Create a linear regression model class\n\nfrom torch import nn, optim\n\nclass linear_regression(nn.Module):\n\n    # Constructor\n    def __init__(self, input_size, output_size):\n        super(linear_regression, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n\n    # Prediction\n    def forward(self, x):\n        yhat = self.linear(x)\n        return yhat\n</code></pre> <p>Note about the <code>super()</code> method: - When <code>LinearRegression</code> is instantiated, its <code>__init__</code> method is called. - Inside <code>LinearRegression.__init__</code>, <code>super(LinearRegression, self).__init__()</code> calls the <code>__init__</code> method of the parent class (<code>nn.Module</code>). - After the parent class is initialized, the rest of the code in the <code>LinearRegression.__init__</code> method runs too.</p> <p>This mechanism ensures that the <code>nn.Module</code> class is properly initialized before any additional initialization specific to <code>LinearRegression</code> occurs.</p> <pre><code># Build in cost function\n\ncriterion = nn.MSELoss()\n</code></pre> <p>Create a linear regression object and optimizer object, the optimizer object will use the linear regression object.</p> <pre><code># Create optimizer\n\nmodel = linear_regression(1,1)\noptimizer = optim.SGD(model.parameters(), lr = 0.01)\n</code></pre> <pre><code>list(model.parameters())\n</code></pre> <pre>\n<code>[Parameter containing:\n tensor([[0.3636]], requires_grad=True),\n Parameter containing:\n tensor([0.4957], requires_grad=True)]</code>\n</pre> <p>Remember to construct an optimizer you have to give it an iterable containing the parameters i.e. provide <code> model.parameters()</code> as an input to the object constructor </p> <p></p> <p>Similar to the model, the optimizer has a state dictionary:</p> <pre><code>optimizer.state_dict()\n</code></pre> <pre>\n<code>{'state': {},\n 'param_groups': [{'lr': 0.01,\n   'momentum': 0,\n   'dampening': 0,\n   'weight_decay': 0,\n   'nesterov': False,\n   'maximize': False,\n   'foreach': None,\n   'differentiable': False,\n   'fused': None,\n   'params': [0, 1]}]}</code>\n</pre> <p>Many of the keys correspond to more advanced optimizers.</p> <p>Create a <code>Dataloader</code> object: </p> <pre><code># Create Dataloader object\n\ntrainloader = DataLoader(dataset = dataset, batch_size = 1)\n</code></pre> <p>PyTorch randomly initialises your model parameters. If we use those parameters, the result will not be very insightful as convergence will be extremely fast. So we will initialise the parameters such that they will take longer to converge, i.e. look cool  </p> <pre><code># Customize the weight and bias\n\nmodel.state_dict()['linear.weight'][0] = -15\nmodel.state_dict()['linear.bias'][0] = -10\n</code></pre> <p>Create a plotting object, not part of PyTroch, just used to help visualize </p> <pre><code># Create plot surface object\n\nget_surface = plot_error_surfaces(15, 13, dataset.x, dataset.y, 30, go = False)\n</code></pre> <p>Run 10 epochs of stochastic gradient descent: bug data space is 1 iteration ahead of parameter space. </p> <pre><code># Train Model\n\ndef train_model_BGD(iter):\n    for epoch in range(iter):\n        for x,y in trainloader:\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            get_surface.set_para_loss(model, loss.tolist())          \n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n        get_surface.plot_ps()\n\n\ntrain_model_BGD(10)\n</code></pre> <pre><code>model.state_dict()\n</code></pre> <pre>\n<code>OrderedDict([('linear.weight', tensor([[0.9932]])),\n             ('linear.bias', tensor([-1.0174]))])</code>\n</pre> <p>Let's use the following diagram to help clarify the process. The model takes <code>x</code> to produce an estimate <code>yhat</code>, it will then be compared to the actual <code>y</code>  with the loss function.</p> <p></p> <p>When we call <code>backward()</code> on the loss function, it will handle the differentiation. Calling the method step on the optimizer object it will update the parameters as they were inputs when we constructed the optimizer object. The connection is shown in the following figure :</p> <p></p> <p>Try to train the model via BGD with <code>lr = 0.1</code>. Use <code>optimizer</code> and the following given variables.</p> <pre><code># Practice: Train the model via BGD using optimizer\n\nmodel = linear_regression(1,1)\nmodel.state_dict()['linear.weight'][0] = -15\nmodel.state_dict()['linear.bias'][0] = -10\nget_surface = plot_error_surfaces(15, 13, dataset.x, dataset.y, 30, go = False)\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/#linear-regression-1d-training-two-parameter-mini-batch-gradient-descent","title":"Linear Regression 1D: Training Two Parameter Mini-Batch Gradient Descent","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/#objective","title":"Objective","text":"<p><ul><li>     How to use PyTorch build-in functions to create a model.</li></ul></p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will create a model the PyTroch way, this will help you as models get more complicated</p> <ul> <li> Make Some Data </li> <li> Create the Model and Cost Function the PyTorch way </li> <li> Train the Model: Batch Gradient Descent</li> </ul> <p>Estimated Time Needed: 30 min</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/#Model_Cost","title":"Create the Model and Total Loss Function (Cost)","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/#the-loss-and-optimizer-functions","title":"The Loss and Optimizer Functions","text":"<p>We will use PyTorch build-in functions to create a criterion function; this calculates the total loss or cost </p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/#BGD","title":"Train the Model via Batch Gradient Descent","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/#about-the-author","title":"About the Author:","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/","title":"Training & Validation","text":"<p>author: Juma Shafara date: \"2024-08-08\" date-modified: \"2024-08-19\" title: Training and Validation Data keywords: [T     Training and Validation Data,     Create a Linear Regression Object,      Data Loader,      Criterion Function,     Different learning rates,     Data Structures,     different Hyperparameters,     Train different models for different Hyperparameters ] description: How to use learning rate hyperparameter to improve your model result</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>We'll need the following libraries and set the random seed.</p> <pre><code># Import libraries we need for this lab, and set the random seed\n\nfrom torch import nn\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch import nn,optim\n</code></pre> <p>First, we'll create some artificial data in a dataset class. The class will include the option to produce training data or validation data. The training data will include outliers.</p> <pre><code># Create Data class\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass Data(Dataset):\n\n    # Constructor\n    def __init__(self, train = True):\n            self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n            self.f = -3 * self.x + 1\n            self.y = self.f + 0.1 * torch.randn(self.x.size())\n            self.len = self.x.shape[0]\n\n            #outliers \n            if train == True:\n                self.y[0] = 0\n                self.y[50:55] = 20\n            else:\n                pass\n\n    # Getter\n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]\n\n    # Get Length\n    def __len__(self):\n        return self.len\n</code></pre> <p>Create two objects: one that contains training data and a second that contains validation data. Assume that the training data has the outliers. </p> <pre><code># Create training dataset and validation dataset\n\ntrain_data = Data()\nval_data = Data(train = False)\n</code></pre> <p>Overlay the training points in red over the function that generated the data. Notice the outliers at x=-3 and around x=2:</p> <pre><code># Plot out training points\n\nplt.plot(train_data.x.numpy(), train_data.y.numpy(), 'xr',label=\"training data \")\nplt.plot(train_data.x.numpy(), train_data.f.numpy(),label=\"true function  \")\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n</code></pre> <pre><code># Create Linear Regression Class\n\nfrom torch import nn\n\nclass linear_regression(nn.Module):\n\n    # Constructor\n    def __init__(self, input_size, output_size):\n        super(linear_regression, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n\n    # Prediction function\n    def forward(self, x):\n        yhat = self.linear(x)\n        return yhat\n</code></pre> <p>Create the criterion function and a <code>DataLoader</code> object: </p> <pre><code># Create MSELoss function and DataLoader\n\ncriterion = nn.MSELoss()\ntrainloader = DataLoader(dataset = train_data, batch_size = 1)\n</code></pre> <p>Create a list with different learning rates and a tensor (can be a list) for the training and validating cost/total loss. Include the list MODELS, which stores the training model for every value of the learning rate. </p> <pre><code># Create Learning Rate list, the error lists and the MODELS list\n\nlearning_rates=[0.0001, 0.001, 0.01, 0.1]\n\ntrain_error=torch.zeros(len(learning_rates))\nvalidation_error=torch.zeros(len(learning_rates))\n\nMODELS=[]\n</code></pre> <p>Try different values of learning rates, perform stochastic gradient descent, and save the results on the training data and validation data. Finally, save each model in a list.</p> <pre><code># Define the train model function and train the model\n\ndef train_model_with_lr (iter, lr_list):\n\n    # iterate through different learning rates \n    for i, lr in enumerate(lr_list):\n        model = linear_regression(1, 1)\n        optimizer = optim.SGD(model.parameters(), lr = lr)\n        for epoch in range(iter):\n            for x, y in trainloader:\n                yhat = model(x)\n                loss = criterion(yhat, y)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n        # train data\n        Yhat = model(train_data.x)\n        train_loss = criterion(Yhat, train_data.y)\n        train_error[i] = train_loss.item()\n\n        # validation data\n        Yhat = model(val_data.x)\n        val_loss = criterion(Yhat, val_data.y)\n        validation_error[i] = val_loss.item()\n        MODELS.append(model)\n\ntrain_model_with_lr(10, learning_rates)\n</code></pre> <p>Plot the training loss and validation loss for each learning rate:  </p> <pre><code># Plot the training loss and validation loss\n\nplt.semilogx(np.array(learning_rates), train_error.numpy(), label = 'training loss/total Loss')\nplt.semilogx(np.array(learning_rates), validation_error.numpy(), label = 'validation cost/total Loss')\nplt.ylabel('Cost\\ Total Loss')\nplt.xlabel('learning rate')\nplt.legend()\nplt.show()\n</code></pre> <pre>\n<code>&lt;&gt;:5: SyntaxWarning: invalid escape sequence '\\ '\n&lt;&gt;:5: SyntaxWarning: invalid escape sequence '\\ '\n/tmp/ipykernel_80257/980418342.py:5: SyntaxWarning: invalid escape sequence '\\ '\n  plt.ylabel('Cost\\ Total Loss')\n</code>\n</pre> <p>Produce a prediction by using the validation data for each model:  </p> <pre><code># Plot the predictions\n\ni = 0\nfor model, learning_rate in zip(MODELS, learning_rates):\n    yhat = model(val_data.x)\n    plt.plot(val_data.x.numpy(), yhat.detach().numpy(), label = 'lr:' + str(learning_rate))\n    print('i', yhat.detach().numpy()[0:3])\nplt.plot(val_data.x.numpy(), val_data.f.numpy(), 'or', label = 'validation data')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n</code></pre> <pre>\n<code>i [[0.05840436]\n [0.06799743]\n [0.0775905 ]]\ni [[5.4587383]\n [5.343816 ]\n [5.2288933]]\ni [[6.5484743]\n [6.4158573]\n [6.28324  ]]\ni [[14.55032  ]\n [14.1119585]\n [13.673596 ]]\n</code>\n</pre> <p>The object <code>good_model</code> is the best performing model. Use the train loader to get the data samples x and y. Produce an estimate for <code>yhat</code> and print it out for every sample in a for a loop. Compare it to the actual prediction <code>y</code>.</p> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#linear-regression-training-and-validation-data","title":"Linear regression: Training and Validation Data","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#objective","title":"Objective","text":"<p><ul><li> How to use learning rate hyperparameter to improve your model result.</li></ul></p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will learn to select the best learning rate by using validation data.</p> <ul> <li> Make Some Data</li> <li> Create a Linear Regression Object, Data Loader and Criterion Function</li> <li> Different learning rates and Data Structures to Store results for Different Hyperparameters</li> <li> Train different modules for different Hyperparameters</li> <li> View Results</li> </ul> <p>Estimated Time Needed: 30 min</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#LR_Loader_Cost","title":"Create a Linear Regression Object,  Data Loader, and Criterion Function","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#LR_Hyper","title":"Different learning rates and Data Structures to Store results for different Hyperparameters","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#Model","title":"Train different models  for different Hyperparameters","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#Result","title":"View the Results","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#about-the-author","title":"About the Author:","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/linear_regression_in_pytorch_exercise/","title":"Exercise","text":"<p>title: Linear Regression (PyTorch) Exercise author: Juma Shafara date: \"2024-08-29\" keywords: [data science, data analysis, programming, dataidea] description: Programming for Data Science is a subject we\u2019ve designed to explore the various programming components of data science.</p> <p></p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/linear_regression_in_pytorch_exercise/#week-3-linear-regression-in-pytorch-4-questions","title":"Week 3: Linear Regression in PyTorch (4 Questions)","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/linear_regression_in_pytorch_exercise/#1-stochastic-gradient-descent","title":"1. Stochastic Gradient Descent","text":"<ul> <li>Exercise: Implement a linear regression model using stochastic gradient descent (SGD) on a synthetic dataset. Plot the loss curve to show convergence over iterations.</li> </ul>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/linear_regression_in_pytorch_exercise/#2-mini-batch-gradient-descent","title":"2. Mini-Batch Gradient Descent","text":"<ul> <li>Exercise: Modify your SGD implementation to use mini-batch gradient descent. Train the model on a dataset with mini-batches and compare the performance with the full-batch SGD approach.</li> </ul>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/linear_regression_in_pytorch_exercise/#3-pytorch-built-in-functions","title":"3. PyTorch Built-in Functions","text":"<ul> <li>Exercise: Use PyTorch's built-in functions (<code>torch.nn.Linear</code>, <code>torch.optim.SGD</code>) to build and train a linear regression model. Compare the results with your previous implementations.</li> </ul> <p> Before the last question, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/linear_regression_in_pytorch_exercise/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/linear_regression_in_pytorch_exercise/#4-training-and-validation-sets","title":"4. Training and Validation Sets","text":"<ul> <li>Exercise: Split your dataset into training and validation sets. Train a linear regression model on the training set and evaluate its performance on the validation set. Plot the training and validation loss over epochs.</li> </ul>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/linear_regression_in_pytorch_exercise/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.1.multiple_linear_regression_prediction_v2/","title":"Prediction","text":"<p>title: Multiple Linear Regression author: Juma Shafara date: \"2024-08-08\" date-modified: \"2024-08-21\" keywords: [     Multiple Linear Regression,     How to make the prediction for multiple inputs,     How to use linear class to build more complex models,     How to build a custom module,     ] description: In this lab, you will review how to make a prediction in several different ways by using PyTorch.</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>Import the libraries and set the random seed.</p> <pre><code># Import the libraries and set the random seed\n\nfrom torch import nn\nimport torch\ntorch.manual_seed(1)\n</code></pre> <pre>\n<code>&lt;torch._C.Generator at 0x70efcb47eef0&gt;</code>\n</pre> <p>Set weight and bias.</p> <pre><code># Set the weight and bias\n\nw = torch.tensor([[2.0], [3.0]], requires_grad=True)\nb = torch.tensor([[1.0]], requires_grad=True)\n</code></pre> <p>Define the parameters. <code>torch.mm</code> uses matrix multiplication instead of scaler multiplication.</p> <pre><code># Define Prediction Function\n\ndef forward(x):\n    yhat = torch.mm(x, w) + b\n    return yhat\n</code></pre> <p>The function <code>forward</code> implements the following equation:</p> <p></p> <p>If we input a 1x2 tensor, because we have a 2x1 tensor as <code>w</code>, we will get a 1x1 tensor: </p> <pre><code># Calculate yhat\n\nx = torch.tensor([[1.0, 2.0]])\nyhat = forward(x)\nprint(\"The result: \", yhat)\n</code></pre> <pre>\n<code>The result:  tensor([[9.]], grad_fn=&lt;AddBackward0&gt;)\n</code>\n</pre> <p></p> <p>Each row of the following tensor represents a sample:</p> <pre><code># Sample tensor X\n\nX = torch.tensor([[1.0, 1.0], [1.0, 2.0], [1.0, 3.0]])\n</code></pre> <pre><code># Make the prediction of X \n\nyhat = forward(X)\nprint(\"The result: \", yhat)\n</code></pre> <pre>\n<code>The result:  tensor([[ 6.],\n        [ 9.],\n        [12.]], grad_fn=&lt;AddBackward0&gt;)\n</code>\n</pre> <p>We can use the linear class to make a prediction. You'll also use the linear class to build more complex models.</p> <p>Let us create a model.</p> <pre><code># Make a linear regression model using build-in function\n\nmodel = nn.Linear(2, 1)\n</code></pre> <p>Make a prediction with the first sample:</p> <pre><code># Make a prediction of x\n\nyhat = model(x)\nprint(\"The result: \", yhat)\n</code></pre> <pre>\n<code>The result:  tensor([[-0.3969]], grad_fn=&lt;AddmmBackward0&gt;)\n</code>\n</pre> <p>Predict with multiple samples <code>X</code>: </p> <pre><code># Make a prediction of X\n\nyhat = model(X)\nprint(\"The result: \", yhat)\n</code></pre> <pre>\n<code>The result:  tensor([[-0.0848],\n        [-0.3969],\n        [-0.7090]], grad_fn=&lt;AddmmBackward0&gt;)\n</code>\n</pre> <p>The function performs matrix multiplication as shown in this image:</p> <p></p> <p>Now, you'll build a custom module. You can make more complex models by using this method later. </p> <pre><code># Create LinearRegression Class\n\nclass LinearRegression(nn.Module):\n\n    # Constructor\n    def __init__(self, input_size, output_size):\n        super(LinearRegression, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n\n    # Prediction function\n    def forward(self, x):\n        yhat = self.linear(x)\n        return yhat\n</code></pre> <p>Build a linear regression object. The input feature size is two. </p> <pre><code>model = LinearRegression(2, 1)\n</code></pre> <p>This will input the following equation:</p> <p></p> <p>You can see the randomly initialized parameters by using the <code>parameters()</code> method:</p> <pre><code># Print model parameters\n\nprint(\"The parameters: \", list(model.parameters()))\n</code></pre> <pre>\n<code>The parameters:  [Parameter containing:\ntensor([[ 0.3319, -0.6657]], requires_grad=True), Parameter containing:\ntensor([0.4241], requires_grad=True)]\n</code>\n</pre> <p>You can also see the parameters by using the <code>state_dict()</code> method:</p> <pre><code># Print model parameters\n\nprint(\"The parameters: \", model.state_dict())\n</code></pre> <pre>\n<code>The parameters:  OrderedDict({'linear.weight': tensor([[ 0.3319, -0.6657]]), 'linear.bias': tensor([0.4241])})\n</code>\n</pre> <p>Now we input a 1x2 tensor, and we will get a 1x1 tensor.</p> <pre><code># Make a prediction of x\n\nyhat = model(x)\nprint(\"The result: \", yhat)\n</code></pre> <pre>\n<code>The result:  tensor([[-0.5754]], grad_fn=&lt;AddmmBackward0&gt;)\n</code>\n</pre> <p>The shape of the output is shown in the following image: </p> <p></p> <p>Make a prediction for multiple samples:</p> <pre><code># Make a prediction of X\n\nyhat = model(X)\nprint(\"The result: \", yhat)\n</code></pre> <pre>\n<code>The result:  tensor([[ 0.0903],\n        [-0.5754],\n        [-1.2411]], grad_fn=&lt;AddmmBackward0&gt;)\n</code>\n</pre> <p>The shape is shown in the following image: </p> <p></p> <p>Build a model or object of type <code>linear_regression</code>. Using the <code>linear_regression</code> object will predict the following tensor: </p> <pre><code># Practice: Build a model to predict the follow tensor.\n\nX = torch.tensor([[11.0, 12.0, 13, 14], [11, 12, 13, 14]])\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.1.multiple_linear_regression_prediction_v2/#objective","title":"Objective","text":"<p><ul><li> How to make the prediction for multiple inputs.</li><li> How to use linear class to build more complex models.</li><li> How to build a custom module.</li></ul></p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.1.multiple_linear_regression_prediction_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will review how to make a prediction in several different ways by using PyTorch.</p> <ul> <li> Prediction</li> <li> Class Linear</li> <li> Build Custom Modules</li> </ul> <p>Estimated Time Needed: 15 min</p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.1.multiple_linear_regression_prediction_v2/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.1.multiple_linear_regression_prediction_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.1.multiple_linear_regression_prediction_v2/#Prediction","title":"Prediction","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.1.multiple_linear_regression_prediction_v2/#Linear","title":"Class Linear","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.1.multiple_linear_regression_prediction_v2/#Cust","title":"Build Custom Modules","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.1.multiple_linear_regression_prediction_v2/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.1.multiple_linear_regression_prediction_v2/#about-the-author","title":"About the Author:","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.1.multiple_linear_regression_prediction_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.2.multiple_linear_regression_training_v2/","title":"Training","text":"<p>author: Juma Shafara date: \"2024-08-08\" title: Linear Regression Multiple Outputs keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will review how to make a prediction in several different ways by using PyTorch.</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>We'll need the following libraries:</p> <pre><code># Import the libraries we need for this lab\n\nfrom torch import nn,optim\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom torch.utils.data import Dataset, DataLoader\n</code></pre> <p>Set the random seed:</p> <pre><code># Set the random seed to 1. \n\ntorch.manual_seed(1)\n</code></pre> <pre>\n<code>&lt;torch._C.Generator at 0x7b084404eeb0&gt;</code>\n</pre> <p>Use this function for plotting: </p> <pre><code># The function for plotting 2D\n\ndef Plot_2D_Plane(model, dataset, n=0):\n    w1 = model.state_dict()['linear.weight'].numpy()[0][0]\n    w2 = model.state_dict()['linear.weight'].numpy()[0][1]\n    b = model.state_dict()['linear.bias'].numpy()\n\n    # Data\n    x1 = data_set.x[:, 0].view(-1, 1).numpy()\n    x2 = data_set.x[:, 1].view(-1, 1).numpy()\n    y = data_set.y.numpy()\n\n    # Make plane\n    X, Y = np.meshgrid(np.arange(x1.min(), x1.max(), 0.05), np.arange(x2.min(), x2.max(), 0.05))\n    yhat = w1 * X + w2 * Y + b\n\n    # Plotting\n    fig = plt.figure()\n    ax = fig.gca(projection='3d')\n\n    ax.plot(x1[:, 0], x2[:, 0], y[:, 0],'ro', label='y') # Scatter plot\n\n    ax.plot_surface(X, Y, yhat) # Plane plot\n\n    ax.set_xlabel('x1 ')\n    ax.set_ylabel('x2 ')\n    ax.set_zlabel('y')\n    plt.title('estimated plane iteration:' + str(n))\n    ax.legend()\n\n    plt.show()\n</code></pre> <p>Create a dataset class with two-dimensional features:</p> <pre><code># Create a 2D dataset\n\nclass Data2D(Dataset):\n\n    # Constructor\n    def __init__(self):\n        self.x = torch.zeros(20, 2)\n        self.x[:, 0] = torch.arange(-1, 1, 0.1)\n        self.x[:, 1] = torch.arange(-1, 1, 0.1)\n        self.w = torch.tensor([[1.0], [1.0]])\n        self.b = 1\n        self.f = torch.mm(self.x, self.w) + self.b    \n        self.y = self.f + 0.1 * torch.randn((self.x.shape[0],1))\n        self.len = self.x.shape[0]\n\n    # Getter\n    def __getitem__(self, index):          \n        return self.x[index], self.y[index]\n\n    # Get Length\n    def __len__(self):\n        return self.len\n</code></pre> <p>Create a dataset object:</p> <pre><code># Create the dataset object\n\ndata_set = Data2D()\n</code></pre> <p>Create a customized linear regression module: </p> <pre><code># Create a customized linear\n\nclass LinearRegression(nn.Module):\n\n    # Constructor\n    def __init__(self, input_size, output_size):\n        super(LinearRegression, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n\n    # Prediction\n    def forward(self, x):\n        yhat = self.linear(x)\n        return yhat\n</code></pre> <p>Create a model. Use two features: make the input size 2 and the output size 1: </p> <pre><code># Create the linear regression model and print the parameters\n\nmodel = LinearRegression(2,1)\nprint(\"The parameters: \", list(model.parameters()))\n</code></pre> <pre>\n<code>The parameters:  [Parameter containing:\ntensor([[ 0.6209, -0.1178]], requires_grad=True), Parameter containing:\ntensor([0.3026], requires_grad=True)]\n</code>\n</pre> <p>Create an optimizer  object. Set the learning rate to 0.1. Don't forget to enter the model parameters in the constructor.</p> <p></p> <pre><code># Create the optimizer\n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n</code></pre> <p>Create the criterion function that calculates the total loss or cost:</p> <pre><code># Create the cost function\n\ncriterion = nn.MSELoss()\n</code></pre> <p>Create a data loader object. Set the batch_size equal to 2: </p> <pre><code># Create the data loader\n\ntrain_loader = DataLoader(dataset=data_set, batch_size=2)\n</code></pre> <p>Run 100 epochs of Mini-Batch Gradient Descent and store the total loss or cost for every iteration. Remember that this is an approximation of the true total loss or cost:</p> <pre><code># Train the model\n\nLOSS = []\n# print(\"Before Training: \")\n# Plot_2D_Plane(model, data_set)   \nepochs = 100\n\ndef train_model(epochs):    \n    for epoch in range(epochs):\n        for x,y in train_loader:\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            LOSS.append(loss.item())\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()     \ntrain_model(epochs)\n# print(\"After Training: \")\n# Plot_2D_Plane(model, data_set, epochs)  \n</code></pre> <pre><code># Plot out the Loss and iteration diagram\n\nplt.plot(LOSS)\nplt.xlabel(\"Iterations \")\nplt.ylabel(\"Cost/total loss \")\n</code></pre> <pre>\n<code>Text(0, 0.5, 'Cost/total loss ')</code>\n</pre> <p>Create a new <code>model1</code>. Train the model with a batch size 30 and learning rate 0.1, store the loss or total cost in a list <code>LOSS1</code>, and plot the results.</p> <pre><code># Practice create model1. Train the model with batch size 30 and learning rate 0.1, store the loss in a list &lt;code&gt;LOSS1&lt;/code&gt;. Plot the results.\n\ndata_set = Data2D()\n</code></pre> <p>Double-click here for the solution.</p> <p>Use the following validation data to calculate the total loss or cost for both models:</p> <pre><code>torch.manual_seed(2)\n\nvalidation_data = Data2D()\nY = validation_data.y\nX = validation_data.x\n</code></pre>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.2.multiple_linear_regression_training_v2/#objective","title":"Objective","text":"<p><ul><li> How to create a complicated models using pytorch build in functions.</li></ul></p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.2.multiple_linear_regression_training_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will create a model the PyTroch way. This will help you more complicated models.</p> <ul> <li>Make Some Data</li> <li>Create the Model and Cost Function the PyTorch way</li> <li>Train the Model: Batch Gradient Descent</li> </ul> <p>Estimated Time Needed: 20 min</p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.2.multiple_linear_regression_training_v2/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.2.multiple_linear_regression_training_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.2.multiple_linear_regression_training_v2/# #Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.2.multiple_linear_regression_training_v2/#Model_Cost","title":"Create the Model, Optimizer, and Total Loss Function (Cost)","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.2.multiple_linear_regression_training_v2/#BGD","title":"Train the Model via Mini-Batch Gradient Descent","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.2.multiple_linear_regression_training_v2/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.2.multiple_linear_regression_training_v2/#about-the-author","title":"About the Author:","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.2.multiple_linear_regression_training_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.3.multi-target_linear_regression/","title":"Multi-Target Regression","text":"<p>author: Juma Shafara date: \"2024-08-09\" title: Clear Linear keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, we will  review how to make a prediction for Linear Regression with Multiple Output. </p> <p></p> <p></p> <pre><code>from torch import nn\nimport torch\n</code></pre> <p>Set the random seed:</p> <pre><code>torch.manual_seed(1)\n</code></pre> <pre>\n<code>&lt;torch._C.Generator at 0x7fee040d21b0&gt;</code>\n</pre> <p>Set the random seed:</p> <pre><code>class linear_regression(nn.Module):\n    def __init__(self,input_size,output_size):\n        super(linear_regression,self).__init__()\n        self.linear=nn.Linear(input_size,output_size)\n    def forward(self,x):\n        yhat=self.linear(x)\n        return yhat\n</code></pre> <p>create a linear regression  object, as our input and output will be two we set the parameters accordingly </p> <pre><code>model=linear_regression(1,10)\nmodel(torch.tensor([1.0]))\n</code></pre> <pre>\n<code>tensor([ 0.7926, -0.3920,  0.1714,  0.0797, -1.0143,  0.5097, -0.0608,  0.5047,\n         1.0132,  0.1887], grad_fn=&lt;AddBackward0&gt;)</code>\n</pre> <p>we can use the diagram to represent the model or object </p> <p></p> <p>we can see the parameters </p> <pre><code>list(model.parameters())\n</code></pre> <pre>\n<code>[Parameter containing:\n tensor([[ 0.5153],\n         [-0.4414],\n         [-0.1939],\n         [ 0.4694],\n         [-0.9414],\n         [ 0.5997],\n         [-0.2057],\n         [ 0.5087],\n         [ 0.1390],\n         [-0.1224]], requires_grad=True),\n Parameter containing:\n tensor([ 0.2774,  0.0493,  0.3652, -0.3897, -0.0729, -0.0900,  0.1449, -0.0040,\n          0.8742,  0.3112], requires_grad=True)]</code>\n</pre> <p>we can create a tensor with two rows representing one sample of data</p> <pre><code>x=torch.tensor([[1.0]])\n</code></pre> <p>we can make a prediction </p> <pre><code>yhat=model(x)\nyhat\n</code></pre> <pre>\n<code>tensor([[ 0.7926, -0.3920,  0.1714,  0.0797, -1.0143,  0.5097, -0.0608,  0.5047,\n          1.0132,  0.1887]], grad_fn=&lt;AddmmBackward&gt;)</code>\n</pre> <p>each row in the following tensor represents a different sample </p> <pre><code>X=torch.tensor([[1.0],[1.0],[3.0]])\n</code></pre> <p>we can make a prediction using multiple samples </p> <pre><code>Yhat=model(X)\nYhat\n</code></pre> <pre>\n<code>tensor([[ 0.7926, -0.3920,  0.1714,  0.0797, -1.0143,  0.5097, -0.0608,  0.5047,\n          1.0132,  0.1887],\n        [ 0.7926, -0.3920,  0.1714,  0.0797, -1.0143,  0.5097, -0.0608,  0.5047,\n          1.0132,  0.1887],\n        [ 1.8232, -1.2748, -0.2164,  1.0184, -2.8972,  1.7091, -0.4722,  1.5222,\n          1.2912, -0.0561]], grad_fn=&lt;AddmmBackward&gt;)</code>\n</pre> <p>the following figure represents the operation, where the red and blue  represents the different parameters, and the different shades of green represent  different samples.</p> <p></p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.3.multi-target_linear_regression/#objective","title":"Objective","text":"<p><ul><li> How to make a prediction using multiple samples.</li></ul></p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.3.multi-target_linear_regression/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, we will  review how to make a prediction for Linear Regression with Multiple Output.</p> <ul> <li> Build Custom Modules </li> <p></p> Estimated Time Needed: 15 min </ul>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.3.multi-target_linear_regression/#class-linear","title":"Class Linear","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.3.multi-target_linear_regression/#about-the-author","title":"About the Author:","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.3.multi-target_linear_regression/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.4.training_multiple_output_linear_regression/","title":"Multiple Output Training","text":"<p>author: Juma Shafara date: \"2024-08-09\" title: Linear Regression Multiple Outputs keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will create a model the Pytroch way. This will help you as models get more complicated.</p> <p></p> <p>Import the following libraries:  </p> <pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch import nn,optim\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\n</code></pre> <p>Set the random seed:</p> <pre><code>torch.manual_seed(1)\n</code></pre> <p></p> <pre><code>from torch.utils.data import Dataset, DataLoader\nclass Data(Dataset):\n    def __init__(self):\n            self.x=torch.zeros(20,2)\n            self.x[:,0]=torch.arange(-1,1,0.1)\n            self.x[:,1]=torch.arange(-1,1,0.1)\n            self.w=torch.tensor([ [1.0,-1.0],[1.0,3.0]])\n            self.b=torch.tensor([[1.0,-1.0]])\n            self.f=torch.mm(self.x,self.w)+self.b\n\n            self.y=self.f+0.001*torch.randn((self.x.shape[0],1))\n            self.len=self.x.shape[0]\n\n    def __getitem__(self,index):\n\n        return self.x[index],self.y[index]\n\n    def __len__(self):\n        return self.len\n</code></pre> <p>create a dataset object </p> <pre><code>data_set=Data()\n</code></pre> <p></p> <p>Create a custom module:</p> <pre><code>class linear_regression(nn.Module):\n    def __init__(self,input_size,output_size):\n        super(linear_regression,self).__init__()\n        self.linear=nn.Linear(input_size,output_size)\n    def forward(self,x):\n        yhat=self.linear(x)\n        return yhat\n</code></pre> <p>Create an optimizer object and set the learning rate to 0.1. Don't forget to enter the model parameters in the constructor. </p> <pre><code>model=linear_regression(2,2)\n</code></pre> <p>Create an optimizer object and set the learning rate to 0.1. Don't forget to enter the model parameters in the constructor. </p> <p></p> <pre><code>optimizer = optim.SGD(model.parameters(), lr = 0.1)\n</code></pre> <p>Create the criterion function that calculates the total loss or cost:</p> <pre><code>criterion = nn.MSELoss()\n</code></pre> <p>Create a data loader object and set the batch_size to 5:</p> <pre><code>train_loader=DataLoader(dataset=data_set,batch_size=5)\n</code></pre> <p></p> <p>Run 100 epochs of Mini-Batch Gradient Descent and store the total loss or cost for every iteration. Remember that this is an approximation of the true total loss or cost.</p> <pre><code>LOSS=[]\n\nepochs=100\n\nfor epoch in range(epochs):\n    for x,y in train_loader:\n        #make a prediction \n        yhat=model(x)\n        #calculate the loss\n        loss=criterion(yhat,y)\n        #store loss/cost \n        LOSS.append(loss.item())\n        #clear gradient \n        optimizer.zero_grad()\n        #Backward pass: compute gradient of the loss with respect to all the learnable parameters\n        loss.backward()\n        #the step function on an Optimizer makes an update to its parameters\n        optimizer.step()\n</code></pre> <p>Plot the cost:</p> <pre><code>plt.plot(LOSS)\nplt.xlabel(\"iterations \")\nplt.ylabel(\"Cost/total loss \")\nplt.show()\n</code></pre>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.4.training_multiple_output_linear_regression/#objective","title":"Objective","text":"<p><ul><li> How to create a complicated models using pytorch build in functions.</li></ul></p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.4.training_multiple_output_linear_regression/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will create a model the Pytroch way. This will help you as models get more complicated.</p> <ul> <li> Make Some Data</li> <li> Create the Model and Cost Function the Pytorch way</li> <li> Train the Model: Batch Gradient Descent</li> <li> Practice Questions </li> <p></p> Estimated Time Needed: 20 min </ul>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.4.training_multiple_output_linear_regression/#make-some-data","title":"Make Some Data","text":"<p>Create a dataset class with two-dimensional features and two targets: </p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.4.training_multiple_output_linear_regression/#create-the-model-optimizer-and-total-loss-function-cost","title":"Create the Model, Optimizer, and Total Loss Function (cost)","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.4.training_multiple_output_linear_regression/#train-the-model-via-mini-batch-gradient-descent","title":"Train the Model via Mini-Batch Gradient Descent","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.4.training_multiple_output_linear_regression/#about-the-author","title":"About the Author:","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.4.training_multiple_output_linear_regression/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/muliple_input_lr_exercise/","title":"Exercise","text":"<p>title: Multiple Input LR Exercise author: Juma Shafara date: \"2024-08-29\" keywords: [data science, data analysis, programming, dataidea] description: Programming for Data Science is a subject we\u2019ve designed to explore the various programming components of data science.</p> <p></p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/muliple_input_lr_exercise/#week-4-multiple-input-linear-regression","title":"Week 4: Multiple Input Linear Regression","text":"<ol> <li>Making Predictions in Multiple Linear Regression</li> <li> <p>Exercise: Implement a multiple input linear regression model to predict a target variable based on two or more input features. Train the model on synthetic data and evaluate its performance.</p> </li> <li> <p>Training Multiple Linear Regression Models</p> </li> <li> <p>Exercise: Train multiple linear regression models with different feature sets and compare their performance. Analyze how the inclusion or exclusion of features affects the model's predictions.</p> </li> <li> <p>Multi-Target Linear Regression</p> </li> <li>Exercise: Extend your linear regression model to predict multiple target variables simultaneously. Create a dataset with multiple targets and train the model to predict all targets.</li> </ol> <p> Before the last question, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <ol> <li>Training Multiple Output Linear Regression Models</li> <li>Exercise: Implement a model to handle multiple outputs (e.g., predicting multiple continuous variables). Train the model on a dataset with multiple output variables and assess its performance.</li> </ol>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/muliple_input_lr_exercise/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/muliple_input_lr_exercise/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.1logistic_regression_prediction_v2/","title":"Prediction","text":"<p>author: Juma Shafara date: \"2024-08-08\" title: Logistic Regression keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, we will cover logistic regression using PyTorch.</p> <p></p> <p>We'll need the following libraries:  </p> <pre><code># Import the libraries we need for this lab\n\nimport torch.nn as nn\nimport torch\nimport matplotlib.pyplot as plt \n</code></pre> <p>Set the random seed:</p> <pre><code># Set the random seed\n\ntorch.manual_seed(2)\n</code></pre> <pre>\n<code>&lt;torch._C.Generator at 0x714cbc14af50&gt;</code>\n</pre> <p>Create a tensor ranging from -100 to 100:</p> <pre><code>z = torch.arange(-100, 100, 0.1).view(-1, 1)\nprint(\"The tensor: \", z)\n</code></pre> <pre>\n<code>The tensor:  tensor([[-100.0000],\n        [ -99.9000],\n        [ -99.8000],\n        ...,\n        [  99.7000],\n        [  99.8000],\n        [  99.9000]])\n</code>\n</pre> <p>Create a sigmoid object: </p> <pre><code># Create sigmoid object\n\nsig = nn.Sigmoid()\n</code></pre> <p>Apply the element-wise function Sigmoid with the object:</p> <pre><code># Use sigmoid object to calculate the \n\nyhat = sig(z)\n</code></pre> <p>Plot the results: </p> <pre><code>plt.plot(z.numpy(), yhat.numpy())\nplt.xlabel('z')\nplt.ylabel('yhat')\n</code></pre> <pre>\n<code>Text(0, 0.5, 'yhat')</code>\n</pre> <p>Apply the element-wise Sigmoid from the function module and plot the results:</p> <pre><code>yhat = torch.sigmoid(z)\nplt.plot(z.numpy(), yhat.numpy())\n</code></pre> <pre>\n<code>[&lt;matplotlib.lines.Line2D at 0x714c557a9a30&gt;]</code>\n</pre> <p>Create a 1x1 tensor where x represents one data sample with one dimension, and 2x1 tensor X represents two data samples of one dimension:</p> <pre><code># Create x and X tensor\n\nx = torch.tensor([[1.0]])\nX = torch.tensor([[1.0], [100]])\nprint('x = ', x)\nprint('X = ', X)\n</code></pre> <pre>\n<code>x =  tensor([[1.]])\nX =  tensor([[  1.],\n        [100.]])\n</code>\n</pre> <p>Create a logistic regression object with the <code>nn.Sequential</code> model with a one-dimensional input:</p> <pre><code># Use sequential function to create model\n\nmodel = nn.Sequential(nn.Linear(1, 1), nn.Sigmoid())\n</code></pre> <p>The object is represented in the following diagram: </p> <p></p> <p>In this case, the parameters are randomly initialized. You can view them the following ways:</p> <pre><code># Print the parameters\n\nprint(\"list(model.parameters()):\\n \", list(model.parameters()))\nprint(\"\\nmodel.state_dict():\\n \", model.state_dict())\n</code></pre> <pre>\n<code>list(model.parameters()):\n  [Parameter containing:\ntensor([[-0.5717,  0.1614]], requires_grad=True), Parameter containing:\ntensor([-0.6260], requires_grad=True)]\n\nmodel.state_dict():\n  OrderedDict({'linear.weight': tensor([[-0.5717,  0.1614]]), 'linear.bias': tensor([-0.6260])})\n</code>\n</pre> <p>Make a prediction with one sample:</p> <pre><code># The prediction for x\n\nyhat = model(x)\nprint(\"The prediction: \", yhat)\n</code></pre> <pre>\n<code>The prediction:  tensor([[0.2943]], grad_fn=&lt;SigmoidBackward0&gt;)\n</code>\n</pre> <p>Calling the object with tensor <code>X</code> performed the following operation (code values may not be the same as the diagrams value  depending on the version of PyTorch) :</p> <p></p> <p>Make a prediction with multiple samples:</p> <pre><code># The prediction for X\n\nyhat = model(X)\nyhat\n</code></pre> <pre>\n<code>tensor([[0.4979],\n        [1.0000]], grad_fn=&lt;SigmoidBackward0&gt;)</code>\n</pre> <p>Calling the object performed the following operation: </p> <p>Create a 1x2 tensor where x represents one data sample with one dimension, and 2x3 tensor X represents one data sample of two dimensions:</p> <pre><code># Create and print samples\n\nx = torch.tensor([[1.0, 1.0]])\nX = torch.tensor([[1.0, 1.0], [1.0, 2.0], [1.0, 3.0]])\nprint('x = ', x)\nprint('X = ', X)\n</code></pre> <pre>\n<code>x =  tensor([[1., 1.]])\nX =  tensor([[1., 1.],\n        [1., 2.],\n        [1., 3.]])\n</code>\n</pre> <p>Create a logistic regression object with the <code>nn.Sequential</code> model with a two-dimensional input: </p> <pre><code># Create new model using nn.sequential()\n\nmodel = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())\n</code></pre> <p>The object will apply the Sigmoid function to the output of the linear function as shown in the following diagram:</p> <p></p> <p>In this case, the parameters are randomly initialized. You can view them the following ways:</p> <pre><code># Print the parameters\n\nprint(\"list(model.parameters()):\\n \", list(model.parameters()))\nprint(\"\\nmodel.state_dict():\\n \", model.state_dict())\n</code></pre> <pre>\n<code>list(model.parameters()):\n  [Parameter containing:\ntensor([[ 0.1939, -0.0361]], requires_grad=True), Parameter containing:\ntensor([0.3021], requires_grad=True)]\n\nmodel.state_dict():\n  OrderedDict({'0.weight': tensor([[ 0.1939, -0.0361]]), '0.bias': tensor([0.3021])})\n</code>\n</pre> <p>Make a prediction with one sample:</p> <pre><code># Make the prediction of x\n\nyhat = model(x)\nprint(\"The prediction: \", yhat)\n</code></pre> <pre>\n<code>The prediction:  tensor([[0.6130]], grad_fn=&lt;SigmoidBackward0&gt;)\n</code>\n</pre> <p>The operation is represented in the following diagram:</p> <p></p> <p>Make a prediction with multiple samples:</p> <pre><code># The prediction of X\n\nyhat = model(X)\nprint(\"The prediction: \", yhat)\n</code></pre> <pre>\n<code>The prediction:  tensor([[0.6130],\n        [0.6044],\n        [0.5957]], grad_fn=&lt;SigmoidBackward0&gt;)\n</code>\n</pre> <p>The operation is represented in the following diagram: </p> <p></p> <p>In this section, you will build a custom Module or class. The model or object function is identical to using <code>nn.Sequential</code>.</p> <p>Create a logistic regression custom module:</p> <pre><code># Create logistic_regression custom class\n\nclass logistic_regression(nn.Module):\n\n    # Constructor\n    def __init__(self, n_inputs):\n        super(logistic_regression, self).__init__()\n        self.linear = nn.Linear(n_inputs, 1)\n\n    # Prediction\n    def forward(self, x):\n        yhat = torch.sigmoid(self.linear(x))\n        return yhat\n</code></pre> <p>Create a 1x1 tensor where x represents one data sample with one dimension, and 3x1 tensor where \\(X\\) represents one data sample of one dimension:</p> <pre><code># Create x and X tensor\n\nx = torch.tensor([[1.0]])\nX = torch.tensor([[-100], [0], [100.0]])\nprint('x = ', x)\nprint('X = ', X)\n</code></pre> <pre>\n<code>x =  tensor([[1.]])\nX =  tensor([[-100.],\n        [   0.],\n        [ 100.]])\n</code>\n</pre> <p>Create a model to predict one dimension: </p> <pre><code># Create logistic regression model\n\nmodel = logistic_regression(1)\n</code></pre> <p>In this case, the parameters are randomly initialized. You can view them the following ways:</p> <pre><code># Print parameters \n\nprint(\"list(model.parameters()):\\n \", list(model.parameters()))\nprint(\"\\nmodel.state_dict():\\n \", model.state_dict())\n</code></pre> <pre>\n<code>list(model.parameters()):\n  [Parameter containing:\ntensor([[0.2381]], requires_grad=True), Parameter containing:\ntensor([-0.1149], requires_grad=True)]\n\nmodel.state_dict():\n  OrderedDict({'linear.weight': tensor([[0.2381]]), 'linear.bias': tensor([-0.1149])})\n</code>\n</pre> <p>Make a prediction with one sample:</p> <pre><code># Make the prediction of x\n\nyhat = model(x)\nprint(\"The prediction result: \\n\", yhat)\n</code></pre> <pre>\n<code>The prediction result: \n tensor([[0.5307]], grad_fn=&lt;SigmoidBackward0&gt;)\n</code>\n</pre> <p>Make a prediction with multiple samples:</p> <pre><code># Make the prediction of X\n\nyhat = model(X)\nprint(\"The prediction result: \\n\", yhat)\n</code></pre> <pre>\n<code>The prediction result: \n tensor([[4.0805e-11],\n        [4.7130e-01],\n        [1.0000e+00]], grad_fn=&lt;SigmoidBackward0&gt;)\n</code>\n</pre> <p>Create a logistic regression object with a function with two inputs: </p> <pre><code># Create logistic regression model\n\nmodel = logistic_regression(2)\n</code></pre> <p>Create a 1x2 tensor where x represents one data sample with one dimension, and 3x2 tensor X represents one data sample of one dimension:</p> <pre><code># Create x and X tensor\n\nx = torch.tensor([[1.0, 2.0]])\nX = torch.tensor([[100, -100], [0.0, 0.0], [-100, 100]])\nprint('x = ', x)\nprint('X = ', X)\n</code></pre> <pre>\n<code>x =  tensor([[1., 2.]])\nX =  tensor([[ 100., -100.],\n        [   0.,    0.],\n        [-100.,  100.]])\n</code>\n</pre> <p>Make a prediction with one sample:</p> <pre><code># Make the prediction of x\n\nyhat = model(x)\nprint(\"The prediction result: \\n\", yhat)\n</code></pre> <pre>\n<code>The prediction result: \n tensor([[0.2943]], grad_fn=&lt;SigmoidBackward0&gt;)\n</code>\n</pre> <p>Make a prediction with multiple samples: </p> <pre><code># Make the prediction of X\n\nyhat = model(X)\nprint(\"The prediction result: \\n\", yhat)\n</code></pre> <pre>\n<code>The prediction result: \n tensor([[7.7529e-33],\n        [3.4841e-01],\n        [1.0000e+00]], grad_fn=&lt;SigmoidBackward0&gt;)\n</code>\n</pre> <p>Make your own model <code>my_model</code> as applying linear regression first and then logistic regression using <code>nn.Sequential()</code>. Print out your prediction.</p> <pre><code># Practice: Make your model and make the prediction\n\nX = torch.tensor([-10.0])\n\nmy_model = nn.Sequential(nn.Linear(1, 1), nn.Sigmoid())\n\nmy_model(X)\n</code></pre> <pre>\n<code>tensor([0.2231], grad_fn=&lt;SigmoidBackward0&gt;)</code>\n</pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.1logistic_regression_prediction_v2/#objective","title":"Objective","text":"<p><ul><li> How to create a logistic regression object with the nn.Sequential model.</li></ul></p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.1logistic_regression_prediction_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, we will cover logistic regression using PyTorch.</p> <ul> <li> Logistic Function</li> <li> Build a Logistic Regression Using nn.Sequential</li> <li> Build Custom Modules</li> </ul> <p>Estimated Time Needed: 15 min</p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.1logistic_regression_prediction_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.1logistic_regression_prediction_v2/#Log","title":"Logistic Function","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.1logistic_regression_prediction_v2/#Seq","title":"Build a Logistic Regression with <code>nn.Sequential</code>","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.1logistic_regression_prediction_v2/#Model","title":"Build Custom Modules","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.1logistic_regression_prediction_v2/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.1logistic_regression_prediction_v2/#about-the-author","title":"About the Author:","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.1logistic_regression_prediction_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/","title":"MSE Problems","text":"<p>author: Juma Shafara date: \"2024-08-12\" title: Logistic Regression and Bad Initialization Value keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will see what happens when you use the root mean square error cost or total loss function and select a bad initialization value for the parameter values.</p> <p></p> <p>We'll need the following libraries:</p> <pre><code># Import the libraries we need for this lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nfrom mpl_toolkits import mplot3d\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n</code></pre> <p>The class <code>plot_error_surfaces</code> is just to help you visualize the data space and the Parameter space during training and has nothing to do with Pytorch. </p> <pre><code># Create class for plotting and the function for plotting\n\nclass plot_error_surfaces(object):\n\n    # Construstor\n    def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):\n        W = np.linspace(-w_range, w_range, n_samples)\n        B = np.linspace(-b_range, b_range, n_samples)\n        w, b = np.meshgrid(W, B)    \n        Z = np.zeros((30, 30))\n        count1 = 0\n        self.y = Y.numpy()\n        self.x = X.numpy()\n        for w1, b1 in zip(w, b):\n            count2 = 0\n            for w2, b2 in zip(w1, b1):\n                Z[count1, count2] = np.mean((self.y - (1 / (1 + np.exp(-1*w2 * self.x - b2)))) ** 2)\n                count2 += 1   \n            count1 += 1\n        self.Z = Z\n        self.w = w\n        self.b = b\n        self.W = []\n        self.B = []\n        self.LOSS = []\n        self.n = 0\n        if go == True:\n            plt.figure()\n            plt.figure(figsize=(7.5, 5))\n            plt.axes(projection='3d').plot_surface(self.w, self.b, self.Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n            plt.title('Loss Surface')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.show()\n            plt.figure()\n            plt.title('Loss Surface Contour')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.contour(self.w, self.b, self.Z)\n            plt.show()\n\n     # Setter\n    def set_para_loss(self, model, loss):\n        self.n = self.n + 1\n        self.W.append(list(model.parameters())[0].item())\n        self.B.append(list(model.parameters())[1].item())\n        self.LOSS.append(loss)\n\n    # Plot diagram\n    def final_plot(self): \n        ax = plt.axes(projection='3d')\n        ax.plot_wireframe(self.w, self.b, self.Z)\n        ax.scatter(self.W, self.B, self.LOSS, c='r', marker='x', s=200, alpha=1)\n        plt.figure()\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c='r', marker='x')\n        plt.xlabel('w')\n        plt.ylabel('b')\n        plt.show()\n\n    # Plot diagram\n    def plot_ps(self):\n        plt.subplot(121)\n        plt.ylim\n        plt.plot(self.x, self.y, 'ro', label=\"training points\")\n        plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label=\"estimated line\")\n        plt.plot(self.x, 1 / (1 + np.exp(-1 * (self.W[-1] * self.x + self.B[-1]))), label='sigmoid')\n        plt.xlabel('x')\n        plt.ylabel('y')\n        plt.ylim((-0.1, 2))\n        plt.title('Data Space Iteration: ' + str(self.n))\n        plt.show()\n        plt.subplot(122)\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c='r', marker='x')\n        plt.title('Loss Surface Contour Iteration' + str(self.n))\n        plt.xlabel('w')\n        plt.ylabel('b')\n\n# Plot the diagram\n\ndef PlotStuff(X, Y, model, epoch, leg=True):\n    plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))\n    plt.plot(X.numpy(), Y.numpy(), 'r')\n    if leg == True:\n        plt.legend()\n    else:\n        pass\n</code></pre> <p>Set the random seed:</p> <pre><code># Set random seed\n\ntorch.manual_seed(0)\n</code></pre> <pre>\n<code>&lt;torch._C.Generator at 0x7c89b4a521f0&gt;</code>\n</pre> <p>Create the <code>Data</code> class</p> <pre><code># Create the data class\n\nclass Data(Dataset):\n\n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-1, 1, 0.1).view(-1, 1)\n        self.y = torch.zeros(self.x.shape[0], 1)\n        self.y[self.x[:, 0] &amp;gt; 0.2] = 1\n        self.len = self.x.shape[0]\n\n    # Getter\n    def __getitem__(self, index):      \n        return self.x[index], self.y[index]\n\n    # Get Length\n    def __len__(self):\n        return self.len\n</code></pre> <p>Make <code>Data</code> object</p> <pre><code># Create Data object\n\ndata_set = Data()\n</code></pre> <p>Create a custom module for logistic regression:</p> <pre><code># Create logistic_regression class\n\nclass logistic_regression(nn.Module):\n\n    # Constructor\n    def __init__(self, n_inputs):\n        super(logistic_regression, self).__init__()\n        self.linear = nn.Linear(n_inputs, 1)\n\n    # Prediction\n    def forward(self, x):\n        yhat = torch.sigmoid(self.linear(x))\n        return yhat\n</code></pre> <p>Create a logistic regression object or model: </p> <pre><code># Create the logistic_regression result\n\nmodel = logistic_regression(1)\n</code></pre> <p>Replace the random initialized variable values with some predetermined values that will not converge:  </p> <pre><code># Set the weight and bias\n\nmodel.state_dict() ['linear.weight'].data[0] = torch.tensor([[-5]])\nmodel.state_dict() ['linear.bias'].data[0] = torch.tensor([[-10]])\nprint(\"The parameters: \", model.state_dict())\n</code></pre> <pre>\n<code>The parameters:  OrderedDict({'linear.weight': tensor([[-5.]]), 'linear.bias': tensor([-10.])})\n</code>\n</pre> <p>Create a <code> plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> <pre><code># Create the plot_error_surfaces object\n\nget_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30)\n</code></pre> <pre>\n<code>&lt;Figure size 640x480 with 0 Axes&gt;</code>\n</pre> <p>Define the dataloader, the cost or criterion function, the optimizer: </p> <pre><code># Create dataloader object, crierion function and optimizer.\n\ntrainloader = DataLoader(dataset=data_set, batch_size=3)\ncriterion_rms = nn.MSELoss()\nlearning_rate = 2\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n</code></pre> <p></p> <p>Train the model</p> <pre><code># Train the model\n\ndef train_model(epochs):\n    for epoch in range(epochs):\n        for x, y in trainloader: \n            yhat = model(x)\n            loss = criterion_rms(yhat, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            get_surface.set_para_loss(model, loss.tolist())\n        if epoch % 20 == 0:\n            get_surface.plot_ps()\n\ntrain_model(100)\n</code></pre> <p>Get the actual class of each sample and calculate the accuracy on the test data:</p> <pre><code># Make the Prediction\n\nyhat = model(data_set.x)\nlabel = yhat &amp;gt; 0.5\nprint(\"The accuracy: \", torch.mean((label == data_set.y.type(torch.ByteTensor)).type(torch.float)))\n</code></pre> <pre>\n<code>The accuracy:  tensor(0.6500)\n</code>\n</pre> <p>Accuracy is 60% compared to 100% in the last lab using a good Initialization value. </p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/#objective","title":"Objective","text":"<p><ul><li> How bad initialization value can affects the accuracy of model. .</li></ul></p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will see what happens when you use the root mean square error cost or total loss function and select a bad initialization value for the parameter values.</p> <ul> <li> Make Some Data</li> <li> Create the Model and Cost Function the PyTorch way</li> <li> Train the Model:Batch Gradient Descent</li> </ul> <p></p> <p>Estimated Time Needed: 30 min</p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/#helper-functions","title":"Helper functions","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/#Makeup_Data","title":"Get Some Data","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/#Model_Cost","title":"Create the Model and Total Loss Function (Cost)","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/#train-the-model-via-batch-gradient-descent","title":"Train the Model via Batch Gradient Descent","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/#about-the-author","title":"About the Author:","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.3_cross_entropy_logistic_regression_v2/","title":"Cross-Entropy","text":"<p>author: Juma Shafara date: \"2024-08-12\" title: Training Negative Log likelihood (Cross-Entropy) keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will see what happens when you use the Cross-Entropy or total loss function using random initialization for a parameter value.</p> <p></p> <p>We'll need the following libraries:</p> <pre><code># Import the libraries we need for this lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nfrom mpl_toolkits import mplot3d\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n</code></pre> <p>The class <code>plot_error_surfaces</code> is just to help you visualize the data space and the parameter space during training and has nothing to do with Pytorch. </p> <pre><code># Create class for plotting and the function for plotting\n\nclass plot_error_surfaces(object):\n\n    # Construstor\n    def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):\n        W = np.linspace(-w_range, w_range, n_samples)\n        B = np.linspace(-b_range, b_range, n_samples)\n        w, b = np.meshgrid(W, B)    \n        Z = np.zeros((30, 30))\n        count1 = 0\n        self.y = Y.numpy()\n        self.x = X.numpy()\n        for w1, b1 in zip(w, b):\n            count2 = 0\n            for w2, b2 in zip(w1, b1):\n                yhat= 1 / (1 + np.exp(-1*(w2*self.x+b2)))\n                Z[count1,count2]=-1*np.mean(self.y*np.log(yhat+1e-16) +(1-self.y)*np.log(1-yhat+1e-16))\n                count2 += 1   \n            count1 += 1\n        self.Z = Z\n        self.w = w\n        self.b = b\n        self.W = []\n        self.B = []\n        self.LOSS = []\n        self.n = 0\n        if go == True:\n            plt.figure()\n            plt.figure(figsize=(7.5, 5))\n            plt.axes(projection='3d').plot_surface(self.w, self.b, self.Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n            plt.title('Loss Surface')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.show()\n            plt.figure()\n            plt.title('Loss Surface Contour')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.contour(self.w, self.b, self.Z)\n            plt.show()\n\n     # Setter\n    def set_para_loss(self, model, loss):\n        self.n = self.n + 1\n        self.W.append(list(model.parameters())[0].item())\n        self.B.append(list(model.parameters())[1].item())\n        self.LOSS.append(loss)\n\n    # Plot diagram\n    def final_plot(self): \n        ax = plt.axes(projection='3d')\n        ax.plot_wireframe(self.w, self.b, self.Z)\n        ax.scatter(self.W, self.B, self.LOSS, c='r', marker='x', s=200, alpha=1)\n        plt.figure()\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c='r', marker='x')\n        plt.xlabel('w')\n        plt.ylabel('b')\n        plt.show()\n\n    # Plot diagram\n    def plot_ps(self):\n        plt.subplot(121)\n        plt.ylim\n        plt.plot(self.x, self.y, 'ro', label=\"training points\")\n        plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label=\"estimated line\")\n        plt.plot(self.x, 1 / (1 + np.exp(-1 * (self.W[-1] * self.x + self.B[-1]))), label='sigmoid')\n        plt.xlabel('x')\n        plt.ylabel('y')\n        plt.ylim((-0.1, 2))\n        plt.title('Data Space Iteration: ' + str(self.n))\n        plt.show()\n        plt.subplot(122)\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c='r', marker='x')\n        plt.title('Loss Surface Contour Iteration' + str(self.n))\n        plt.xlabel('w')\n        plt.ylabel('b')\n\n# Plot the diagram\n\ndef PlotStuff(X, Y, model, epoch, leg=True):\n    plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))\n    plt.plot(X.numpy(), Y.numpy(), 'r')\n    if leg == True:\n        plt.legend()\n    else:\n        pass\n</code></pre> <p>Set the random seed:</p> <pre><code># Set random seed\n\ntorch.manual_seed(0)\n</code></pre> <pre>\n<code>&lt;torch._C.Generator at 0x75ee4c9721f0&gt;</code>\n</pre> <pre><code># Create the data class\n\nclass Data(Dataset):\n\n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-1, 1, 0.1).view(-1, 1)\n        self.y = torch.zeros(self.x.shape[0], 1)\n        self.y[self.x[:, 0] &amp;gt; 0.2] = 1\n        self.len = self.x.shape[0]\n\n    # Getter\n    def __getitem__(self, index):      \n        return self.x[index], self.y[index]\n\n    # Get length\n    def __len__(self):\n        return self.len\n</code></pre> <p>Make <code>Data</code> object</p> <pre><code># Create Data object\n\ndata_set = Data()\n</code></pre> <p>Create a custom module for logistic regression:</p> <pre><code># Create logistic_regression class\n\nclass logistic_regression(nn.Module):\n\n    # Constructor\n    def __init__(self, n_inputs):\n        super(logistic_regression, self).__init__()\n        self.linear = nn.Linear(n_inputs, 1)\n\n    # Prediction\n    def forward(self, x):\n        yhat = torch.sigmoid(self.linear(x))\n        return yhat\n</code></pre> <p>Create a logistic regression object or model: </p> <pre><code># Create the logistic_regression result\n\nmodel = logistic_regression(1)\n</code></pre> <p>Replace the random initialized variable values. Theses random initialized variable values did convergence for the RMS Loss but will converge for the Cross-Entropy Loss.</p> <pre><code># Set the weight and bias\n\nmodel.state_dict() ['linear.weight'].data[0] = torch.tensor([[-5]])\nmodel.state_dict() ['linear.bias'].data[0] = torch.tensor([[-10]])\nprint(\"The parameters: \", model.state_dict())\n</code></pre> <pre>\n<code>The parameters:  OrderedDict({'linear.weight': tensor([[-5.]]), 'linear.bias': tensor([-10.])})\n</code>\n</pre> <p>Create a <code> plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> <pre><code># Create the plot_error_surfaces object\n\nget_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30)\n</code></pre> <pre>\n<code>&lt;Figure size 640x480 with 0 Axes&gt;</code>\n</pre> <p>Define the cost or criterion function: </p> <pre><code># Create dataloader, criterion function and optimizer\n\ndef criterion(yhat,y):\n    out = -1 * torch.mean(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))\n    return out\n\n# Build in criterion\n# criterion = nn.BCELoss()\n\ntrainloader = DataLoader(dataset = data_set, batch_size = 3)\nlearning_rate = 2\noptimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n</code></pre> <p>Train the model</p> <pre><code># Train the Model\n\ndef train_model(epochs):\n    for epoch in range(epochs):\n        for x, y in trainloader:\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            get_surface.set_para_loss(model, loss.tolist())\n        if epoch % 20 == 0:\n            get_surface.plot_ps()\n\ntrain_model(100)\n</code></pre> <p>Get the actual class of each sample and calculate the accuracy on the test data:</p> <pre><code># Make the Prediction\n\nyhat = model(data_set.x)\nlabel = yhat &amp;gt; 0.5\nprint(\"The accuracy: \", torch.mean((label == data_set.y.type(torch.ByteTensor)).type(torch.float)))\n</code></pre> <pre>\n<code>The accuracy:  tensor(1.)\n</code>\n</pre> <p>The accuracy is perfect.</p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.3_cross_entropy_logistic_regression_v2/#objective","title":"Objective","text":"<p><ul><li> How Cross-Entropy using random initialization influence the accuracy of the model.</li></ul></p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.3_cross_entropy_logistic_regression_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will see what happens when you use the Cross-Entropy or total loss function using random initialization for a parameter value.</p> <ul> <li> Make Some Data</li> <li> Create the Model and Cost Function the PyTorch way </li> <li> Train the Model: Batch Gradient Descent</li> </ul> <p>Estimated Time Needed: 30 min</p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.3_cross_entropy_logistic_regression_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.3_cross_entropy_logistic_regression_v2/#Makeup_Data","title":"Get Some Data","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.3_cross_entropy_logistic_regression_v2/#Model_Cost","title":"Create the Model and Total Loss Function (Cost)","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.3_cross_entropy_logistic_regression_v2/#BGD","title":"Train the Model via Batch Gradient Descent","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.3_cross_entropy_logistic_regression_v2/#about-the-author","title":"About the Author:","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.3_cross_entropy_logistic_regression_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.4softmax_in_one_dimension_v2/","title":"Softmax","text":"<p>author: Juma Shafara date: \"2024-08-12\" title: Softmax Classifer 1D keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will use Softmax to classify three linearly separable classes, the features are in one dimension </p> <p></p> <p>We'll need the following libraries:</p> <pre><code># Import the libraries we need for this lab\n\nimport torch.nn as nn\nimport torch\nimport matplotlib.pyplot as plt \nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\n</code></pre> <p>Use the helper function to plot labeled data points: </p> <pre><code># Create class for plotting\n\ndef plot_data(data_set, model = None, n = 1, color = False):\n    X = data_set[:][0]\n    Y = data_set[:][1]\n    plt.plot(X[Y == 0, 0].numpy(), Y[Y == 0].numpy(), 'bo', label = 'y = 0')\n    plt.plot(X[Y == 1, 0].numpy(), 0 * Y[Y == 1].numpy(), 'ro', label = 'y = 1')\n    plt.plot(X[Y == 2, 0].numpy(), 0 * Y[Y == 2].numpy(), 'go', label = 'y = 2')\n    plt.ylim((-0.1, 3))\n    plt.legend()\n    if model != None:\n        w = list(model.parameters())[0][0].detach()\n        b = list(model.parameters())[1][0].detach()\n        y_label = ['yhat=0', 'yhat=1', 'yhat=2']\n        y_color = ['b', 'r', 'g']\n        Y = []\n        for w, b, y_l, y_c in zip(model.state_dict()['0.weight'], model.state_dict()['0.bias'], y_label, y_color):\n            Y.append((w * X + b).numpy())\n            plt.plot(X.numpy(), (w * X + b).numpy(), y_c, label = y_l)\n        if color == True:\n            x = X.numpy()\n            x = x.reshape(-1)\n            top = np.ones(x.shape)\n            y0 = Y[0].reshape(-1)\n            y1 = Y[1].reshape(-1)\n            y2 = Y[2].reshape(-1)\n            plt.fill_between(x, y0, where = y1 &amp;gt; y1, interpolate = True, color = 'blue')\n            plt.fill_between(x, y0, where = y1 &amp;gt; y2, interpolate = True, color = 'blue')\n            plt.fill_between(x, y1, where = y1 &amp;gt; y0, interpolate = True, color = 'red')\n            plt.fill_between(x, y1, where = ((y1 &amp;gt; y2) * (y1 &amp;gt; y0)),interpolate = True, color = 'red')\n            plt.fill_between(x, y2, where = (y2 &amp;gt; y0) * (y0 &amp;gt; 0),interpolate = True, color = 'green')\n            plt.fill_between(x, y2, where = (y2 &amp;gt; y1), interpolate = True, color = 'green')\n    plt.legend()\n    plt.show()\n</code></pre> <p>Set the random seed:</p> <pre><code>#Set the random seed\n\ntorch.manual_seed(0)\n</code></pre> <pre>\n<code>&lt;torch._C.Generator at 0x7b9882167170&gt;</code>\n</pre> <p>Create some linearly separable data with three classes: </p> <pre><code># Create the data class\n\nclass Data(Dataset):\n\n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-2, 2, 0.1).view(-1, 1)\n        self.y = torch.zeros(self.x.shape[0])\n        self.y[(self.x &amp;gt; -1.0)[:, 0] * (self.x &amp;lt; 1.0)[:, 0]] = 1\n        self.y[(self.x &amp;gt;= 1.0)[:, 0]] = 2\n        self.y = self.y.type(torch.LongTensor)\n        self.len = self.x.shape[0]\n\n    # Getter\n    def __getitem__(self,index):      \n        return self.x[index], self.y[index]\n\n    # Get Length\n    def __len__(self):\n        return self.len\n</code></pre> <p>Create the dataset object:</p> <pre><code># Create the dataset object and plot the dataset object\n\ndata_set = Data()\ndata_set.x\nplot_data(data_set)\n</code></pre> <p>Build a Softmax classifier by using the Sequential module:</p> <pre><code># Build Softmax Classifier technically you only need nn.Linear\n\nmodel = nn.Sequential(nn.Linear(1, 3))\nmodel.state_dict()\n</code></pre> <pre>\n<code>OrderedDict([('0.weight',\n              tensor([[ 0.5239],\n                      [-0.4626],\n                      [-0.4925]])),\n             ('0.bias', tensor([-0.0875, -0.0961, -0.7790]))])</code>\n</pre> <pre><code>data_set[0]\n</code></pre> <pre>\n<code>(tensor([-2.]), tensor(0))</code>\n</pre> <pre><code>model(data_set[0][0])\n</code></pre> <pre>\n<code>tensor([-0.7210, -1.4580,  1.9142], grad_fn=&lt;ViewBackward0&gt;)</code>\n</pre> <p>Create the criterion function, the optimizer and the dataloader</p> <pre><code># Create criterion function, optimizer, and dataloader\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\ntrainloader = DataLoader(dataset = data_set, batch_size = 5)\n</code></pre> <p>Train the model for every 50 epochs plot, the line generated for each class.</p> <pre><code># Train the model\n\nLOSS = []\ndef train_model(epochs):\n    for epoch in range(epochs):\n        if epoch % 50 == 0:\n            pass\n            plot_data(data_set, model)\n        for x, y in trainloader:\n            optimizer.zero_grad()\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            LOSS.append(loss)\n            loss.backward()\n            optimizer.step()\ntrain_model(300)\n</code></pre> <p>Find the predicted class on the test data: </p> <pre><code>pred_0 = model(data_set.x[10:20])\n_, yhat_0 = pred_0.max(1)\nprint(yhat_0)\n</code></pre> <pre>\n<code>tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n</code>\n</pre> <pre><code># Make the prediction\n\nz =  model(data_set.x)\n_, yhat = z.max(1)\nprint(\"The prediction:\", yhat)\n</code></pre> <pre>\n<code>The prediction: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n</code>\n</pre> <p>Calculate the accuracy on the test data:</p> <pre><code># Print the accuracy\n\ncorrect = (data_set.y == yhat).sum().item()\naccuracy = correct / len(data_set)\nprint(\"The accuracy: \", accuracy)\n</code></pre> <pre>\n<code>The accuracy:  0.225\n</code>\n</pre> <p>You can also use the softmax function to convert the output to a probability,first, we create a Softmax object:</p> <pre><code>Softmax_fn=nn.Softmax(dim=-1)\n</code></pre> <p>The result is a tensor <code> Probability </code>, where each row corresponds to a different sample, and each column corresponds to that sample  belonging to a particular class</p> <pre><code>Probability = Softmax_fn(z)\n</code></pre> <p>we can obtain the probability of the first sample belonging to the first, second and third class respectively as follows:</p> <pre><code>for i in range(3):\n    print(f\"probability of class {i} isg given by  {Probability[0,i]}\")\n</code></pre> <pre>\n<code>probability of class 0 isg given by  0.08364968001842499\nprobability of class 1 isg given by  0.5964528322219849\nprobability of class 2 isg given by  0.31989753246307373\n</code>\n</pre>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.4softmax_in_one_dimension_v2/#objective","title":"Objective","text":"<p><ul><li> How to build a Softmax classifier by using the Sequential module in pytorch.</li></ul></p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.4softmax_in_one_dimension_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will use Softmax to classify three linearly separable classes, the features are in one dimension </p> <ul> <li> Make Some Data</li> <li> Build Softmax Classifier</li> <li> Train the Model</li> <li> Analyze Results</li> </ul> <p>Estimated Time Needed: 25 min</p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.4softmax_in_one_dimension_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.4softmax_in_one_dimension_v2/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.4softmax_in_one_dimension_v2/#Softmax","title":"Build a Softmax Classifier","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.4softmax_in_one_dimension_v2/#Model","title":"Train the Model","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.4softmax_in_one_dimension_v2/#Result","title":"Analyze Results","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.4softmax_in_one_dimension_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.4softmax_in_one_dimension_v2/#about-the-author","title":"About the Author:","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/logistic_regression_exercise/","title":"Exercise","text":"<p>title: Logistic Regression Exercise author: Juma Shafara date: \"2024-08-29\" keywords: [data science, data analysis, programming, dataidea] description: Programming for Data Science is a subject we\u2019ve designed to explore the various programming components of data science.</p> <p></p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/logistic_regression_exercise/#week-5-logistic-regression","title":"Week 5: Logistic Regression","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/logistic_regression_exercise/#1-making-predictions-in-logistic-regression","title":"1. Making Predictions in Logistic Regression","text":"<ul> <li>Exercise: Implement a logistic regression model to classify data into two classes. Use a synthetic dataset with two features and evaluate the model\u2019s accuracy using confusion matrix.</li> </ul>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/logistic_regression_exercise/#2-logistic-regression-and-bad-initialization-values","title":"2. Logistic Regression and Bad Initialization Values","text":"<ul> <li>Exercise: Train a logistic regression model with various initialization strategies. Analyze how different initial weights affect the convergence and performance of the model.</li> </ul>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/logistic_regression_exercise/#3-cross-entropy-loss-function","title":"3. Cross Entropy Loss Function","text":"<ul> <li>Exercise: Implement the cross-entropy loss function from scratch for a logistic regression model. Compare its output with PyTorch\u2019s built-in <code>torch.nn.CrossEntropyLoss</code> function.</li> </ul> <p> Before the last, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/logistic_regression_exercise/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/logistic_regression_exercise/#4-softmax-activation-in-1-dimension","title":"4. Softmax Activation in 1 Dimension","text":"<ul> <li>Exercise: Implement the softmax activation function for a vector of logits in a 1D tensor. Use this function to convert raw model outputs into probabilities and interpret the results.</li> </ul>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/logistic_regression_exercise/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week6-Practice/6.1_custom_datasets/","title":"Custom Datasets","text":"<p>author: Juma Shafara date: \"2024-09-04\" title: Custom Datasets Practice keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will review how to make a prediction in several different ways by using PyTorch.</p> <p></p> <pre><code>import pandas as pd\nimport dataidea_science as ds\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n</code></pre> <pre><code>boston_ = ds.loadDataset('boston')\n</code></pre> <pre><code>class BostonDataset(Dataset): \n\n    def __init__(self):\n        # define our dataset\n        self.data = boston_\n        self.x = torch.tensor(self.data.drop('MEDV', axis=1).values, dtype=torch.float32)\n        self.y = torch.tensor(self.data.MEDV.values, dtype=torch.float32)\n        self.samples = self.data.shape[0]\n\n    def __getitem__(self, index):\n        # access samples\n        return self.x[index], self.y[index]\n\n    def __len__(self):\n        # len(dataset)\n        return self.samples\n</code></pre> <pre><code>boston_dataset = BostonDataset()\n\nrow_1 = boston_dataset[1]\nprint('Row 1 Features:', row_1[0])\nprint('Row 1 Outcome:', row_1[1])\n\nlength_ = len(boston_dataset)\nprint('Total Samples: ', length_)\n</code></pre> <pre>\n<code>Row 1 Features: tensor([2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01, 6.4210e+00,\n        7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01, 3.9690e+02,\n        9.1400e+00])\nRow 1 Outcome: tensor(21.6000)\nTotal Samples:  506\n</code>\n</pre>"},{"location":"Deep%20Learning/Week6-Practice/6.1_custom_datasets/#custom-dataset-class","title":"Custom Dataset Class","text":""},{"location":"Deep%20Learning/Week6-Practice/6.2_dataloaders/","title":"DataLoaders","text":"<p>author: Juma Shafara date: \"2024-09-04\" title: DataLoaders Practice keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will review how to make a prediction in several different ways by using PyTorch.</p> <p></p> <pre><code>import pandas as pd\nimport dataidea_science as ds\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n</code></pre> <pre><code>boston_ = ds.loadDataset('boston')\n</code></pre> <pre><code>class BostonDataset(Dataset): \n\n    def __init__(self):\n        # define our dataset\n        self.data = boston_\n        self.x = torch.tensor(self.data.drop('MEDV', axis=1).values, dtype=torch.float32)\n        self.y = torch.tensor(self.data.MEDV.values, dtype=torch.float32)\n        self.samples = self.data.shape[0]\n\n    def __getitem__(self, index):\n        # access samples\n        return self.x[index], self.y[index]\n\n    def __len__(self):\n        # len(dataset)\n        return self.samples\n</code></pre> <pre><code>boston_dataset = BostonDataset()\n\nrow_1 = boston_dataset[1]\nprint('Row 1 Features:', row_1[0])\nprint('Row 1 Outcome:', row_1[1])\n\nlength_ = len(boston_dataset)\nprint('Total Samples: ', length_)\n</code></pre> <pre>\n<code>Row 1 Features: tensor([2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01, 6.4210e+00,\n        7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01, 3.9690e+02,\n        9.1400e+00])\nRow 1 Outcome: tensor(21.6000)\nTotal Samples:  506\n</code>\n</pre> <pre><code>boston_dataloader = DataLoader(dataset=boston_dataset,\n                               batch_size=3,\n                               shuffle=True,\n                               num_workers=2)\n</code></pre> <pre><code>for batch_no, (x, y) in enumerate(boston_dataloader):\n    print(f'Batch: {batch_no}:')\n    print(f'Data: {x}')\n    print(f'Labels: {y}')\n\n    if batch_no == 0:\n        break\n</code></pre> <pre>\n<code>Batch: 0:\nData: tensor([[2.9819e-01, 0.0000e+00, 6.2000e+00, 0.0000e+00, 5.0400e-01, 7.6860e+00,\n         1.7000e+01, 3.3751e+00, 8.0000e+00, 3.0700e+02, 1.7400e+01, 3.7751e+02,\n         3.9200e+00],\n        [6.8012e+00, 0.0000e+00, 1.8100e+01, 0.0000e+00, 7.1300e-01, 6.0810e+00,\n         8.4400e+01, 2.7175e+00, 2.4000e+01, 6.6600e+02, 2.0200e+01, 3.9690e+02,\n         1.4700e+01],\n        [1.5874e+01, 0.0000e+00, 1.8100e+01, 0.0000e+00, 6.7100e-01, 6.5450e+00,\n         9.9100e+01, 1.5192e+00, 2.4000e+01, 6.6600e+02, 2.0200e+01, 3.9690e+02,\n         2.1080e+01]])\nLabels: tensor([46.7000, 20.0000, 10.9000])\n</code>\n</pre>"},{"location":"Deep%20Learning/Week6-Practice/6.2_dataloaders/#custom-dataset","title":"Custom Dataset","text":""},{"location":"Deep%20Learning/Week6-Practice/6.2_dataloaders/#dataloaders","title":"DataLoaders","text":""},{"location":"Deep%20Learning/Week6-Practice/6.3_transforms/","title":"Transforms","text":"<p>author: Juma Shafara date: \"2024-09-04\" title: Transform Practice keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will review how to make a prediction in several different ways by using PyTorch.</p> <p></p> <pre><code>import dataidea_science as ds\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n</code></pre> <pre><code>boston_ = ds.loadDataset('boston')\n</code></pre> <pre><code>class BostonDataset(Dataset): \n\n    def __init__(self, transform=None):\n        # define our dataset\n        self.data = boston_\n        self.x = self.data.drop('MEDV', axis=1).values\n        self.y = self.data.MEDV.values\n        self.samples = self.data.shape[0]\n        self.transform = transform\n\n    def __getitem__(self, index):\n        # access samples\n        sample = (self.x[index], self.y[index])\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n    def __len__(self):\n        # len(dataset)\n        return self.samples\n</code></pre> <pre><code>boston_dataset = BostonDataset()\n\nrow_1 = boston_dataset[1]\nprint('Row 1 Features:', row_1[0])\nprint('Row 1 Outcome:', row_1[1])\n\nlength_ = len(boston_dataset)\nprint('Total Samples: ', length_)\n</code></pre> <pre>\n<code>Row 1 Features: [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n 7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n 9.1400e+00]\nRow 1 Outcome: 21.6\nTotal Samples:  506\n</code>\n</pre> <pre><code>boston_dataloader = DataLoader(dataset=boston_dataset,\n                               batch_size=3,\n                               shuffle=True,\n                               num_workers=2)\n</code></pre> <pre><code>for batch_no, (x, y) in enumerate(boston_dataloader):\n    print(f'Batch: {batch_no}:')\n    print(f'Data: {x}')\n    print(f'Labels: {y}')\n\n    if batch_no == 0:\n        break\n</code></pre> <pre>\n<code>Batch: 0:\nData: tensor([[9.7617e-01, 0.0000e+00, 2.1890e+01, 0.0000e+00, 6.2400e-01, 5.7570e+00,\n         9.8400e+01, 2.3460e+00, 4.0000e+00, 4.3700e+02, 2.1200e+01, 2.6276e+02,\n         1.7310e+01],\n        [2.9090e-01, 0.0000e+00, 2.1890e+01, 0.0000e+00, 6.2400e-01, 6.1740e+00,\n         9.3600e+01, 1.6119e+00, 4.0000e+00, 4.3700e+02, 2.1200e+01, 3.8808e+02,\n         2.4160e+01],\n        [5.5007e-01, 2.0000e+01, 3.9700e+00, 0.0000e+00, 6.4700e-01, 7.2060e+00,\n         9.1600e+01, 1.9301e+00, 5.0000e+00, 2.6400e+02, 1.3000e+01, 3.8789e+02,\n         8.1000e+00]], dtype=torch.float64)\nLabels: tensor([15.6000, 14.0000, 36.5000], dtype=torch.float64)\n</code>\n</pre> <pre><code>class TensorTransformer:\n\n    def __init__(self, dtype=torch.float32):\n        self.dtype = dtype\n\n    def __call__(self, sample):\n        x_tensor = torch.tensor(data=sample[0], dtype=self.dtype)\n        y_tensor = torch.tensor(data=sample[1], dtype=self.dtype)\n        return x_tensor, y_tensor\n</code></pre> <pre><code>boston_dataset = BostonDataset(transform=TensorTransformer())\n\nrow_1 = boston_dataset[1]\nprint('Row 1 Features:', row_1[0])\nprint('Row 1 Outcome:', row_1[1])\n</code></pre> <pre>\n<code>Row 1 Features: tensor([2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01, 6.4210e+00,\n        7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01, 3.9690e+02,\n        9.1400e+00])\nRow 1 Outcome: tensor(21.6000)\n</code>\n</pre> <pre><code>\n</code></pre>"},{"location":"Deep%20Learning/Week6-Practice/6.3_transforms/#transforms","title":"Transforms","text":""},{"location":"Deep%20Learning/Week6-Practice/6.3_transforms/#custom-dataset","title":"Custom Dataset","text":""},{"location":"Deep%20Learning/Week6-Practice/6.3_transforms/#dataloader","title":"DataLoader","text":""},{"location":"Deep%20Learning/Week6-Practice/6.3_transforms/#transformer","title":"Transformer","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.1_simple1hiddenlayer/","title":"Hidden Layers","text":"<p>author: Juma Shafara date: \"2024-08-09\" title: Simple One Hidden Layer Neural Network keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will use a single-layer neural network to classify non linearly seprable data in 1-Ddatabase.</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>We'll need the following libraries</p> <pre><code># Import the libraries we need for this lab\n\nimport torch \nimport torch.nn as nn\nfrom torch import sigmoid\nimport matplotlib.pylab as plt\nimport numpy as np\ntorch.manual_seed(0)\n</code></pre> <pre>\n<code>&lt;torch._C.Generator at 0x7c4f98773170&gt;</code>\n</pre> <p>Used for plotting the model</p> <pre><code># The function for plotting the model\n\ndef PlotStuff(X, Y, model, epoch, leg=True):\n\n    plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))\n    plt.plot(X.numpy(), Y.numpy(), 'r')\n    plt.xlabel('x')\n    if leg == True:\n        plt.legend()\n    else:\n        pass\n</code></pre> <p>Define the activations and the output of the first linear layer as an attribute. Note that this is not good practice. </p> <pre><code># Define the class Net\n\nclass Net(nn.Module):\n\n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        # hidden layer \n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n        # Define the first linear layer as an attribute, this is not good practice\n        self.a1 = None\n        self.l1 = None\n        self.l2=None\n\n    # Prediction\n    def forward(self, x):\n        self.l1 = self.linear1(x)\n        self.a1 = sigmoid(self.l1)\n        self.l2=self.linear2(self.a1)\n        yhat = sigmoid(self.linear2(self.a1))\n        return yhat\n</code></pre> <p>Define the training function:</p> <pre><code># Define the training function\n\ndef train(Y, X, model, optimizer, criterion, epochs=1000):\n    cost = []\n    total=0\n    for epoch in range(epochs):\n        total=0\n        for y, x in zip(Y, X):\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            #cumulative loss \n            total+=loss.item() \n        cost.append(total)\n        if epoch % 300 == 0:    \n            PlotStuff(X, Y, model, epoch, leg=True)\n            plt.show()\n            model(X)\n            plt.scatter(model.a1.detach().numpy()[:, 0], model.a1.detach().numpy()[:, 1], c=Y.numpy().reshape(-1))\n            plt.title('activations')\n            plt.show()\n    return cost\n</code></pre> <pre><code># Make some data\n\nX = torch.arange(-20, 20, 1).view(-1, 1).type(torch.FloatTensor)\nY = torch.zeros(X.shape[0])\nY[(X[:, 0] &amp;gt; -4) &amp;amp; (X[:, 0] &amp;lt; 4)] = 1.0\n</code></pre> <p>Create the Cross-Entropy loss function: </p> <pre><code># The loss function\n\ndef criterion_cross(outputs, labels):\n    out = -1 * torch.mean(labels * torch.log(outputs) + (1 - labels) * torch.log(1 - outputs))\n    return out\n</code></pre> <p>Define the Neural Network, Optimizer, and Train the Model:</p> <pre><code># Train the model\n# size of input \nD_in = 1\n# size of hidden layer \nH = 2\n# number of outputs \nD_out = 1\n# learning rate \nlearning_rate = 0.1\n# create the model \nmodel = Net(D_in, H, D_out)\n#optimizer \noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n#train the model usein\ncost_cross = train(Y, X, model, optimizer, criterion_cross, epochs=1000)\n#plot the loss\nplt.plot(cost_cross)\nplt.xlabel('epoch')\nplt.title('cross entropy loss')\n</code></pre> <pre>\n<code>Text(0.5, 1.0, 'cross entropy loss')</code>\n</pre> <p>By examining the output of the  activation, you see by the 600th epoch that the data has been mapped to a linearly separable space.</p> <p>we can make a prediction for a arbitrary one tensors </p> <pre><code>x=torch.tensor([0.0])\nyhat=model(x)\nyhat\n</code></pre> <p>we can make a prediction for some arbitrary one tensors  </p> <pre><code>X_=torch.tensor([[0.0],[2.0],[3.0]])\nYhat=model(X_)\nYhat\n</code></pre> <p>we  can threshold the predication</p> <pre><code>Yhat=Yhat&amp;gt;0.5\nYhat\n</code></pre> <p>Repeat the previous steps above by using the MSE cost or total loss: </p> <pre><code># Practice: Train the model with MSE Loss Function\n\n# Type your code here\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.1_simple1hiddenlayer/#objective","title":"Objective","text":"<p><ul><li> How to create simple Neural Network in pytorch.</li></ul></p>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.1_simple1hiddenlayer/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will use a single-layer neural network to classify non linearly seprable data in 1-Ddatabase.</p> <ul> <li> Neural Network Module and Training Function</li> <li> Make Some Data</li> <li> Define the Neural Network, Criterion Function, Optimizer, and Train the Model</li> </ul> <p>Estimated Time Needed: 25 min</p>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.1_simple1hiddenlayer/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.1_simple1hiddenlayer/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.1_simple1hiddenlayer/#Model","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.1_simple1hiddenlayer/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.1_simple1hiddenlayer/#Train","title":"Define the Neural Network, Criterion Function, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.1_simple1hiddenlayer/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.1_simple1hiddenlayer/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.1_simple1hiddenlayer/#about-the-author","title":"About the Author:","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.2multiple_neurons/","title":"Multiple Neurons","text":"<p>author: Juma Shafara date: \"2024-08-12\" title: Softmax Classifer 1D keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: How to create complex Neural Network in pytorch.</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>We'll need to import the following libraries for this lab.</p> <pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n</code></pre> <p>Define the plotting functions.</p> <pre><code>def get_hist(model,data_set):\n    activations=model.activation(data_set.x)\n    for i,activation in enumerate(activations):\n        plt.hist(activation.numpy(),4,density=True)\n        plt.title(\"Activation layer \" + str(i+1))\n        plt.xlabel(\"Activation\")\n        plt.xlabel(\"Activation\")\n        plt.legend()\n        plt.show()\n</code></pre> <pre><code>def PlotStuff(X,Y,model=None,leg=False):\n\n    plt.plot(X[Y==0].numpy(),Y[Y==0].numpy(),'or',label='training points y=0 ' )\n    plt.plot(X[Y==1].numpy(),Y[Y==1].numpy(),'ob',label='training points y=1 ' )\n\n    if model!=None:\n        plt.plot(X.numpy(),model(X).detach().numpy(),label='neral network ')\n\n    plt.legend()\n    plt.show()\n</code></pre> <p>Define the class to get our dataset.</p> <pre><code>class Data(Dataset):\n    def __init__(self):\n        self.x=torch.linspace(-20, 20, 100).view(-1,1)\n\n        self.y=torch.zeros(self.x.shape[0])\n        self.y[(self.x[:,0]&amp;gt;-10)&amp;amp; (self.x[:,0]&amp;lt;-5)]=1\n        self.y[(self.x[:,0]&amp;gt;5)&amp;amp; (self.x[:,0]&amp;lt;10)]=1\n        self.y=self.y.view(-1,1)\n        self.len=self.x.shape[0]\n    def __getitem__(self,index):    \n\n        return self.x[index],self.y[index]\n    def __len__(self):\n        return self.len\n</code></pre> <p>Define the class for creating our model.</p> <pre><code>class Net(nn.Module):\n    def __init__(self,D_in,H,D_out):\n        super(Net,self).__init__()\n        self.linear1=nn.Linear(D_in,H)\n        self.linear2=nn.Linear(H,D_out)\n\n\n    def forward(self,x):\n        x=torch.sigmoid(self.linear1(x))  \n        x=torch.sigmoid(self.linear2(x))\n        return x\n</code></pre> <p>Create the function to train our model, which accumulate lost for each iteration to obtain the cost.</p> <pre><code>def train(data_set,model,criterion, train_loader, optimizer, epochs=5,plot_number=10):\n    cost=[]\n\n    for epoch in range(epochs):\n        total=0\n\n        for x,y in train_loader:\n            optimizer.zero_grad()\n\n            yhat=model(x)\n            loss=criterion(yhat,y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total+=loss.item()\n\n        if epoch%plot_number==0:\n            PlotStuff(data_set.x,data_set.y,model)\n\n        cost.append(total)\n    plt.figure()\n    plt.plot(cost)\n    plt.xlabel('epoch')\n    plt.ylabel('cost')\n    plt.show()\n    return cost\n</code></pre> <pre><code>data_set=Data()\n</code></pre> <pre><code>PlotStuff(data_set.x,data_set.y,leg=False)\n</code></pre> <p>Create our model with 9 neurons in the hidden layer. And then create a BCE loss and an Adam optimizer.</p> <pre><code>torch.manual_seed(0)\nmodel=Net(1,9,1)\nlearning_rate=0.1\ncriterion=nn.BCELoss()\noptimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\ntrain_loader=DataLoader(dataset=data_set,batch_size=100)\nCOST=train(data_set,model,criterion, train_loader, optimizer, epochs=600,plot_number=200)\n</code></pre>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.2multiple_neurons/#objective","title":"Objective","text":"<p><ul><li> How to create complex Neural Network in pytorch.</li></ul></p>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.2multiple_neurons/#table-of-contents","title":"Table of Contents","text":"<ul> <li> Preperation</li> <li> Get Our Data</li> <li> Define the Neural Network, Optimizer, and Train the Model</li> </ul> <p>Estimated Time Needed: 25 min</p>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.2multiple_neurons/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.2multiple_neurons/#Prep","title":"Preparation","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.2multiple_neurons/#Data","title":"Get Our Data","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.2multiple_neurons/#Train","title":"Define the Neural Network, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.2multiple_neurons/#this-is-for-exercises","title":"this is for exercises","text":"<p>model= torch.nn.Sequential(     torch.nn.Linear(1, 6),      torch.nn.Sigmoid(),     torch.nn.Linear(6,1),     torch.nn.Sigmoid()</p> <p>)</p> <pre><code>plt.plot(COST)\n</code></pre> <pre>\n<code>[&lt;matplotlib.lines.Line2D at 0x707076694800&gt;]</code>\n</pre>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.2multiple_neurons/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/","title":"XOR Problem","text":"<p>author: Juma Shafara date: \"2024-08-12\" title: Noisy XO keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will see how many neurons it takes to classify noisy XOR data with one hidden layer neural network.</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>We'll need the following libraries</p> <pre><code># Import the libraries we need for this lab\n\n\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt \nfrom matplotlib.colors import ListedColormap\nfrom torch.utils.data import Dataset, DataLoader\n</code></pre> <p>Use the following function to plot the data: </p> <pre><code># Plot the data\n\ndef plot_decision_regions_2class(model,data_set):\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#00AAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])\n    X = data_set.x.numpy()\n    y = data_set.y.numpy()\n    h = .02\n    x_min, x_max = X[:, 0].min() - 0.1 , X[:, 0].max() + 0.1 \n    y_min, y_max = X[:, 1].min() - 0.1 , X[:, 1].max() + 0.1 \n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n    XX = torch.Tensor(np.c_[xx.ravel(), yy.ravel()])\n\n    yhat = np.logical_not((model(XX)[:, 0] &amp;gt; 0.5).numpy()).reshape(xx.shape)\n    plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)\n    plt.plot(X[y[:, 0] == 0, 0], X[y[:, 0] == 0, 1], 'o', label='y=0')\n    plt.plot(X[y[:, 0] == 1, 0], X[y[:, 0] == 1, 1], 'ro', label='y=1')\n    plt.title(\"decision region\")\n    plt.legend()\n</code></pre> <p>Use the following function to calculate accuracy: </p> <pre><code># Calculate the accuracy\n\ndef accuracy(model, data_set):\n    return np.mean(data_set.y.view(-1).numpy() == (model(data_set.x)[:, 0] &amp;gt; 0.5).numpy())\n</code></pre> <p>Define the neural network module or class: </p> <pre><code># Define the class Net with one hidden layer \n\nclass Net(nn.Module):\n\n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        #hidden layer \n        self.linear1 = nn.Linear(D_in, H)\n        #output layer \n        self.linear2 = nn.Linear(H, D_out)\n\n    # Prediction    \n    def forward(self, x):\n        x = torch.sigmoid(self.linear1(x))  \n        x = torch.sigmoid(self.linear2(x))\n        return x\n</code></pre> <p>Define a function to train the model: </p> <pre><code># Define the train model\n\ndef train(data_set, model, criterion, train_loader, optimizer, epochs=5):\n    COST = []\n    ACC = []\n    for epoch in range(epochs):\n        total=0\n        for x, y in train_loader:\n            optimizer.zero_grad()\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            #cumulative loss \n            total+=loss.item()\n        ACC.append(accuracy(model, data_set))\n        COST.append(total)\n\n    fig, ax1 = plt.subplots()\n    color = 'tab:red'\n    ax1.plot(COST, color=color)\n    ax1.set_xlabel('epoch', color=color)\n    ax1.set_ylabel('total loss', color=color)\n    ax1.tick_params(axis='y', color=color)\n\n    ax2 = ax1.twinx()  \n    color = 'tab:blue'\n    ax2.set_ylabel('accuracy', color=color)  # we already handled the x-label with ax1\n    ax2.plot(ACC, color=color)\n    ax2.tick_params(axis='y', color=color)\n    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n\n    plt.show()\n\n    return COST\n</code></pre> <p>Dataset class:</p> <pre><code># Define the class XOR_Data\n\nclass XOR_Data(Dataset):\n\n    # Constructor\n    def __init__(self, N_s=100):\n        self.x = torch.zeros((N_s, 2))\n        self.y = torch.zeros((N_s, 1))\n        for i in range(N_s // 4):\n            self.x[i, :] = torch.Tensor([0.0, 0.0]) \n            self.y[i, 0] = torch.Tensor([0.0])\n\n            self.x[i + N_s // 4, :] = torch.Tensor([0.0, 1.0])\n            self.y[i + N_s // 4, 0] = torch.Tensor([1.0])\n\n            self.x[i + N_s // 2, :] = torch.Tensor([1.0, 0.0])\n            self.y[i + N_s // 2, 0] = torch.Tensor([1.0])\n\n            self.x[i + 3 * N_s // 4, :] = torch.Tensor([1.0, 1.0])\n            self.y[i + 3 * N_s // 4, 0] = torch.Tensor([0.0])\n\n            self.x = self.x + 0.01 * torch.randn((N_s, 2))\n        self.len = N_s\n\n    # Getter\n    def __getitem__(self, index):    \n        return self.x[index],self.y[index]\n\n    # Get Length\n    def __len__(self):\n        return self.len\n\n    # Plot the data\n    def plot_stuff(self):\n        plt.plot(self.x[self.y[:, 0] == 0, 0].numpy(), self.x[self.y[:, 0] == 0, 1].numpy(), 'o', label=\"y=0\")\n        plt.plot(self.x[self.y[:, 0] == 1, 0].numpy(), self.x[self.y[:, 0] == 1, 1].numpy(), 'ro', label=\"y=1\")\n        plt.legend()\n</code></pre> <p>Dataset object:</p> <pre><code># Create dataset object\n\ndata_set = XOR_Data()\ndata_set.plot_stuff()\n</code></pre> <p>Create a neural network <code>model</code> with one neuron. Then, use the following code to train it:</p> <pre><code># Practice: create a model with one neuron\n\n# Type your code here\n</code></pre> <p>Double-click here for the solution.</p> <pre><code># Train the model\n\nlearning_rate = 0.001\ncriterion = nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntrain_loader = DataLoader(dataset=data_set, batch_size=1)\nLOSS12 = train(data_set, model, criterion, train_loader, optimizer, epochs=500)\nplot_decision_regions_2class(model, data_set)\n</code></pre> <p>Create a neural network <code>model</code> with two neurons. Then, use the following code to train it:</p> <pre><code># Practice: create a model with two neuron\n\n# Type your code here\n</code></pre> <p>Double-click here for the solution.</p> <pre><code># Train the model\n\nlearning_rate = 0.1\ncriterion = nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntrain_loader = DataLoader(dataset=data_set, batch_size=1)\nLOSS12 = train(data_set, model, criterion, train_loader, optimizer, epochs=500)\nplot_decision_regions_2class(model, data_set)\n</code></pre> <p>Create a neural network <code>model</code> with three neurons. Then, use the following code to train it:</p> <pre><code># Practice: create a model with two neuron\nmodel = Net(2, 4, 1)\n# Type your code here\n</code></pre> <p>Double-click here for the solution.</p> <pre><code># Train the model\n\nlearning_rate = 0.1\ncriterion = nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntrain_loader = DataLoader(dataset=data_set, batch_size=1)\nLOSS12 = train(data_set, model, criterion, train_loader, optimizer, epochs=500)\nplot_decision_regions_2class(model, data_set)\n</code></pre>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#practice-neural-networks-with-one-hidden-layer-noisy-xor","title":"Practice: Neural Networks with One Hidden Layer: Noisy XOR","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#objective","title":"Objective","text":"<p><ul><li> How to create a neural network model with multiple neurons.</li></ul></p>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will see how many neurons it takes to classify noisy XOR data with one hidden layer neural network.</p> <ul> <li> Neural Network Module and Training Function</li> <li> Make Some Data</li> <li> One Neuron</li> <li> Two Neurons</li> <li> Three Neurons</li> </ul> <p>Estimated Time Needed: 25 min</p>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#Model","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#One","title":"One Neuron","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#try","title":"Try","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#Two","title":"Two Neurons","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#try_1","title":"Try","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#Three","title":"Three Neurons","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#try_2","title":"Try","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/","title":"MNIST","text":"<p>author: Juma Shafara date: \"2024-08-12\" title: One Hidden Layer keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will see how many neurons it takes to classify noisy XOR data with one hidden layer neural network.</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>We'll need the following libraries</p> <pre><code># Import the libraries we need for this lab\n\n# Using the following line code to install the torchvision library\n# !mamba install -y torchvision\n\nimport torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport torch.nn.functional as F\nimport matplotlib.pylab as plt\nimport numpy as np\n</code></pre> <p>Use the following helper functions for plotting the loss: </p> <pre><code># Define a function to plot accuracy and loss\n\ndef plot_accuracy_loss(training_results): \n    plt.subplot(2, 1, 1)\n    plt.plot(training_results['training_loss'], 'r')\n    plt.ylabel('loss')\n    plt.title('training loss iterations')\n    plt.subplot(2, 1, 2)\n    plt.plot(training_results['validation_accuracy'])\n    plt.ylabel('accuracy')\n    plt.xlabel('epochs')   \n    plt.show()\n</code></pre> <p>Use the following function for printing the model parameters: </p> <pre><code># Define a function to plot model parameters\n\ndef print_model_parameters(model):\n    count = 0\n    for ele in model.state_dict():\n        count += 1\n        if count % 2 != 0:\n            print (\"The following are the parameters for the layer \", count // 2 + 1)\n        if ele.find(\"bias\") != -1:\n            print(\"The size of bias: \", model.state_dict()[ele].size())\n        else:\n            print(\"The size of weights: \", model.state_dict()[ele].size())\n</code></pre> <p>Define the neural network module or class: </p> <pre><code># Define a function to display data\n\ndef show_data(data_sample):\n    plt.imshow(data_sample.numpy().reshape(28, 28), cmap='gray')\n    plt.show()\n</code></pre> <p>Define the neural network module or class: </p> <pre><code># Define a Neural Network class\n\nclass Net(nn.Module):\n\n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n\n    # Prediction    \n    def forward(self, x):\n        x = torch.sigmoid(self.linear1(x))  \n        x = self.linear2(x)\n        return x\n</code></pre> <p>Define a function to train the model. In this case, the function returns a Python dictionary to store the training loss and accuracy on the validation data. </p> <pre><code># Define a training function to train the model\n\ndef train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n    i = 0\n    useful_stuff = {'training_loss': [],'validation_accuracy': []}  \n    for epoch in range(epochs):\n        for i, (x, y) in enumerate(train_loader): \n            optimizer.zero_grad()\n            z = model(x.view(-1, 28 * 28))\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n             #loss for every iteration\n            useful_stuff['training_loss'].append(loss.data.item())\n        correct = 0\n        for x, y in validation_loader:\n            #validation \n            z = model(x.view(-1, 28 * 28))\n            _, label = torch.max(z, 1)\n            correct += (label == y).sum().item()\n        accuracy = 100 * (correct / len(validation_dataset))\n        useful_stuff['validation_accuracy'].append(accuracy)\n    return useful_stuff\n</code></pre> <p>Load the training dataset by setting the parameters <code>train</code> to <code>True</code> and convert it to a tensor by placing a transform object in the argument <code>transform</code>.</p> <pre><code># Create training dataset\n\ntrain_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n</code></pre> <pre>\n<code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n</code>\n</pre> <pre>\n<code>100.0%\n</code>\n</pre> <pre>\n<code>Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n</code>\n</pre> <pre>\n<code>100.0%\n</code>\n</pre> <pre>\n<code>Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n</code>\n</pre> <pre>\n<code>100.0%\n</code>\n</pre> <pre>\n<code>Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n</code>\n</pre> <pre>\n<code>100.0%</code>\n</pre> <pre>\n<code>Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n</code>\n</pre> <pre>\n<code>\n</code>\n</pre> <p>Load the testing dataset and convert it to a tensor by placing a transform object in the argument <code>transform</code>:</p> <pre><code># Create validating dataset\n\nvalidation_dataset = dsets.MNIST(root='./data', download=False, transform=transforms.ToTensor())\n</code></pre> <p>Create the criterion function:  </p> <pre><code># Create criterion function\n\ncriterion = nn.CrossEntropyLoss()\n</code></pre> <p>Create the training-data loader and the validation-data loader objects: </p> <pre><code># Create data loader for both train dataset and valdiate dataset\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)\n</code></pre> <p>Create the model with 100 neurons: </p> <pre><code># Create the model with 100 neurons\n\ninput_dim = 28 * 28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = Net(input_dim, hidden_dim, output_dim)\n</code></pre> <p>Print the model parameters: </p> <pre><code># Print the parameters for model\n\nprint_model_parameters(model)\n</code></pre> <pre>\n<code>The following are the parameters for the layer  1\nThe size of weights:  torch.Size([100, 784])\nThe size of bias:  torch.Size([100])\nThe following are the parameters for the layer  2\nThe size of weights:  torch.Size([10, 100])\nThe size of bias:  torch.Size([10])\n</code>\n</pre> <p>Define the optimizer object with a learning rate of 0.01: </p> <pre><code># Set the learning rate and the optimizer\n\nlearning_rate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n</code></pre> <p>Train the model by using 100 epochs (this process takes time): </p> <pre><code># Train the model\n\ntraining_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=30)\n</code></pre> <p>Plot the training total loss or cost for every iteration and plot the training accuracy for every epoch:  </p> <pre><code># Plot the accuracy and loss\n\nplot_accuracy_loss(training_results)\n</code></pre> <p>Plot the first five misclassified samples:   </p> <pre><code># Plot the first five misclassified samples\n\ncount = 0\nfor x, y in validation_dataset:\n    z = model(x.reshape(-1, 28 * 28))\n    _,yhat = torch.max(z, 1)\n    if yhat != y:\n        show_data(x)\n        count += 1\n    if count &amp;gt;= 5:\n        break\n</code></pre> <p>Use <code>nn.Sequential</code> to build exactly the same model as you just built. Use the function train to train the model and use the function <code>plot_accuracy_loss</code> to see the metrics. Also, try different epoch numbers. </p> <pre><code># Practice: Use nn.Sequential to build the same model. Use plot_accuracy_loss to print out the accuarcy and loss\n\n# Type your code here\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/#neural-networks-with-one-hidden-layer","title":"Neural Networks with One Hidden Layer","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/#objective","title":"Objective","text":"<p><ul><li> How to classify handwritten digits using Neural Network.</li></ul></p>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will use a single layer neural network to classify handwritten digits from the MNIST database.</p> <ul> <li>Neural Network Module and Training Function</li> <li>Make Some Data</li> <li>Define the Neural Network, Optimizer, and Train the  Model</li> <li>Analyze Results</li> </ul> <p>Estimated Time Needed: 25 min</p>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/#Model","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/#Train","title":"Define the Neural Network, Optimizer, and Train the Model","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/#Result","title":"Analyze Results","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.1activationfuction_v2/","title":"Activation Functions","text":"<p>author: Juma Shafara date: \"2024-08-12\" title: Activation Functions keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: How to apply different Activation functions in Neural Network.</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>We'll need the following libraries</p> <pre><code># Import the libraries we need for this lab\n\nimport torch.nn as nn\nimport torch\n\nimport matplotlib.pyplot as plt\ntorch.manual_seed(2)\n</code></pre> <pre>\n<code>&lt;torch._C.Generator at 0x7759bb532ff0&gt;</code>\n</pre> <p>Create a tensor ranging from -10 to 10: </p> <pre><code># Create a tensor\n\nz = torch.arange(-10, 10, 0.1,).view(-1, 1)\n</code></pre> <p>When you use sequential, you can create a sigmoid object: </p> <pre><code># Create a sigmoid object\n\nsig = nn.Sigmoid()\n</code></pre> <p>Apply the element-wise function Sigmoid with the object:</p> <pre><code># Make a prediction of sigmoid function\n\nyhat = sig(z)\n</code></pre> <p>Plot the results: </p> <pre><code># Plot the result\n\nplt.plot(z.detach().numpy(),yhat.detach().numpy())\nplt.xlabel('z')\nplt.ylabel('yhat')\n</code></pre> <pre>\n<code>Text(0, 0.5, 'yhat')</code>\n</pre> <p>For custom modules, call the sigmoid from the torch (<code>nn.functional</code> for the old version), which applies the element-wise sigmoid from the function module and plots the results:</p> <pre><code># Use the build in function to predict the result\n\nyhat = torch.sigmoid(z)\nplt.plot(z.numpy(), yhat.numpy())\n\nplt.show()\n</code></pre> <p>When you use sequential, you can create a tanh object:</p> <pre><code># Create a tanh object\n\nTANH = nn.Tanh()\n</code></pre> <p>Call the object and plot it:</p> <pre><code># Make the prediction using tanh object\n\nyhat = TANH(z)\nplt.plot(z.numpy(), yhat.numpy())\nplt.show()\n</code></pre> <p>For custom modules, call the Tanh object from the torch (nn.functional for the old version), which applies the element-wise sigmoid from the function module and plots the results:</p> <pre><code># Make the prediction using the build-in tanh object\n\nyhat = torch.tanh(z)\nplt.plot(z.numpy(), yhat.numpy())\nplt.show()\n</code></pre> <p>When you use sequential, you can create a Relu object: </p> <pre><code># Create a relu object and make the prediction\n\nRELU = nn.ReLU()\nyhat = RELU(z)\nplt.plot(z.numpy(), yhat.numpy())\n</code></pre> <pre>\n<code>[&lt;matplotlib.lines.Line2D at 0x77595cfe2c60&gt;]</code>\n</pre> <p>For custom modules, call the relu object from the nn.functional, which applies the element-wise sigmoid from the function module and plots the results:</p> <pre><code># Use the build-in function to make the prediction\n\nyhat = torch.relu(z)\nplt.plot(z.numpy(), yhat.numpy())\nplt.show()\n</code></pre> <p></p> <pre><code># Plot the results to compare the activation functions\n\nx = torch.arange(-2, 2, 0.1).view(-1, 1)\nplt.plot(x.numpy(), torch.relu(x).numpy(), label='relu')\nplt.plot(x.numpy(), torch.sigmoid(x).numpy(), label='sigmoid')\nplt.plot(x.numpy(), torch.tanh(x).numpy(), label='tanh')\nplt.legend()\n</code></pre> <pre>\n<code>&lt;matplotlib.legend.Legend at 0x77595f989340&gt;</code>\n</pre> <p></p> <p>Compare the activation functions with a tensor in the range (-1, 1)</p> <pre><code># Practice: Compare the activation functions again using a tensor in the range (-1, 1)\n\n# Type your code here\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.1activationfuction_v2/#objective","title":"Objective","text":"<p><ul><li> How to apply different Activation functions in Neural Network.</li></ul></p>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.1activationfuction_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will cover logistic regression by using PyTorch.</p> <ul> <li> Logistic Function</li> <li> Tanh</li> <li> Relu</li> <li> Compare Activation Functions</li> </ul> <p>Estimated Time Needed: 15 min</p>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.1activationfuction_v2/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.1activationfuction_v2/#Log","title":"Logistic Function","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.1activationfuction_v2/#Tanh","title":"Tanh","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.1activationfuction_v2/#Relu","title":"Relu","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.1activationfuction_v2/#compare-activation-functions","title":"Compare Activation Functions","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.1activationfuction_v2/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.1activationfuction_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/","title":"MNIST One Layer","text":"<p>author: Juma Shafara date: \"2024-08-12\" title: Test Activation Functions on MNIST keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will test sigmoid, tanh, and relu activation functions on the MNIST dataset.</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>We'll need the following libraries</p> <pre><code># Uncomment the following line to install the torchvision library\n# !mamba install -y torchvision\n\n# Import the libraries we need for this lab\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\nimport matplotlib.pylab as plt\nimport numpy as np\n</code></pre> <p>Define the neural network module or class using the sigmoid activation function: </p> <pre><code># Build the model with sigmoid function\n\nclass Net(nn.Module):\n\n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n\n    # Prediction\n    def forward(self, x):\n        x = torch.sigmoid(self.linear1(x))  \n        x = self.linear2(x)\n        return x\n</code></pre> <p>Define the neural network module or class using the Tanh activation function:</p> <pre><code># Build the model with Tanh function\n\nclass NetTanh(nn.Module):\n\n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(NetTanh, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n\n    # Prediction\n    def forward(self, x):\n        x = torch.tanh(self.linear1(x))\n        x = self.linear2(x)\n        return x\n</code></pre> <p>Define the neural network module or class using the Relu activation function:</p> <pre><code># Build the model with Relu function\n\nclass NetRelu(nn.Module):\n\n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(NetRelu, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n\n    # Prediction\n    def forward(self, x):\n        x = torch.relu(self.linear1(x))\n        x = self.linear2(x)\n        return x\n</code></pre> <p>Define a function to train the model. In this case, the function returns a Python dictionary to store the training loss for each iteration  and accuracy on the validation data.</p> <pre><code># Define the function for training the model\n\ndef train(model, criterion, train_loader, validation_loader, optimizer, epochs = 100):\n    i = 0\n    useful_stuff = {'training_loss':[], 'validation_accuracy':[]}  \n\n    for epoch in range(epochs):\n        for i, (x, y) in enumerate(train_loader):\n            optimizer.zero_grad()\n            z = model(x.view(-1, 28 * 28))\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n            useful_stuff['training_loss'].append(loss.item())\n\n        correct = 0\n        for x, y in validation_loader:\n            z = model(x.view(-1, 28 * 28))\n            _, label=torch.max(z, 1)\n            correct += (label == y).sum().item()\n        accuracy = 100 * (correct / len(validation_dataset))\n        useful_stuff['validation_accuracy'].append(accuracy)\n\n    return useful_stuff\n</code></pre> <p>Load the training dataset by setting the parameters <code>train</code> to <code>True</code> and convert it to a tensor by placing a transform object in the argument <code>transform</code>.</p> <pre><code># Create the training dataset\n\ntrain_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n</code></pre> <p>Load the testing dataset by setting the parameter <code>train</code> to <code>False</code> and convert it to a tensor by placing a transform object in the argument <code>transform</code>.</p> <pre><code># Create the validation  dataset\n\nvalidation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n</code></pre> <p>Create the criterion function:  </p> <pre><code># Create the criterion function\n\ncriterion = nn.CrossEntropyLoss()\n</code></pre> <p>Create the training-data loader and the validation-data loader object:</p> <pre><code># Create the training data loader and validation data loader object\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)\n</code></pre> <p>Create the criterion function: </p> <pre><code># Create the criterion function\n\ncriterion = nn.CrossEntropyLoss()\n</code></pre> <p>Create the model with 100 hidden neurons:  </p> <pre><code># Create the model object\n\ninput_dim = 28 * 28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = Net(input_dim, hidden_dim, output_dim)\n</code></pre> <p>Train the network by using the sigmoid activations function:</p> <pre><code># Train a model with sigmoid function\n\nlearning_rate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntraining_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=30)\n</code></pre> <p>Train the network by using the Tanh activations function:</p> <pre><code># Train a model with Tanh function\n\nmodel_Tanh = NetTanh(input_dim, hidden_dim, output_dim)\noptimizer = torch.optim.SGD(model_Tanh.parameters(), lr=learning_rate)\ntraining_results_tanch = train(model_Tanh, criterion, train_loader, validation_loader, optimizer, epochs=30)\n</code></pre> <p>Train the network by using the Relu activations function:</p> <pre><code># Train a model with Relu function\n\nmodelRelu = NetRelu(input_dim, hidden_dim, output_dim)\noptimizer = torch.optim.SGD(modelRelu.parameters(), lr=learning_rate)\ntraining_results_relu = train(modelRelu, criterion, train_loader, validation_loader, optimizer, epochs=30)\n</code></pre> <p>Compare the training loss for each activation: </p> <pre><code># Compare the training loss\n\nplt.plot(training_results_tanch['training_loss'], label='tanh')\nplt.plot(training_results['training_loss'], label='sigmoid')\nplt.plot(training_results_relu['training_loss'], label='relu')\nplt.ylabel('loss')\nplt.title('training loss iterations')\nplt.legend()\nplt.show()\n</code></pre> <p>Compare the validation loss for each model:  </p> <pre><code># Compare the validation loss\n\nplt.plot(training_results_tanch['validation_accuracy'], label='tanh')\nplt.plot(training_results['validation_accuracy'], label='sigmoid')\nplt.plot(training_results_relu['validation_accuracy'], label='relu') \nplt.ylabel('validation accuracy')\nplt.xlabel('epochs ')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#test-sigmoid-tanh-and-relu-activations-functions-on-the-mnist-dataset","title":"Test Sigmoid, Tanh, and Relu Activations Functions on the MNIST Dataset","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#objective","title":"Objective","text":"<p><ul><li> How to apply different activation functions on the MNIST dataset.</li></ul></p>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will test sigmoid, tanh, and relu activation functions on the MNIST dataset.</p> <ul> <li>Neural Network Module and Training Function</li> <li>Make Some Data</li> <li>Define Several Neural Network, Criterion Function, and Optimizer</li> <li>Test Sigmoid, Tanh, and Relu</li> <li>Analyze Results</li> </ul> <p></p> <p>Estimated Time Needed: 25 min</p>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#Model","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#Train","title":"Define the Neural Network, Criterion Function, Optimizer, and Train the Model","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#Test","title":"Test Sigmoid, Tanh, and Relu","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#Result","title":"Analyze Results","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#which-activation-function-performed-best","title":"Which activation function performed best ?","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/","title":"Two-Layer MNIST","text":"<p>author: Juma Shafara date: \"2024-08-08\" title: Multiple Linear Regression keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will review how to make a prediction in several different ways by using PyTorch.</p> <p></p> <p>We'll need the following libraries</p> <pre><code># Import the libraries we need for this lab\n\n# Using the following line code to install the torchvision library\n# !mamba install -y torchvision\n\n!pip install torchvision==0.9.1 torch==1.8.1 \nimport torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport torch.nn.functional as F\nimport matplotlib.pylab as plt\nimport numpy as np\ntorch.manual_seed(2)\n</code></pre> <p>Define the neural network module or class, with two hidden Layers </p> <p></p> <pre><code># Create the model class using sigmoid as the activation function\n\nclass Net(nn.Module):\n\n    # Constructor\n    def __init__(self, D_in, H1, H2, D_out):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(D_in, H1)\n        self.linear2 = nn.Linear(H1, H2)\n        self.linear3 = nn.Linear(H2, D_out)\n\n    # Prediction\n    def forward(self,x):\n        x = torch.sigmoid(self.linear1(x)) \n        x = torch.sigmoid(self.linear2(x))\n        x = self.linear3(x)\n        return x\n</code></pre> <p>Define the class with the Tanh activation function </p> <pre><code># Create the model class using Tanh as a activation function\n\nclass NetTanh(nn.Module):\n\n    # Constructor\n    def __init__(self, D_in, H1, H2, D_out):\n        super(NetTanh, self).__init__()\n        self.linear1 = nn.Linear(D_in, H1)\n        self.linear2 = nn.Linear(H1, H2)\n        self.linear3 = nn.Linear(H2, D_out)\n\n    # Prediction\n    def forward(self, x):\n        x = torch.tanh(self.linear1(x))\n        x = torch.tanh(self.linear2(x))\n        x = self.linear3(x)\n        return x\n</code></pre> <p>Define the class for the Relu activation function </p> <pre><code># Create the model class using Relu as a activation function\n\nclass NetRelu(nn.Module):\n\n    # Constructor\n    def __init__(self, D_in, H1, H2, D_out):\n        super(NetRelu, self).__init__()\n        self.linear1 = nn.Linear(D_in, H1)\n        self.linear2 = nn.Linear(H1, H2)\n        self.linear3 = nn.Linear(H2, D_out)\n\n    # Prediction\n    def forward(self, x):\n        x = torch.relu(self.linear1(x))  \n        x = torch.relu(self.linear2(x))\n        x = self.linear3(x)\n        return x\n</code></pre> <p>Define a function to  train the model, in this case the function returns a Python dictionary to store the training loss and accuracy on the validation data </p> <pre><code># Train the model\n\ndef train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n    i = 0\n    useful_stuff = {'training_loss': [], 'validation_accuracy': []}  \n\n    for epoch in range(epochs):\n        for i, (x, y) in enumerate(train_loader):\n            optimizer.zero_grad()\n            z = model(x.view(-1, 28 * 28))\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n            useful_stuff['training_loss'].append(loss.data.item())\n\n        correct = 0\n        for x, y in validation_loader:\n            z = model(x.view(-1, 28 * 28))\n            _, label = torch.max(z, 1)\n            correct += (label == y).sum().item()\n\n        accuracy = 100 * (correct / len(validation_dataset))\n        useful_stuff['validation_accuracy'].append(accuracy)\n\n    return useful_stuff\n</code></pre> <p>Load the training dataset by setting the parameters <code>train</code> to <code>True</code> and convert it to a tensor  by placing a transform object int the argument <code>transform</code></p> <pre><code># Create the training dataset\n\ntrain_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n</code></pre> <p>Load the testing dataset by setting the parameters <code>train</code> to <code>False</code> and convert it to a tensor  by placing a transform object int the argument <code>transform</code></p> <pre><code># Create the validating dataset\n\nvalidation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n</code></pre> <p>Create the criterion function  </p> <pre><code># Create the criterion function\n\ncriterion = nn.CrossEntropyLoss()\n</code></pre> <p>Create the training-data loader and the validation-data loader object </p> <pre><code># Create the training data loader and validation data loader object\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)\n</code></pre> <p>Create  the model with 100 hidden layers  </p> <pre><code># Set the parameters for create the model\n\ninput_dim = 28 * 28\nhidden_dim1 = 50\nhidden_dim2 = 50\noutput_dim = 10\n</code></pre> <p>The epoch number in the video is 35. You can try 10 for now. If you try 35, it may take a long time.</p> <pre><code># Set the number of iterations\n\ncust_epochs = 10\n</code></pre> <p>Train the network using the Sigmoid activation function</p> <pre><code># Train the model with sigmoid function\n\nlearning_rate = 0.01\nmodel = Net(input_dim, hidden_dim1, hidden_dim2, output_dim)\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntraining_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=cust_epochs)\n</code></pre> <p>Train the network using the Tanh activation function</p> <pre><code># Train the model with tanh function\n\nlearning_rate = 0.01\nmodel_Tanh = NetTanh(input_dim, hidden_dim1, hidden_dim2, output_dim)\noptimizer = torch.optim.SGD(model_Tanh.parameters(), lr=learning_rate)\ntraining_results_tanch = train(model_Tanh, criterion, train_loader, validation_loader, optimizer, epochs=cust_epochs)\n</code></pre> <p>Train the network using the Relu activation function</p> <pre><code># Train the model with relu function\n\nlearning_rate = 0.01\nmodelRelu = NetRelu(input_dim, hidden_dim1, hidden_dim2, output_dim)\noptimizer = torch.optim.SGD(modelRelu.parameters(), lr=learning_rate)\ntraining_results_relu = train(modelRelu, criterion, train_loader, validation_loader, optimizer, epochs=cust_epochs)\n</code></pre> <p>Compare the training loss for each activation </p> <pre><code># Compare the training loss\n\nplt.plot(training_results_tanch['training_loss'], label='tanh')\nplt.plot(training_results['training_loss'], label='sigmoid')\nplt.plot(training_results_relu['training_loss'], label='relu')\nplt.ylabel('loss')\nplt.title('training loss iterations')\nplt.legend()\n</code></pre> <p>Compare the validation loss for each model  </p> <pre><code># Compare the validation loss\n\nplt.plot(training_results_tanch['validation_accuracy'], label = 'tanh')\nplt.plot(training_results['validation_accuracy'], label = 'sigmoid')\nplt.plot(training_results_relu['validation_accuracy'], label = 'relu') \nplt.ylabel('validation accuracy')\nplt.xlabel('Iteration')   \nplt.legend()\n</code></pre>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/#hidden-layer-deep-network-sigmoid-tanh-and-relu-activations-functions-mnist-dataset","title":"Hidden Layer Deep Network: Sigmoid, Tanh and Relu Activations Functions MNIST Dataset","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/#objective-for-this-notebook-1-define-several-neural-network-criterion-function-optimizer-2-test-sigmoid-tanh-and-relu-3-analyse-results","title":"Objective for this Notebook","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/#1-define-several-neural-network-criterion-function-optimizer","title":"1. Define Several Neural Network, Criterion function, Optimizer.","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/#2-test-sigmoid-tanh-and-relu","title":"2. Test Sigmoid ,Tanh and Relu.","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/#3-analyse-results","title":"3. Analyse Results.","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will test Sigmoid, Tanh and Relu activation functions on the MNIST dataset with two hidden Layers.</p> <ul> <li>Neural Network Module and Training Function</li> <li>Make Some Data</li> <li>Define Several Neural Network, Criterion function, Optimizer</li> <li>Test Sigmoid ,Tanh and Relu </li> <li>Analyse Results</li> </ul> <p>Estimated Time Needed: 25 min</p>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/#Model","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/#Train","title":"Define Neural Network, Criterion function, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/#Test","title":"Test Sigmoid ,Tanh and Relu","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/#Result","title":"Analyze Results","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.2mulitclassspiralrulu_v2/","title":"Multiclass Spiral","text":"<p>We'll need the following libraries</p> <pre><code># Import the libraries we need for this lab\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom matplotlib.colors import ListedColormap\nfrom torch.utils.data import Dataset, DataLoader\n\ntorch.manual_seed(1)\n</code></pre> <pre>\n<code>&lt;torch._C.Generator at 0x11c408610&gt;</code>\n</pre> <p>Function used to plot:</p> <pre><code># Define the function to plot the diagram\n\ndef plot_decision_regions_3class(model, data_set):\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#00AAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])\n    X = data_set.x.numpy()\n    y = data_set.y.numpy()\n    h = .02\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 \n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 \n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    XX = torch.Tensor(np.c_[xx.ravel(), yy.ravel()])\n    _, yhat = torch.max(model(XX), 1)\n    yhat = yhat.numpy().reshape(xx.shape)\n    plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)\n    plt.plot(X[y[:] == 0, 0], X[y[:] == 0, 1], 'ro', label = 'y=0')\n    plt.plot(X[y[:] == 1, 0], X[y[:] == 1, 1], 'go', label = 'y=1')\n    plt.plot(X[y[:] == 2, 0], X[y[:] == 2, 1], 'o', label = 'y=2')\n    plt.title(\"decision region\")\n    plt.legend()\n</code></pre> <p>Create Dataset <code>Class</code></p> <pre><code># Create Data Class\n\nclass Data(Dataset):\n\n    #  modified from: http://cs231n.github.io/neural-networks-case-study/\n    # Constructor\n    def __init__(self, K=3, N=500):\n        D = 2\n        X = np.zeros((N * K, D)) # data matrix (each row = single example)\n        y = np.zeros(N * K, dtype='uint8') # class labels\n        for j in range(K):\n          ix = range(N * j, N * (j + 1))\n          r = np.linspace(0.0, 1, N) # radius\n          t = np.linspace(j * 4, (j + 1) * 4, N) + np.random.randn(N) * 0.2 # theta\n          X[ix] = np.c_[r * np.sin(t), r*np.cos(t)]\n          y[ix] = j\n        self.y = torch.from_numpy(y).type(torch.LongTensor)\n        self.x = torch.from_numpy(X).type(torch.FloatTensor)\n        self.len = y.shape[0]\n\n    # Getter\n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]\n\n    # Get Length\n    def __len__(self):\n        return self.len\n\n    # Plot the diagram\n    def plot_stuff(self):\n        plt.plot(self.x[self.y[:] == 0, 0].numpy(), self.x[self.y[:] == 0, 1].numpy(), 'o', label=\"y = 0\")\n        plt.plot(self.x[self.y[:] == 1, 0].numpy(), self.x[self.y[:] == 1, 1].numpy(), 'ro', label=\"y = 1\")\n        plt.plot(self.x[self.y[:] == 2, 0].numpy(), self.x[self.y[:] == 2, 1].numpy(), 'go', label=\"y = 2\")\n        plt.legend()\n</code></pre> <p>Neural Network Module using <code>ModuleList()</code></p> <pre><code># Create Net model class\n\nclass Net(nn.Module):\n\n    # Constructor\n    def __init__(self, Layers):\n        super(Net, self).__init__()\n        self.hidden = nn.ModuleList()\n        for input_size, output_size in zip(Layers, Layers[1:]):\n            self.hidden.append(nn.Linear(input_size, output_size))\n\n    # Prediction\n    def forward(self, activation):\n        L = len(self.hidden)\n        for (l, linear_transform) in zip(range(L), self.hidden):\n            if l &amp;lt; L - 1:\n                activation = F.relu(linear_transform(activation))\n            else:\n                activation = linear_transform(activation)\n        return activation\n</code></pre> <p>A function used to train. </p> <pre><code># Define the function for training the model\n\ndef train(data_set, model, criterion, train_loader, optimizer, epochs=100):\n    LOSS = []\n    ACC = []\n    for epoch in range(epochs):\n        for x, y in train_loader:\n            optimizer.zero_grad()\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            LOSS.append(loss.item())\n        ACC.append(accuracy(model, data_set))\n\n    fig, ax1 = plt.subplots()\n    color = 'tab:red'\n    ax1.plot(LOSS, color = color)\n    ax1.set_xlabel('Iteration', color = color)\n    ax1.set_ylabel('total loss', color = color)\n    ax1.tick_params(axis = 'y', color = color)\n\n    ax2 = ax1.twinx()  \n    color = 'tab:blue'\n    ax2.set_ylabel('accuracy', color = color)  # we already handled the x-label with ax1\n    ax2.plot(ACC, color = color)\n    ax2.tick_params(axis = 'y', color = color)\n    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n\n    plt.show()\n    return LOSS\n</code></pre> <p>A function used to calculate accuracy </p> <pre><code># The function to calculate the accuracy\n\ndef accuracy(model, data_set):\n    _, yhat = torch.max(model(data_set.x), 1)\n    return (yhat == data_set.y).numpy().mean()\n</code></pre> <p>Crate a dataset object:</p> <pre><code># Create a Dataset object\n\ndata_set = Data()\ndata_set.plot_stuff()\ndata_set.y = data_set.y.view(-1)\n</code></pre> <p>Create a  network to classify three classes with 1 hidden layer with 50 neurons </p> <pre><code># Train the model with 1 hidden layer with 50 neurons\n\nLayers = [2, 50, 3]\nmodel = Net(Layers)\nlearning_rate = 0.10\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntrain_loader = DataLoader(dataset=data_set, batch_size=20)\ncriterion = nn.CrossEntropyLoss()\nLOSS = train(data_set, model, criterion, train_loader, optimizer, epochs=100)\n\nplot_decision_regions_3class(model, data_set)\n</code></pre> <p>Create a  network to classify three classes with 2 hidden layers with 20 neurons in total </p> <pre><code>Net([3,3,4,3]).parameters\n</code></pre> <pre>\n<code>&lt;bound method Module.parameters of Net(\n  (hidden): ModuleList(\n    (0): Linear(in_features=3, out_features=3, bias=True)\n    (1): Linear(in_features=3, out_features=4, bias=True)\n    (2): Linear(in_features=4, out_features=3, bias=True)\n  )\n)&gt;</code>\n</pre> <pre><code># Train the model with 2 hidden layers with 20 neurons\n\nLayers = [2, 10, 10, 3]\nmodel = Net(Layers)\nlearning_rate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntrain_loader = DataLoader(dataset=data_set, batch_size=20)\ncriterion = nn.CrossEntropyLoss()\nLOSS = train(data_set, model, criterion, train_loader, optimizer, epochs=1000)\n\nplot_decision_regions_3class(model, data_set)\n</code></pre> <p>Create a network with three hidden layers each with ten neurons, then train the network using the same process as above </p> <pre><code># Practice: Create a network with three hidden layers each with ten neurons.\n\n# Type your code here\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.2mulitclassspiralrulu_v2/#deeper-neural-networks-with-nnmodulelist","title":"Deeper Neural Networks with nn.ModuleList()","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.2mulitclassspiralrulu_v2/#objective-for-this-notebook-1-create-a-deeper-neural-network-with-nnmodulelist-2-train-and-validate-the-model","title":"Objective for this Notebook","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.2mulitclassspiralrulu_v2/#1-create-a-deeper-neural-network-with-nnmodulelist","title":"1. Create a Deeper Neural Network with <code>nn.ModuleList()</code>","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.2mulitclassspiralrulu_v2/#2-train-and-validate-the-model","title":"2. Train and Validate the Model.","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.2mulitclassspiralrulu_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will create a Deeper Neural Network with <code>nn.ModuleList()</code></p> <ul> <li>Neural Network Module and Function for Training</li> <li>Train and Validate the Model</li> </ul> <p>Estimated Time Needed: 25 min</p>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.2mulitclassspiralrulu_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.2mulitclassspiralrulu_v2/#Model","title":"Neural Network Module and Function for Training","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.2mulitclassspiralrulu_v2/#Train","title":"Train and Validate the Model","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.2mulitclassspiralrulu_v2/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.2mulitclassspiralrulu_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.1dropoutPredictin_v2/","title":"Dropout Prediction","text":"<p>We'll need the following libraries</p> <pre><code># Import the libraries we need for this lab\n\nimport torch\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom matplotlib.colors import ListedColormap\nfrom torch.utils.data import Dataset, DataLoader\n</code></pre> <p>Use this function only for plotting:</p> <pre><code># The function for plotting the diagram\n\ndef plot_decision_regions_3class(data_set, model=None):\n    cmap_light = ListedColormap([ '#0000FF','#FF0000'])\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])\n    X = data_set.x.numpy()\n    y = data_set.y.numpy()\n    h = .02\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 \n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 \n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    newdata = np.c_[xx.ravel(), yy.ravel()]\n\n    Z = data_set.multi_dim_poly(newdata).flatten()\n    f = np.zeros(Z.shape)\n    f[Z &amp;gt; 0] = 1\n    f = f.reshape(xx.shape)\n    if model != None:\n        model.eval()\n        XX = torch.Tensor(newdata)\n        _, yhat = torch.max(model(XX), 1)\n        yhat = yhat.numpy().reshape(xx.shape)\n        plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)\n        plt.contour(xx, yy, f, cmap=plt.cm.Paired)\n    else:\n        plt.contour(xx, yy, f, cmap=plt.cm.Paired)\n        plt.pcolormesh(xx, yy, f, cmap=cmap_light) \n\n    plt.title(\"decision region vs True decision boundary\")\n</code></pre> <p>Use this function to calculate accuracy: </p> <pre><code># The function for calculating accuracy\n\ndef accuracy(model, data_set):\n    _, yhat = torch.max(model(data_set.x), 1)\n    return (yhat == data_set.y).numpy().mean()\n</code></pre> <p>Create a nonlinearly separable dataset:    </p> <pre><code># Create data class for creating dataset object\n\nclass Data(Dataset):\n\n    # Constructor\n    def __init__(self, N_SAMPLES=1000, noise_std=0.15, train=True):\n        a = np.matrix([-1, 1, 2, 1, 1, -3, 1]).T\n        self.x = np.matrix(np.random.rand(N_SAMPLES, 2))\n        self.f = np.array(a[0] + (self.x) * a[1:3] + np.multiply(self.x[:, 0], self.x[:, 1]) * a[4] + np.multiply(self.x, self.x) * a[5:7]).flatten()\n        self.a = a\n\n        self.y = np.zeros(N_SAMPLES)\n        self.y[self.f &amp;gt; 0] = 1\n        self.y = torch.from_numpy(self.y).type(torch.LongTensor)\n        self.x = torch.from_numpy(self.x).type(torch.FloatTensor)\n        self.x = self.x + noise_std * torch.randn(self.x.size())\n        self.f = torch.from_numpy(self.f)\n        self.a = a\n        if train == True:\n            torch.manual_seed(1)\n            self.x = self.x + noise_std * torch.randn(self.x.size())\n            torch.manual_seed(0)\n\n    # Getter        \n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]\n\n    # Get Length\n    def __len__(self):\n        return self.len\n\n    # Plot the diagram\n    def plot(self):\n        X = data_set.x.numpy()\n        y = data_set.y.numpy()\n        h = .02\n        x_min, x_max = X[:, 0].min(), X[:, 0].max()\n        y_min, y_max = X[:, 1].min(), X[:, 1].max() \n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n        Z = data_set.multi_dim_poly(np.c_[xx.ravel(), yy.ravel()]).flatten()\n        f = np.zeros(Z.shape)\n        f[Z &amp;gt; 0] = 1\n        f = f.reshape(xx.shape)\n\n        plt.title('True decision boundary  and sample points with noise ')\n        plt.plot(self.x[self.y == 0, 0].numpy(), self.x[self.y == 0,1].numpy(), 'bo', label='y=0') \n        plt.plot(self.x[self.y == 1, 0].numpy(), self.x[self.y == 1,1].numpy(), 'ro', label='y=1')\n        plt.contour(xx, yy, f,cmap=plt.cm.Paired)\n        plt.xlim(0,1)\n        plt.ylim(0,1)\n        plt.legend()\n\n    # Make a multidimension ploynomial function\n    def multi_dim_poly(self, x):\n        x = np.matrix(x)\n        out = np.array(self.a[0] + (x) * self.a[1:3] + np.multiply(x[:, 0], x[:, 1]) * self.a[4] + np.multiply(x, x) * self.a[5:7])\n        out = np.array(out)\n        return out\n</code></pre> <p>Create a dataset object:</p> <pre><code># Create a dataset object\n\ndata_set = Data(noise_std=0.2)\ndata_set.plot()\n</code></pre> <p>Validation data: </p> <pre><code># Get some validation data\n\ntorch.manual_seed(0) \nvalidation_set = Data(train=False)\n</code></pre> <p>Create a custom module with three layers. <code>in_size</code> is the size of the input features, <code>n_hidden</code> is the size of the layers, and <code>out_size</code> is the size. <code>p</code> is the dropout probability. The default is 0, that is, no dropout.</p> <pre><code># Create Net Class\n\nclass Net(nn.Module):\n\n    # Constructor\n    def __init__(self, in_size, n_hidden, out_size, p=0):\n        super(Net, self).__init__()\n        self.drop = nn.Dropout(p=p)\n        self.linear1 = nn.Linear(in_size, n_hidden)\n        self.linear2 = nn.Linear(n_hidden, n_hidden)\n        self.linear3 = nn.Linear(n_hidden, out_size)\n\n    # Prediction function\n    def forward(self, x):\n        x = F.relu(self.drop(self.linear1(x)))\n        x = F.relu(self.drop(self.linear2(x)))\n        x = self.linear3(x)\n        return x\n</code></pre> <p>Create two model objects: <code>model</code> had no dropout and <code>model_drop</code> has a dropout probability of 0.5:</p> <pre><code># Create two model objects: model without dropout and model with dropout\n\nmodel = Net(2, 300, 2)\nmodel_drop = Net(2, 300, 2, p=0.5)\n</code></pre> <p>Set the model using dropout to training mode; this is the default mode, but it's  good practice to write this in your code : </p> <pre><code># Set the model to training mode\n\nmodel_drop.train()\n</code></pre> <p>Train the model by using the Adam optimizer. See the unit on other optimizers. Use the Cross Entropy Loss:</p> <pre><code># Set optimizer functions and criterion functions\n\noptimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)\noptimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)\ncriterion = torch.nn.CrossEntropyLoss()\n</code></pre> <p>Initialize a dictionary that stores the training and validation loss for each model:</p> <pre><code># Initialize the LOSS dictionary to store the loss\n\nLOSS = {}\nLOSS['training data no dropout'] = []\nLOSS['validation data no dropout'] = []\nLOSS['training data dropout'] = []\nLOSS['validation data dropout'] = []\n</code></pre> <p>Run 500 iterations of batch gradient gradient descent: </p> <pre><code># Train the model\n\nepochs = 500\n\ndef train_model(epochs):\n\n    for epoch in range(epochs):\n        #all the samples are used for training \n        yhat = model(data_set.x)\n        yhat_drop = model_drop(data_set.x)\n        loss = criterion(yhat, data_set.y)\n        loss_drop = criterion(yhat_drop, data_set.y)\n\n        #store the loss for both the training and validation data for both models \n        LOSS['training data no dropout'].append(loss.item())\n        LOSS['validation data no dropout'].append(criterion(model(validation_set.x), validation_set.y).item())\n        LOSS['training data dropout'].append(loss_drop.item())\n        model_drop.eval()\n        LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())\n        model_drop.train()\n\n        optimizer_ofit.zero_grad()\n        optimizer_drop.zero_grad()\n        loss.backward()\n        loss_drop.backward()\n        optimizer_ofit.step()\n        optimizer_drop.step()\n\ntrain_model(epochs)\n</code></pre> <p>Set the model with dropout to evaluation mode: </p> <pre><code># Set the model to evaluation model\n\nmodel_drop.eval()\n</code></pre> <p>Test the model without dropout on the validation data: </p> <pre><code># Print out the accuracy of the model without dropout\n\nprint(\"The accuracy of the model without dropout: \", accuracy(model, validation_set))\n</code></pre> <p>Test the model with dropout on the validation data: </p> <pre><code># Print out the accuracy of the model with dropout\n\nprint(\"The accuracy of the model with dropout: \", accuracy(model_drop, validation_set))\n</code></pre> <p>You see that the model with dropout performs better on the validation data.</p> <p>Plot the decision boundary and the prediction of the networks in different colors.</p> <pre><code># Plot the decision boundary and the prediction\n\nplot_decision_regions_3class(data_set)\n</code></pre> <p>Model without Dropout:</p> <pre><code># The model without dropout\n\nplot_decision_regions_3class(data_set, model)\n</code></pre> <p>Model with Dropout:</p> <pre><code># The model with dropout\n\nplot_decision_regions_3class(data_set, model_drop)\n</code></pre> <p>You can see that the model using dropout does better at tracking the function that generated the data. </p> <p>Plot out the loss for the training and validation data on both models, we use the log to make the difference more apparent</p> <pre><code># Plot the LOSS\n\nplt.figure(figsize=(6.1, 10))\ndef plot_LOSS():\n    for key, value in LOSS.items():\n        plt.plot(np.log(np.array(value)), label=key)\n        plt.legend()\n        plt.xlabel(\"iterations\")\n        plt.ylabel(\"Log of cost or total loss\")\n\nplot_LOSS()\n</code></pre> <p>You see that the model without dropout performs better on the training data, but it performs worse on the validation data. This suggests overfitting.  However, the model using dropout performed better on the validation data, but worse on the training data. </p>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.1dropoutPredictin_v2/#using-dropout-for-classification","title":"Using Dropout for Classification","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.1dropoutPredictin_v2/#objective-for-this-notebook-1-create-the-model-and-cost-function-the-pytorch-way-2-batch-gradient-descent","title":"Objective for this Notebook","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.1dropoutPredictin_v2/#1-create-the-model-and-cost-function-the-pytorch-way","title":"1. Create the Model and Cost Function the PyTorch way.","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.1dropoutPredictin_v2/#2-batch-gradient-descent","title":"2. Batch Gradient Descent","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.1dropoutPredictin_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will see how adding dropout to your model will decrease overfitting.</p> <ul> <li>Make Some Data</li> <li>Create the Model and Cost Function the PyTorch way</li> <li>Batch Gradient Descent</li> </ul> <p>Estimated Time Needed: 20 min</p>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.1dropoutPredictin_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.1dropoutPredictin_v2/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.1dropoutPredictin_v2/#Model_Cost","title":"Create the Model, Optimizer, and Total Loss Function (Cost)","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.1dropoutPredictin_v2/#BGD","title":"Train the Model via Mini-Batch Gradient Descent","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.1dropoutPredictin_v2/#true-function","title":"True Function","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.1dropoutPredictin_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.2dropoutRegression_v2/","title":"Dropout Regression","text":"<p>We'll need the following libraries</p> <pre><code># Import the libraries we need for the lab\n\nimport torch\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\n\ntorch.manual_seed(0) \n</code></pre> <p>Create polynomial dataset class: </p> <pre><code># Create Data object\n\nclass Data(Dataset):\n\n    # Constructor\n    def __init__(self, N_SAMPLES=40, noise_std=1, train=True):\n        self.x = torch.linspace(-1, 1, N_SAMPLES).view(-1, 1)\n        self.f = self.x ** 2\n        if train != True:\n            torch.manual_seed(1)\n            self.y = self.f + noise_std * torch.randn(self.f.size())\n            self.y = self.y.view(-1, 1)\n            torch.manual_seed(0)\n        else:\n            self.y = self.f + noise_std * torch.randn(self.f.size())\n            self.y = self.y.view(-1, 1)\n\n    # Getter\n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]\n\n    # Get Length\n    def __len__(self):\n        return self.len\n\n    # Plot the data\n    def plot(self):\n        plt.figure(figsize = (6.1, 10))\n        plt.scatter(self.x.numpy(), self.y.numpy(), label=\"Samples\")\n        plt.plot(self.x.numpy(), self.f.numpy() ,label=\"True Function\", color='orange')\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n        plt.xlim((-1, 1))\n        plt.ylim((-2, 2.5))\n        plt.legend(loc=\"best\")\n        plt.show()\n</code></pre> <p>Create a dataset object:</p> <pre><code># Create the dataset object and plot the dataset\n\ndata_set = Data()\ndata_set.plot()\n</code></pre> <p>Get some validation data: </p> <pre><code># Create validation dataset object\n\nvalidation_set = Data(train=False)\n</code></pre> <p>Create a custom module with three layers. <code>in_size</code> is the size of the input features, <code>n_hidden</code> is the size of the layers, and <code>out_size</code> is the size. <code>p</code> is dropout probability. The default is 0 which is no dropout.</p> <pre><code># Create the class for model\n\nclass Net(nn.Module):\n\n    # Constructor\n    def __init__(self, in_size, n_hidden, out_size, p=0):\n        super(Net, self).__init__()\n        self.drop = nn.Dropout(p=p)\n        self.linear1 = nn.Linear(in_size, n_hidden)\n        self.linear2 = nn.Linear(n_hidden, n_hidden)\n        self.linear3 = nn.Linear(n_hidden, out_size)\n\n    def forward(self, x):\n        x = F.relu(self.drop(self.linear1(x)))\n        x = F.relu(self.drop(self.linear2(x)))\n        x = self.linear3(x)\n        return x\n</code></pre> <p>Create two model objects: <code>model</code> had no dropout, and <code>model_drop  has a dropout probability of 0.5: <p></p> <pre><code># Create the model objects\n\nmodel = Net(1, 300, 1)\nmodel_drop = Net(1, 300, 1, p=0.5)\n</code></pre>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.2dropoutRegression_v2/#using-dropout-in-regression","title":"Using Dropout in Regression","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.2dropoutRegression_v2/#objective-for-this-notebook-1-create-the-model-and-cost-function-the-pytorch-way-2-learn-batch-gradient-descent","title":"Objective for this Notebook","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.2dropoutRegression_v2/#1-create-the-model-and-cost-function-the-pytorch-way","title":"1. Create the Model and Cost Function the PyTorch way.","text":"<p>Set the model using dropout to training mode; this is the default mode, but it's good practice. </p> <pre><code># Set the model to train mode\n\nmodel_drop.train()\n</code></pre> <p>Train the model by using the Adam optimizer. See the unit on other optimizers. Use the mean square loss: </p> <pre><code># Set the optimizer and criterion function\n\noptimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)\noptimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)\ncriterion = torch.nn.MSELoss()\n</code></pre> <p>Initialize a dictionary that stores the training and validation loss for each model:</p> <pre><code># Initialize the dict to contain the loss results\n\nLOSS={}\nLOSS['training data no dropout']=[]\nLOSS['validation data no dropout']=[]\nLOSS['training data dropout']=[]\nLOSS['validation data dropout']=[]\n</code></pre> <p>Run 500 iterations of batch gradient descent: </p> <pre><code># Train the model\n\nepochs = 500\n\ndef train_model(epochs):\n    for epoch in range(epochs):\n        yhat = model(data_set.x)\n        yhat_drop = model_drop(data_set.x)\n        loss = criterion(yhat, data_set.y)\n        loss_drop = criterion(yhat_drop, data_set.y)\n\n        #store the loss for  both the training and validation  data for both models \n        LOSS['training data no dropout'].append(loss.item())\n        LOSS['validation data no dropout'].append(criterion(model(validation_set.x), validation_set.y).item())\n        LOSS['training data dropout'].append(loss_drop.item())\n        model_drop.eval()\n        LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())\n        model_drop.train()\n\n        optimizer_ofit.zero_grad()\n        optimizer_drop.zero_grad()\n        loss.backward()\n        loss_drop.backward()\n        optimizer_ofit.step()\n        optimizer_drop.step()\n\ntrain_model(epochs)\n</code></pre> <p>Set the model with dropout to evaluation mode:</p> <pre><code># Set the model with dropout to evaluation mode\n\nmodel_drop.eval()\n</code></pre> <p>Make a prediction by using both models: </p> <pre><code># Make the prediction\n\nyhat = model(data_set.x)\nyhat_drop = model_drop(data_set.x)\n</code></pre> <p>Plot predictions of both models. Compare them to the training points and the true function: </p> <pre><code># Plot the predictions for both models\n\nplt.figure(figsize=(6.1, 10))\n\nplt.scatter(data_set.x.numpy(), data_set.y.numpy(), label=\"Samples\")\nplt.plot(data_set.x.numpy(), data_set.f.numpy(), label=\"True function\", color='orange')\nplt.plot(data_set.x.numpy(), yhat.detach().numpy(), label='no dropout', c='r')\nplt.plot(data_set.x.numpy(), yhat_drop.detach().numpy(), label=\"dropout\", c ='g')\n\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xlim((-1, 1))\nplt.ylim((-2, 2.5))\nplt.legend(loc = \"best\")\nplt.show()\n</code></pre> <p>You can see that the model using dropout does better at tracking the function that generated the data. We use the log to make the difference more apparent </p> <p>Plot out the loss for training and validation data on both models:  </p> <pre><code># Plot the loss\n\nplt.figure(figsize=(6.1, 10))\nfor key, value in LOSS.items():\n    plt.plot(np.log(np.array(value)), label=key)\n    plt.legend()\n    plt.xlabel(\"iterations\")\n    plt.ylabel(\"Log of cost or total loss\")\n</code></pre> <p>You see that the model without dropout performs better on the training data, but it performs worse on the validation data. This suggests overfitting.  However, the model using dropout performs better on the validation data, but worse on the training data. </p>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.2dropoutRegression_v2/#2-learn-batch-gradient-descent","title":"2. Learn Batch Gradient Descent","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.2dropoutRegression_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will see how adding dropout to your model will decrease overfitting.</p> <ul> <li>Make Some Data</li> <li>Create the Model and Cost Function the PyTorch way</li> <li>Batch Gradient Descent</li> </ul> <p>Estimated Time Needed: 20 min</p>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.2dropoutRegression_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.2dropoutRegression_v2/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.2dropoutRegression_v2/#Model_Cost","title":"Create the Model, Optimizer, and Total Loss Function (Cost)","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.2dropoutRegression_v2/#Train","title":"Train the Model via Mini-Batch Gradient Descent","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.2dropoutRegression_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/","title":"Initialization","text":"<p>We'll need the following libraries</p> <pre><code># Import the libraries we need for this lab\n\nimport torch \nimport torch.nn as nn\nfrom torch import sigmoid\nimport matplotlib.pylab as plt\nimport numpy as np\ntorch.manual_seed(0)\n</code></pre> <p>Used for plotting the model</p> <pre><code># The function for plotting the model\n\ndef PlotStuff(X, Y, model, epoch, leg=True):\n\n    plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))\n    plt.plot(X.numpy(), Y.numpy(), 'r')\n    plt.xlabel('x')\n    if leg == True:\n        plt.legend()\n    else:\n        pass\n</code></pre> <p>Define the activations and the output of the first linear layer as an attribute. Note that this is not good practice. </p> <pre><code># Define the class Net\n\nclass Net(nn.Module):\n\n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        # hidden layer \n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n        # Define the first linear layer as an attribute, this is not good practice\n        self.a1 = None\n        self.l1 = None\n        self.l2=None\n\n    # Prediction\n    def forward(self, x):\n        self.l1 = self.linear1(x)\n        self.a1 = sigmoid(self.l1)\n        self.l2=self.linear2(self.a1)\n        yhat = sigmoid(self.linear2(self.a1))\n        return yhat\n</code></pre> <p>Define the training function:</p> <pre><code># Define the training function\n\ndef train(Y, X, model, optimizer, criterion, epochs=1000):\n    cost = []\n    total=0\n    for epoch in range(epochs):\n        total=0\n        for y, x in zip(Y, X):\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            #cumulative loss \n            total+=loss.item() \n        cost.append(total)\n        if epoch % 300 == 0:    \n            PlotStuff(X, Y, model, epoch, leg=True)\n            plt.show()\n            model(X)\n            plt.scatter(model.a1.detach().numpy()[:, 0], model.a1.detach().numpy()[:, 1], c=Y.numpy().reshape(-1))\n            plt.title('activations')\n            plt.show()\n    return cost\n</code></pre> <pre><code># Make some data\n\nX = torch.arange(-20, 20, 1).view(-1, 1).type(torch.FloatTensor)\nY = torch.zeros(X.shape[0])\nY[(X[:, 0] &amp;gt; -4) &amp;amp; (X[:, 0] &amp;lt; 4)] = 1.0\n</code></pre> <p>Create the Cross-Entropy loss function: </p> <pre><code># The loss function\n\ndef criterion_cross(outputs, labels):\n    out = -1 * torch.mean(labels * torch.log(outputs) + (1 - labels) * torch.log(1 - outputs))\n    return out\n</code></pre> <p>Define the Neural Network</p> <pre><code># Train the model\n# size of input \nD_in = 1\n# size of hidden layer \nH = 2\n# number of outputs \nD_out = 1\n# learning rate \nlearning_rate = 0.1\n# create the model \nmodel = Net(D_in, H, D_out)\n</code></pre> <p>This is the PyTorch default installation</p> <pre><code>model.state_dict()\n</code></pre> <p>Same Weights Initialization with all ones for weights and zeros for the bias.</p> <pre><code>model.state_dict()['linear1.weight'][0]=1.0\nmodel.state_dict()['linear1.weight'][1]=1.0\nmodel.state_dict()['linear1.bias'][0]=0.0\nmodel.state_dict()['linear1.bias'][1]=0.0\nmodel.state_dict()['linear2.weight'][0]=1.0\nmodel.state_dict()['linear2.bias'][0]=0.0\nmodel.state_dict()\n</code></pre> <p>Optimizer, and Train the Model:</p> <pre><code>#optimizer \noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n#train the model usein\ncost_cross = train(Y, X, model, optimizer, criterion_cross, epochs=1000)\n#plot the loss\nplt.plot(cost_cross)\nplt.xlabel('epoch')\nplt.title('cross entropy loss')\n</code></pre> <p>By examining the output of the  paramters all thought they have changed they are identical.</p> <pre><code>model.state_dict()\n</code></pre> <pre><code>yhat=model(torch.tensor([[-2.0],[0.0],[2.0]]))\nyhat\n</code></pre> <pre><code># Train the model\n# size of input \nD_in = 1\n# size of hidden layer \nH = 2\n# number of outputs \nD_out = 1\n# learning rate \nlearning_rate = 0.1\n# create the model \nmodel = Net(D_in, H, D_out)\n</code></pre> <p>Repeat the previous steps above by using the MSE cost or total loss: </p> <pre><code>#optimizer \noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n#train the model usein\ncost_cross = train(Y, X, model, optimizer, criterion_cross, epochs=1000)\n#plot the loss\nplt.plot(cost_cross)\nplt.xlabel('epoch')\nplt.title('cross entropy loss')\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/#initialization-with-same-weights","title":"Initialization with Same Weights","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/#objective-for-this-notebook-1-learn-hw-to-define-the-neural-network-with-same-weights-initialization-define-criterion-function-optimizer-and-train-the-model-2define-the-neural-network-with-defult-weights-initialization-define-criterion-function-optimizer-3-train-the-model","title":"Objective for this Notebook","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/#1-learn-hw-to-define-the-neural-network-with-same-weights-initialization-define-criterion-function-optimizer-and-train-the-model","title":"1. Learn hw to Define the Neural Network with Same Weights Initialization define  Criterion Function, Optimizer, and Train the Model","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/#2define-the-neural-network-with-defult-weights-initialization-define-criterion-function-optimizer","title":"2.Define the Neural Network with defult Weights Initialization define  Criterion Function, Optimizer","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/#3-train-the-model","title":"3. Train the Model","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, we will see the problem of initializing the weights with the same value. We will see that even for a simple network, our model will not train properly. .</p> <ul> <li>Neural Network Module and Training Function</li> <li>Make Some Data</li> <li>Define the Neural Network with Same Weights Initialization define  Criterion Function, Optimizer, and Train the Model</li> <li>Define the Neural Network with defult Weights Initialization define  Criterion Function, Optimizer, and Train the Model</li> </ul> <p>Estimated Time Needed: 25 min</p>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/#Model","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/#Train","title":"Define the Neural Network with Same Weights Initialization define, Criterion Function, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/#Train2","title":"Define the Neural Network, Criterion Function, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/","title":"Xavier Initialization","text":"<p>We'll need the following libraries:  </p> <pre><code># Import the libraries we need to use in this lab\n\n# Using the following line code to install the torchvision library\n# !mamba install -y torchvision\n\nimport torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport matplotlib.pylab as plt\nimport numpy as np\n\ntorch.manual_seed(0)\n</code></pre> <p>Define the neural network module or class with Xavier Initialization</p> <pre><code># Define the neural network with Xavier initialization\n\nclass Net_Xavier(nn.Module):\n\n    # Constructor\n    def __init__(self, Layers):\n        super(Net_Xavier, self).__init__()\n        self.hidden = nn.ModuleList()\n\n        for input_size, output_size in zip(Layers, Layers[1:]):\n            linear = nn.Linear(input_size, output_size)\n            torch.nn.init.xavier_uniform_(linear.weight)\n            self.hidden.append(linear)\n\n    # Prediction\n    def forward(self, x):\n        L = len(self.hidden)\n        for (l, linear_transform) in zip(range(L), self.hidden):\n            if l &amp;lt; L - 1:\n                x = torch.tanh(linear_transform(x))\n            else:\n                x = linear_transform(x)\n        return x\n</code></pre> <p>Define the neural network module with Uniform Initialization:</p> <pre><code># Define the neural network with Uniform initialization\n\nclass Net_Uniform(nn.Module):\n\n    # Constructor\n    def __init__(self, Layers):\n        super(Net_Uniform, self).__init__()\n        self.hidden = nn.ModuleList()\n\n        for input_size, output_size in zip(Layers, Layers[1:]):\n            linear = nn.Linear(input_size, output_size)\n            linear.weight.data.uniform_(0, 1)\n            self.hidden.append(linear)\n\n    # Prediction\n    def forward(self, x):\n        L = len(self.hidden)\n        for (l, linear_transform) in zip(range(L), self.hidden):\n            if l &amp;lt; L - 1:\n                x = torch.tanh(linear_transform(x))\n            else:\n                x = linear_transform(x)\n        return x\n</code></pre> <p>Define the neural network module with PyTroch Default Initialization</p> <pre><code># Define the neural network with Default initialization\n\nclass Net(nn.Module):\n\n    # Constructor\n    def __init__(self, Layers):\n        super(Net, self).__init__()\n        self.hidden = nn.ModuleList()\n\n        for input_size, output_size in zip(Layers, Layers[1:]):\n            linear = nn.Linear(input_size, output_size)\n            self.hidden.append(linear)\n\n    # Prediction\n    def forward(self, x):\n        L = len(self.hidden)\n        for (l, linear_transform) in zip(range(L), self.hidden):\n            if l &amp;lt; L - 1:\n                x = torch.tanh(linear_transform(x))\n            else:\n                x = linear_transform(x)\n        return x\n</code></pre> <p>Define a function to train the model, in this case the function returns a Python dictionary to store the training loss and accuracy on the validation data </p> <pre><code># function to Train the model\n\ndef train(model, criterion, train_loader, validation_loader, optimizer, epochs = 100):\n    i = 0\n    loss_accuracy = {'training_loss':[], 'validation_accuracy':[]}  \n\n    for epoch in range(epochs):\n        for i,(x, y) in enumerate(train_loader):\n            optimizer.zero_grad()\n            z = model(x.view(-1, 28 * 28))\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n            loss_accuracy['training_loss'].append(loss.data.item())\n\n        correct = 0\n        for x, y in validation_loader:\n            yhat = model(x.view(-1, 28 * 28))\n            _, label = torch.max(yhat, 1)\n            correct += (label==y).sum().item()\n        accuracy = 100 * (correct / len(validation_dataset))\n        loss_accuracy['validation_accuracy'].append(accuracy)\n\n    return loss_accuracy\n</code></pre> <p>Load the training dataset by setting the parameters <code>train </code> to <code>True</code> and convert it to a tensor  by placing a transform object int the argument <code>transform</code></p> <pre><code># Create the train dataset\n\ntrain_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n</code></pre> <p>Load the testing dataset by setting the parameters <code>train</code> to <code>False</code> and convert it to a tensor  by placing a transform object int the argument <code>transform</code></p> <pre><code># Create the validation dataset\n\nvalidation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n</code></pre> <p>Create the training-data loader and the validation-data loader object </p> <pre><code># Create Dataloader for both train dataset and validation dataset\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)\n</code></pre> <p>Create the criterion function</p> <pre><code># Define criterion function\n\ncriterion = nn.CrossEntropyLoss()\n</code></pre> <p>Create the model with 100 hidden layers  </p> <pre><code># Set the parameters\n\ninput_dim = 28 * 28\noutput_dim = 10\nlayers = [input_dim, 100, 10, 100, 10, 100, output_dim]\nepochs = 15\n</code></pre> <p>Train the network using PyTorch Default Initialization</p> <pre><code># Train the model with default initialization\n\nmodel = Net(layers)\nlearning_rate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntraining_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=epochs)\n</code></pre> <p>Train the network using Xavier Initialization function</p> <pre><code># Train the model with Xavier initialization\n\nmodel_Xavier = Net_Xavier(layers)\noptimizer = torch.optim.SGD(model_Xavier.parameters(), lr=learning_rate)\ntraining_results_Xavier = train(model_Xavier, criterion, train_loader, validation_loader, optimizer, epochs=epochs)\n</code></pre> <p>Train the network using Uniform Initialization</p> <pre><code># Train the model with Uniform initialization\n\nmodel_Uniform = Net_Uniform(layers)\noptimizer = torch.optim.SGD(model_Uniform.parameters(), lr=learning_rate)\ntraining_results_Uniform = train(model_Uniform, criterion, train_loader, validation_loader, optimizer, epochs=epochs)\n</code></pre> <p>Compare the training loss for each initialization</p> <pre><code># Plot the loss\n\nplt.plot(training_results_Xavier['training_loss'], label='Xavier')\nplt.plot(training_results['training_loss'], label='Default')\nplt.plot(training_results_Uniform['training_loss'], label='Uniform')\nplt.ylabel('loss')\nplt.xlabel('iteration ')  \nplt.title('training loss iterations')\nplt.legend()\n</code></pre> <p>compare the validation loss for each model  </p> <pre><code># Plot the accuracy\n\nplt.plot(training_results_Xavier['validation_accuracy'], label='Xavier')\nplt.plot(training_results['validation_accuracy'], label='Default')\nplt.plot(training_results_Uniform['validation_accuracy'], label='Uniform') \nplt.ylabel('validation accuracy')\nplt.xlabel('epochs')   \nplt.legend()\n</code></pre>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/#test-uniform-default-and-xavier-uniform-initialization-on-mnist-dataset-with-tanh-activation","title":"Test Uniform, Default and Xavier Uniform Initialization on MNIST dataset with tanh activation","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/#objective-for-this-notebook-1-define-several-neural-network-criterion-function-optimizer-2-test-uniform-default-and-xavier-initialization","title":"Objective for this Notebook","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/#1-define-several-neural-network-criterion-function-optimizer","title":"1. Define Several Neural Network, Criterion function, Optimizer","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/#2-test-uniform-default-and-xavier-initialization","title":"2. Test Uniform, Default and Xavier Initialization","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will test PyTroch Default Initialization, Xavier Initialization and Uniform Initialization on the MNIST dataset. </p> <ul> <li>Neural Network Module and Training Function</li> <li>Make Some Data</li> <li>Define Several Neural Network, Criterion function, Optimizer</li> <li>Test Uniform, Default and Xavier Initialization</li> <li>Analyze Results</li> </ul> <p>Estimated Time Needed: 25 min</p>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/#Model","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/#Cost","title":"Define Neural Network, Criterion function, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/#Train","title":"Test PyTorch Default Initialization, Xavier Initialization, Uniform Initialization","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/#Result","title":"Analyse Results","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/","title":"He Initialization","text":"<p>We'll need the following libraries:  </p> <pre><code># Import the libraries we need to use in this lab\n\n# Using the following line code to install the torchvision library\n# !mamba install -y torchvision\n\nimport torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport torch.nn.functional as F\nimport matplotlib.pylab as plt\nimport numpy as np\n\ntorch.manual_seed(0)\n</code></pre> <p>Define the neural network module or class with He Initialization</p> <pre><code># Define the class for neural network model with He Initialization\n\nclass Net_He(nn.Module):\n\n    # Constructor\n    def __init__(self, Layers):\n        super(Net_He, self).__init__()\n        self.hidden = nn.ModuleList()\n\n        for input_size, output_size in zip(Layers, Layers[1:]):\n            linear = nn.Linear(input_size, output_size)\n            torch.nn.init.kaiming_uniform_(linear.weight, nonlinearity='relu')\n            self.hidden.append(linear)\n\n    # Prediction\n    def forward(self, x):\n        L = len(self.hidden)\n        for (l, linear_transform) in zip(range(L), self.hidden):\n            if l &amp;lt; L - 1:\n                x = F.relu(linear_transform(x))\n            else:\n                x = linear_transform(x)\n        return x\n</code></pre> <p>Define the class or neural network with Uniform Initialization</p> <pre><code># Define the class for neural network model with Uniform Initialization\n\nclass Net_Uniform(nn.Module):\n\n    # Constructor\n    def __init__(self, Layers):\n        super(Net_Uniform, self).__init__()\n        self.hidden = nn.ModuleList()\n\n        for input_size, output_size in zip(Layers, Layers[1:]):\n            linear = nn.Linear(input_size,output_size)\n            linear.weight.data.uniform_(0, 1)\n            self.hidden.append(linear)\n\n    # Prediction\n    def forward(self, x):\n        L = len(self.hidden)\n        for (l, linear_transform) in zip(range(L), self.hidden):\n            if l &amp;lt; L - 1:\n                x = F.relu(linear_transform(x))\n            else:\n                x = linear_transform(x)\n\n        return x\n</code></pre> <p>Class or Neural Network with PyTorch Default Initialization</p> <pre><code># Define the class for neural network model with PyTorch Default Initialization\n\nclass Net(nn.Module):\n\n    # Constructor\n    def __init__(self, Layers):\n        super(Net, self).__init__()\n        self.hidden = nn.ModuleList()\n\n        for input_size, output_size in zip(Layers, Layers[1:]):\n            linear = nn.Linear(input_size, output_size)\n            self.hidden.append(linear)\n\n    def forward(self, x):\n        L=len(self.hidden)\n        for (l, linear_transform) in zip(range(L), self.hidden):\n            if l &amp;lt; L - 1:\n                x = F.relu(linear_transform(x))\n            else:\n                x = linear_transform(x)\n\n        return x\n</code></pre> <p>Define a function to train the model, in this case the function returns a Python dictionary to store the training loss and accuracy on the validation data </p> <pre><code># Define function to  train model\n\ndef train(model, criterion, train_loader, validation_loader, optimizer, epochs = 100):\n    i = 0\n    loss_accuracy = {'training_loss': [], 'validation_accuracy': []}  \n\n    #n_epochs\n    for epoch in range(epochs):\n        for i, (x, y) in enumerate(train_loader):\n            optimizer.zero_grad()\n            z = model(x.view(-1, 28 * 28))\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n            loss_accuracy['training_loss'].append(loss.data.item())\n\n        correct = 0\n        for x, y in validation_loader:\n            yhat = model(x.view(-1, 28 * 28))\n            _, label = torch.max(yhat, 1)\n            correct += (label == y).sum().item()\n        accuracy = 100 * (correct / len(validation_dataset))\n        loss_accuracy['validation_accuracy'].append(accuracy)\n\n    return loss_accuracy\n</code></pre> <p>Load the training dataset by setting the parameters <code>train </code> to <code>True</code> and convert it to a tensor  by placing a transform object int the argument <code>transform</code></p> <pre><code># Create the training dataset\n\ntrain_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n</code></pre> <p>Load the testing dataset by setting the parameters train  <code>False</code> and convert it to a tensor  by placing a transform object int the argument <code>transform</code></p> <pre><code># Create the validation dataset\n\nvalidation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n</code></pre> <p>Create the training-data loader and the validation-data loader object </p> <pre><code># Create the data loader for training and validation\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)\n</code></pre> <p>Create the criterion function  </p> <pre><code># Create the criterion function\n\ncriterion = nn.CrossEntropyLoss()\n</code></pre> <p>Create a list that contains layer size </p> <pre><code># Create the parameters\n\ninput_dim = 28 * 28\noutput_dim = 10\nlayers = [input_dim, 100, 200, 100, output_dim]\n</code></pre> <p>Train the network using PyTorch Default Initialization</p> <pre><code># Train the model with the default initialization\n\nmodel = Net(layers)\nlearning_rate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntraining_results = train(model, criterion, train_loader,validation_loader, optimizer, epochs=30)\n</code></pre> <p>Train the network using He Initialization function</p> <pre><code># Train the model with the He initialization\n\nmodel_He = Net_He(layers)\noptimizer = torch.optim.SGD(model_He.parameters(), lr=learning_rate)\ntraining_results_He = train(model_He, criterion, train_loader, validation_loader, optimizer, epochs=30)\n</code></pre> <p>Train the network using Uniform Initialization function</p> <pre><code># Train the model with the Uniform initialization\n\nmodel_Uniform = Net_Uniform(layers)\noptimizer = torch.optim.SGD(model_Uniform.parameters(), lr=learning_rate)\ntraining_results_Uniform = train(model_Uniform, criterion, train_loader, validation_loader, optimizer, epochs=30)\n</code></pre> <p>Compare the training loss for each activation </p> <pre><code># Plot the loss\n\nplt.plot(training_results_He['training_loss'], label='He')\nplt.plot(training_results['training_loss'], label='Default')\nplt.plot(training_results_Uniform['training_loss'], label='Uniform')\nplt.ylabel('loss')\nplt.xlabel('iteration ') \nplt.title('training loss iterations')\nplt.legend()\n</code></pre> <p>Compare the validation loss for each model  </p> <pre><code># Plot the accuracy\n\nplt.plot(training_results_He['validation_accuracy'], label='He')\nplt.plot(training_results['validation_accuracy'], label='Default')\nplt.plot(training_results_Uniform['validation_accuracy'], label='Uniform') \nplt.ylabel('validation accuracy')\nplt.xlabel('epochs ')   \nplt.legend()\nplt.show()\n</code></pre>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/#test-uniform-default-and-he-initialization-on-mnist-dataset-with-relu-activation","title":"Test Uniform, Default and He Initialization on MNIST Dataset with Relu Activation","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/#objective-for-this-notebook-1-learn-how-to-define-several-neural-network-criterion-function-optimizer-2-test-uniform-default-and-he-initialization","title":"Objective for this Notebook","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/#1-learn-how-to-define-several-neural-network-criterion-function-optimizer","title":"1. Learn how to Define Several Neural Network, Criterion function, Optimizer.","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/#2-test-uniform-default-and-he-initialization","title":"2. Test Uniform, Default and He Initialization","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will test the Uniform Initialization, Default Initialization and He Initialization on the MNIST dataset with Relu Activation</p> <ul> <li>Neural Network Module and Training Function</li> <li>Make Some Data</li> <li>Define Several Neural Network, Criterion function, Optimizer</li> <li>Test Uniform, Default and He Initialization</li> <li>Analyze Results</li> </ul> <p>Estimated Time Needed: 25 min</p>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/#Model","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/#Makeup_Data","title":"Make some Data","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/#Cost","title":"Define Neural Network, Criterion function, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/#Train","title":"Test PyTorch Default Initialization, Xavier Initialization and Uniform Initialization","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/#Result","title":"Analyze Results","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.1_MomentumwithPolynomialFunctions_v2/","title":"Momentum","text":"<p>Import the following libraries that you'll use for this lab:</p> <pre><code># These are the libraries that will be used for this lab.\n\nimport torch \nimport torch.nn as nn\nimport matplotlib.pylab as plt\nimport numpy as np\n\ntorch.manual_seed(0)\n</code></pre> <p>This function will plot a cubic function and the parameter values obtained via Gradient Descent.</p> <pre><code># Plot the cubic\n\ndef plot_cubic(w, optimizer):\n    LOSS = []\n    # parameter values \n    W = torch.arange(-4, 4, 0.1)\n    # plot the loss fuction \n    for w.state_dict()['linear.weight'][0] in W:\n        LOSS.append(cubic(w(torch.tensor([[1.0]]))).item())\n    w.state_dict()['linear.weight'][0] = 4.0\n    n_epochs = 10\n    parameter = []\n    loss_list = []\n\n    # n_epochs\n    # Use PyTorch custom module to implement a ploynomial function\n    for n in range(n_epochs):\n        optimizer.zero_grad() \n        loss = cubic(w(torch.tensor([[1.0]])))\n        loss_list.append(loss)\n        parameter.append(w.state_dict()['linear.weight'][0].detach().data.item())\n        loss.backward()\n        optimizer.step()\n    plt.plot(parameter, loss_list, 'ro', label='parameter values')\n    plt.plot(W.numpy(), LOSS, label='objective function')\n    plt.xlabel('w')\n    plt.ylabel('l(w)')\n    plt.legend()\n</code></pre> <p>This function will plot a 4th order function and the parameter values obtained via Gradient Descent. You can also add Gaussian noise with a standard deviation determined by the parameter <code>std</code>.</p> <pre><code># Plot the fourth order function and the parameter values\n\ndef plot_fourth_order(w, optimizer, std=0, color='r', paramlabel='parameter values', objfun=True):\n    W = torch.arange(-4, 6, 0.1)\n    LOSS = []\n    for w.state_dict()['linear.weight'][0] in W:\n        LOSS.append(fourth_order(w(torch.tensor([[1.0]]))).item())\n    w.state_dict()['linear.weight'][0] = 6\n    n_epochs = 100\n    parameter = []\n    loss_list = []\n\n    #n_epochs\n    for n in range(n_epochs):\n        optimizer.zero_grad()\n        loss = fourth_order(w(torch.tensor([[1.0]]))) + std * torch.randn(1, 1)\n        loss_list.append(loss)\n        parameter.append(w.state_dict()['linear.weight'][0].detach().data.item())\n        loss.backward()\n        optimizer.step()\n\n    # Plotting\n    if objfun:\n        plt.plot(W.numpy(), LOSS, label='objective function')\n    plt.plot(parameter, loss_list, 'ro',label=paramlabel, color=color)\n    plt.xlabel('w')\n    plt.ylabel('l(w)')\n    plt.legend()\n</code></pre> <p>This is a custom module. It will behave like a single parameter value. We do it this way so we can use PyTorch's build-in optimizers .</p> <pre><code># Create a linear model\n\nclass one_param(nn.Module):\n\n    # Constructor\n    def __init__(self, input_size, output_size):\n        super(one_param, self).__init__()\n        self.linear = nn.Linear(input_size, output_size, bias=False)\n\n    # Prediction\n    def forward(self, x):\n        yhat = self.linear(x)\n        return yhat\n</code></pre> <p>We create an object <code>w</code>, when we call the object with an input of one, it will behave like an individual parameter value. i.e <code>w(1)</code> is analogous to \\(w\\) </p> <pre><code># Create a one_param object\n\nw = one_param(1, 1)\n</code></pre> <p>Let's create a cubic function with Saddle points </p> <pre><code># Define a function to output a cubic \n\ndef cubic(yhat):\n    out = yhat ** 3\n    return out\n</code></pre> <p>We create an optimizer with no momentum term </p> <pre><code># Create a optimizer without momentum\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.01, momentum=0)\n</code></pre> <p>We run several iterations of stochastic gradient descent and plot the results. We see the parameter values get stuck in the saddle point.</p> <pre><code># Plot the model\n\nplot_cubic(w, optimizer)\n</code></pre> <p>we create an optimizer with momentum term of 0.9</p> <pre><code># Create a optimizer with momentum\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.01, momentum=0.9)\n</code></pre> <p>We run several iterations of stochastic gradient descent with momentum and plot the results. We see the parameter values do not get stuck in the saddle point.</p> <pre><code># Plot the model\n\nplot_cubic(w, optimizer)\n</code></pre> <p>In this section, we will create a fourth order polynomial with a local minimum at 4 and a global minimum a -2. We will then see how the momentum parameter affects convergence to a global minimum. The fourth order polynomial is given by:</p> <pre><code># Create a function to calculate the fourth order polynomial \n\ndef fourth_order(yhat): \n    out = torch.mean(2 * (yhat ** 4) - 9 * (yhat ** 3) - 21 * (yhat ** 2) + 88 * yhat + 48)\n    return out\n</code></pre> <p>We create an optimizer with no momentum term. We run several iterations of stochastic gradient descent and plot the results. We see the parameter values get stuck in the local minimum.</p> <pre><code># Make the prediction without momentum\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.001)\nplot_fourth_order(w, optimizer)\n</code></pre> <p>We create an optimizer with a  momentum term of 0.9. We run several iterations of stochastic gradient descent and plot the results. We see the parameter values reach a global minimum.</p> <pre><code># Make the prediction with momentum\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.001, momentum=0.9)\nplot_fourth_order(w, optimizer)\n</code></pre> <p>In this section, we will create a fourth order polynomial with a local minimum at 4 and a global minimum a -2, but we will add noise to the function when the Gradient is calculated. We will then see how the momentum parameter affects convergence to a global minimum. </p> <p>with no momentum, we get stuck in a local minimum </p> <pre><code># Make the prediction without momentum when there is noise\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.001)\nplot_fourth_order(w, optimizer, std=10)\n</code></pre> <p>with  momentum, we get to the global  minimum </p> <pre><code># Make the prediction with momentum when there is noise\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.001,momentum=0.9)\nplot_fourth_order(w, optimizer, std=10)\n</code></pre> <p>Create two <code> SGD</code>  objects with a learning rate of <code> 0.001</code>. Use the default momentum parameter value  for one and a value of <code> 0.9</code> for the second. Use the function <code>plot_fourth_order</code> with an <code>std=100</code>, to plot the different steps of each. Make sure you run the function on two independent cells.</p> <pre><code># Practice: Create two SGD optimizer with lr = 0.001, and one without momentum and the other with momentum = 0.9. Plot the result out.\n\n# Type your code here\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.1_MomentumwithPolynomialFunctions_v2/#momentum","title":"Momentum","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.1_MomentumwithPolynomialFunctions_v2/#objective-for-this-notebook-1-learn-saddle-points-local-minima-and-noise","title":"Objective for this Notebook","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.1_MomentumwithPolynomialFunctions_v2/#1-learn-saddle-points-local-minima-and-noise","title":"1. Learn Saddle Points, Local Minima, and Noise","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.1_MomentumwithPolynomialFunctions_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will deal with several problems associated with optimization and see how momentum can improve your results.</p> <ul> <li>Saddle Points</li> <li>Local Minima</li> <li> Noise </li> </ul> <p>Estimated Time Needed: 25 min</p>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.1_MomentumwithPolynomialFunctions_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.1_MomentumwithPolynomialFunctions_v2/#Saddle","title":"Saddle Points","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.1_MomentumwithPolynomialFunctions_v2/#Minima","title":"Local Minima","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.1_MomentumwithPolynomialFunctions_v2/#Noise","title":"Noise","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.1_MomentumwithPolynomialFunctions_v2/#practice","title":"Practice","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.1_MomentumwithPolynomialFunctions_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.2_NeuralNetworkswithMomentum_v2/","title":"NN with Momentum","text":"<p>We'll need the following libraries:  </p> <pre><code># Import the libraries for this lab\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom matplotlib.colors import ListedColormap\nfrom torch.utils.data import Dataset, DataLoader\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n</code></pre> <p>Functions used to plot:</p> <pre><code># Define a function for plot the decision region\n\ndef plot_decision_regions_3class(model, data_set):\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA','#00AAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00','#00AAFF'])\n    X=data_set.x.numpy()\n    y=data_set.y.numpy()\n    h = .02\n    x_min, x_max = X[:, 0].min() - 0.1 , X[:, 0].max() + 0.1 \n    y_min, y_max = X[:, 1].min() - 0.1 , X[:, 1].max() + 0.1 \n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n    XX=torch.torch.Tensor(np.c_[xx.ravel(), yy.ravel()])\n    _,yhat=torch.max(model(XX),1)\n    yhat=yhat.numpy().reshape(xx.shape)\n    plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)\n    plt.plot(X[y[:]==0,0], X[y[:]==0,1], 'ro', label='y=0')\n    plt.plot(X[y[:]==1,0], X[y[:]==1,1], 'go', label='y=1')\n    plt.plot(X[y[:]==2,0], X[y[:]==2,1], 'o', label='y=2')\n    plt.title(\"decision region\")\n    plt.legend()\n</code></pre> <p>Create the dataset class </p> <pre><code># Create the dataset class\n\nclass Data(Dataset):\n\n    #  modified from: http://cs231n.github.io/neural-networks-case-study/\n    # Constructor\n    def __init__(self, K=3, N=500):\n        D = 2\n        X = np.zeros((N * K, D)) # data matrix (each row = single example)\n        y = np.zeros(N * K, dtype='uint8') # class labels\n        for j in range(K):\n          ix = range(N * j, N * (j + 1))\n          r = np.linspace(0.0, 1, N) # radius\n          t = np.linspace(j * 4, (j + 1) * 4, N) + np.random.randn(N) * 0.2 # theta\n          X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n          y[ix] = j\n\n        self.y = torch.from_numpy(y).type(torch.LongTensor)\n        self.x = torch.from_numpy(X).type(torch.FloatTensor)\n        self.len = y.shape[0]\n\n    # Getter\n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]\n\n    # Get Length\n    def __len__(self):\n        return self.len\n\n    # Plot the diagram\n    def plot_data(self):\n        plt.plot(self.x[self.y[:] == 0, 0].numpy(), self.x[self.y[:] == 0, 1].numpy(), 'o', label=\"y=0\")\n        plt.plot(self.x[self.y[:] == 1, 0].numpy(), self.x[self.y[:] == 1, 1].numpy(), 'ro', label=\"y=1\")\n        plt.plot(self.x[self.y[:] == 2, 0].numpy(),self.x[self.y[:] == 2, 1].numpy(), 'go',label=\"y=2\")\n        plt.legend()\n</code></pre> <p>Create Neural Network Module using <code>ModuleList()</code></p> <pre><code># Create dataset object\n\nclass Net(nn.Module):\n\n    # Constructor\n    def __init__(self, Layers):\n        super(Net, self).__init__()\n        self.hidden = nn.ModuleList()\n        for input_size, output_size in zip(Layers, Layers[1:]):\n            self.hidden.append(nn.Linear(input_size, output_size))\n\n    # Prediction\n    def forward(self, activation):\n        L = len(self.hidden)\n        for (l, linear_transform) in zip(range(L), self.hidden):\n            if l &amp;lt; L - 1:\n                activation = F.relu(linear_transform(activation))    \n            else:\n                activation = linear_transform(activation)\n        return activation\n</code></pre> <p>Create the function for training the model.</p> <pre><code># Define the function for training the model\n\ndef train(data_set, model, criterion, train_loader, optimizer, epochs=100):\n    LOSS = []\n    ACC = []\n    for epoch in range(epochs):\n        for x, y in train_loader:\n            optimizer.zero_grad()\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        LOSS.append(loss.item())\n        ACC.append(accuracy(model,data_set))\n\n    results ={\"Loss\":LOSS, \"Accuracy\":ACC}\n    fig, ax1 = plt.subplots()\n    color = 'tab:red'\n    ax1.plot(LOSS,color=color)\n    ax1.set_xlabel('epoch', color=color)\n    ax1.set_ylabel('total loss', color=color)\n    ax1.tick_params(axis = 'y', color=color)\n\n    ax2 = ax1.twinx()  \n    color = 'tab:blue'\n    ax2.set_ylabel('accuracy', color=color)  # we already handled the x-label with ax1\n    ax2.plot(ACC, color=color)\n    ax2.tick_params(axis='y', color=color)\n    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n\n    plt.show()\n    return results\n</code></pre> <p>Define a function used to calculate accuracy.</p> <pre><code># Define a function for calculating accuracy\n\ndef accuracy(model, data_set):\n    _, yhat = torch.max(model(data_set.x), 1)\n    return (yhat == data_set.y).numpy().mean()\n</code></pre> <p>Crate a dataset object using <code>Data</code></p> <pre><code># Create the dataset and plot it\n\ndata_set = Data()\ndata_set.plot_data()\ndata_set.y = data_set.y.view(-1)\n</code></pre> <p>Dictionary to contain different cost and  accuracy values for each epoch  for different values of the momentum parameter.</p> <pre><code># Initialize a dictionary to contain the cost and accuracy\n\nResults = {\"momentum 0\": {\"Loss\": 0, \"Accuracy:\": 0}, \"momentum 0.1\": {\"Loss\": 0, \"Accuracy:\": 0}}\n</code></pre> <p>Create a  network to classify three classes with 1 hidden layer with 50 neurons and a momentum value of zero.</p> <pre><code># Train a model with 1 hidden layer and 50 neurons\n\nLayers = [2, 50, 3]\nmodel = Net(Layers)\nlearning_rate = 0.10\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntrain_loader = DataLoader(dataset=data_set, batch_size=20)\ncriterion = nn.CrossEntropyLoss()\nResults[\"momentum 0\"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)\nplot_decision_regions_3class(model, data_set)\n</code></pre> <p>Create a network to classify three classes with 1 hidden layer with 50 neurons and a momentum value of 0.1.</p> <pre><code># Train a model with 1 hidden layer and 50 neurons with 0.1 momentum\n\nLayers = [2, 50, 3]\nmodel = Net(Layers)\nlearning_rate = 0.10\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.1)\ntrain_loader = DataLoader(dataset=data_set, batch_size=20)\ncriterion = nn.CrossEntropyLoss()\nResults[\"momentum 0.1\"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)\nplot_decision_regions_3class(model, data_set)\n</code></pre> <p>Create a network to classify three classes with 1 hidden layer with 50 neurons and a momentum value of 0.2.</p> <pre><code># Train a model with 1 hidden layer and 50 neurons with 0.2 momentum\n\nLayers = [2, 50, 3]\nmodel = Net(Layers)\nlearning_rate = 0.10\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.2)\ntrain_loader = DataLoader(dataset=data_set, batch_size=20)\ncriterion = nn.CrossEntropyLoss()\nResults[\"momentum 0.2\"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)\nplot_decision_regions_3class(model, data_set)\n</code></pre> <p>Create a network to classify three classes with 1 hidden layer with 50 neurons and a momentum value of 0.4.</p> <pre><code># Train a model with 1 hidden layer and 50 neurons with 0.4 momentum\n\nLayers = [2, 50, 3]\nmodel = Net(Layers)\nlearning_rate = 0.10\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.4)\ntrain_loader = DataLoader(dataset=data_set, batch_size=20)\ncriterion = nn.CrossEntropyLoss()\nResults[\"momentum 0.4\"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)\nplot_decision_regions_3class(model, data_set)\n</code></pre> <p>Create a network to classify three classes with 1 hidden layer with 50 neurons and a momentum value of 0.5.</p> <pre><code># Train a model with 1 hidden layer and 50 neurons with 0.5 momentum\n\nLayers = [2, 50, 3]\nmodel = Net(Layers)\nlearning_rate = 0.10\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.5)\ntrain_loader = DataLoader(dataset=data_set, batch_size=20)\ncriterion = nn.CrossEntropyLoss()\nResults[\"momentum 0.5\"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)\nplot_decision_regions_3class(model,data_set)\n</code></pre> <p>The plot below compares results of different momentum terms. We see that in general. The Cost decreases proportionally to the momentum term, but larger momentum terms lead to larger oscillations. While the momentum term decreases faster, it seems that a momentum term of 0.2 reaches the smallest value for the cost. </p> <pre><code># Plot the Loss result for each term\n\nfor key, value in Results.items():\n    plt.plot(value['Loss'],label=key)\n    plt.legend()\n    plt.xlabel('epoch')\n    plt.ylabel('Total Loss or Cost')\n</code></pre> <p>The  accuracy seems to be proportional to the momentum term.</p> <pre><code># Plot the Accuracy result for each term\n\nfor key, value in Results.items():\n    plt.plot(value['Accuracy'],label=key)\n    plt.legend()\n    plt.xlabel('epoch')\n    plt.ylabel('Accuracy')\n</code></pre>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.2_NeuralNetworkswithMomentum_v2/#neural-networks-with-momentum","title":"Neural Networks with Momentum","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.2_NeuralNetworkswithMomentum_v2/#objective-for-this-notebook-1-train-different-neural-networks-model-different-values-for-the-momentum-parameter-2-compare-results-of-different-momentum-terms","title":"Objective for this Notebook","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.2_NeuralNetworkswithMomentum_v2/#1-train-different-neural-networks-model-different-values-for-the-momentum-parameter","title":"1. Train Different Neural Networks Model different values for the Momentum Parameter.","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.2_NeuralNetworkswithMomentum_v2/#2-compare-results-of-different-momentum-terms","title":"2. Compare Results of Different Momentum Terms.","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.2_NeuralNetworkswithMomentum_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will see how different values for the momentum parameters affect the convergence rate of a neural network.</p> <ul> <li>Neural Network Module and Function for Training</li> <li>Train Different Neural Networks Model different values for the Momentum Parameter</li> <li>Compare Results of Different Momentum Terms</li> </ul> <p>Estimated Time Needed: 25 min</p>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.2_NeuralNetworkswithMomentum_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.2_NeuralNetworkswithMomentum_v2/#Model","title":"Neural Network Module and Function for Training","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.2_NeuralNetworkswithMomentum_v2/#Train","title":"Train Different Networks Model different values for the Momentum Parameter","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.2_NeuralNetworkswithMomentum_v2/#Result","title":"Compare Results of Different Momentum Terms","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.2_NeuralNetworkswithMomentum_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/","title":"Batch Normalization","text":"<p>We'll need the following libraries:  </p> <pre><code># These are the libraries will be used for this lab.\n\n# Using the following line code to install the torchvision library\n# !mamba install -y torchvision\n\nimport torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport torch.nn.functional as F\nimport matplotlib.pylab as plt\nimport numpy as np\ntorch.manual_seed(0)\n</code></pre> <pre>\n<code>&lt;torch._C.Generator at 0x7fe38026e6b0&gt;</code>\n</pre> <p>Define the neural network module or class </p> <p>Neural Network Module with two hidden layers using Batch Normalization</p> <pre><code># Define the Neural Network Model using Batch Normalization\n\nclass NetBatchNorm(nn.Module):\n\n    # Constructor\n    def __init__(self, in_size, n_hidden1, n_hidden2, out_size):\n        super(NetBatchNorm, self).__init__()\n        self.linear1 = nn.Linear(in_size, n_hidden1)\n        self.linear2 = nn.Linear(n_hidden1, n_hidden2)\n        self.linear3 = nn.Linear(n_hidden2, out_size)\n        self.bn1 = nn.BatchNorm1d(n_hidden1)\n        self.bn2 = nn.BatchNorm1d(n_hidden2)\n\n    # Prediction\n    def forward(self, x):\n        x = self.bn1(torch.sigmoid(self.linear1(x)))\n        x = self.bn2(torch.sigmoid(self.linear2(x)))\n        x = self.linear3(x)\n        return x\n\n    # Activations, to analyze results \n    def activation(self, x):\n        out = []\n        z1 = self.bn1(self.linear1(x))\n        out.append(z1.detach().numpy().reshape(-1))\n        a1 = torch.sigmoid(z1)\n        out.append(a1.detach().numpy().reshape(-1).reshape(-1))\n        z2 = self.bn2(self.linear2(a1))\n        out.append(z2.detach().numpy().reshape(-1))\n        a2 = torch.sigmoid(z2)\n        out.append(a2.detach().numpy().reshape(-1))\n        return out\n</code></pre> <p>Neural Network Module with two hidden layers with out Batch Normalization</p> <pre><code># Class Net for Neural Network Model\n\nclass Net(nn.Module):\n\n    # Constructor\n    def __init__(self, in_size, n_hidden1, n_hidden2, out_size):\n\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(in_size, n_hidden1)\n        self.linear2 = nn.Linear(n_hidden1, n_hidden2)\n        self.linear3 = nn.Linear(n_hidden2, out_size)\n\n    # Prediction\n    def forward(self, x):\n        x = torch.sigmoid(self.linear1(x))\n        x = torch.sigmoid(self.linear2(x))\n        x = self.linear3(x)\n        return x\n\n    # Activations, to analyze results \n    def activation(self, x):\n        out = []\n        z1 = self.linear1(x)\n        out.append(z1.detach().numpy().reshape(-1))\n        a1 = torch.sigmoid(z1)\n        out.append(a1.detach().numpy().reshape(-1).reshape(-1))\n        z2 = self.linear2(a1)\n        out.append(z2.detach().numpy().reshape(-1))\n        a2 = torch.sigmoid(z2)\n        out.append(a2.detach().numpy().reshape(-1))\n        return out \n</code></pre> <p>Define a function to train the model. In this case the function returns a Python dictionary to store the training loss and accuracy on the validation data </p> <pre><code># Define the function to train model\n\ndef train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n    i = 0\n    useful_stuff = {'training_loss':[], 'validation_accuracy':[]}  \n\n    for epoch in range(epochs):\n        for i, (x, y) in enumerate(train_loader):\n            model.train()\n            optimizer.zero_grad()\n            z = model(x.view(-1, 28 * 28))\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n            useful_stuff['training_loss'].append(loss.data.item())\n\n        correct = 0\n        for x, y in validation_loader:\n            model.eval()\n            yhat = model(x.view(-1, 28 * 28))\n            _, label = torch.max(yhat, 1)\n            correct += (label == y).sum().item()\n\n        accuracy = 100 * (correct / len(validation_dataset))\n        useful_stuff['validation_accuracy'].append(accuracy)\n\n    return useful_stuff\n</code></pre> <p>Load the training dataset by setting the parameters <code>train </code> to <code>True</code> and convert it to a tensor  by placing a transform object int the argument <code>transform</code></p> <pre><code># load the train dataset\n\ntrain_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n</code></pre> <p>Load the validating dataset by setting the parameters train  <code>False</code> and convert it to a tensor by placing a transform object into the argument <code>transform</code></p> <pre><code># load the train dataset\n\nvalidation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n</code></pre> <p>create the training-data loader and the validation-data loader object </p> <pre><code># Create Data Loader for both train and validating\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)\n</code></pre> <p></p> <p>Create the criterion function  </p> <pre><code># Create the criterion function\n\ncriterion = nn.CrossEntropyLoss()\n</code></pre> <p>Variables for Neural Network Shape <code> hidden_dim</code> used for number of neurons in both hidden layers.</p> <pre><code># Set the parameters\n\ninput_dim = 28 * 28\nhidden_dim = 100\noutput_dim = 10\n</code></pre> <p>Train Neural Network using  Batch Normalization :</p> <pre><code># Create model, optimizer and train the model\n\nmodel_norm  = NetBatchNorm(input_dim, hidden_dim, hidden_dim, output_dim)\noptimizer = torch.optim.Adam(model_norm.parameters(), lr = 0.1)\ntraining_results_Norm=train(model_norm , criterion, train_loader, validation_loader, optimizer, epochs=5)\n</code></pre> <p>Train Neural Network with no Batch Normalization:</p> <pre><code># Create model without Batch Normalization, optimizer and train the model\n\nmodel = Net(input_dim, hidden_dim, hidden_dim, output_dim)\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\ntraining_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=5)\n</code></pre> <p>Compare the histograms of the activation for the first layer of the first sample, for both models.</p> <pre><code>model.eval()\nmodel_norm.eval()\nout=model.activation(validation_dataset[0][0].reshape(-1,28*28))\nplt.hist(out[2],label='model with no batch normalization' )\nout_norm=model_norm.activation(validation_dataset[0][0].reshape(-1,28*28))\nplt.hist(out_norm[2],label='model with normalization')\nplt.xlabel(\"activation \")\nplt.legend()\nplt.show()\n</code></pre> <p>We see the activations with Batch Normalization are zero centred and have a smaller variance.</p> <p>Compare the training loss for each iteration</p> <pre><code># Plot the diagram to show the loss\n\nplt.plot(training_results['training_loss'], label='No Batch Normalization')\nplt.plot(training_results_Norm['training_loss'], label='Batch Normalization')\nplt.ylabel('Cost')\nplt.xlabel('iterations ')   \nplt.legend()\nplt.show()\n</code></pre> <p>Compare the validating accuracy for each iteration</p> <pre><code># Plot the diagram to show the accuracy\n\nplt.plot(training_results['validation_accuracy'],label='No Batch Normalization')\nplt.plot(training_results_Norm['validation_accuracy'],label='Batch Normalization')\nplt.ylabel('validation accuracy')\nplt.xlabel('epochs ')   \nplt.legend()\nplt.show()\n</code></pre>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/#batch-normalization-with-the-mnist-dataset","title":"Batch Normalization with the MNIST Dataset","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/#objective-for-this-notebook-1-define-several-neural-networks-criterion-function-optimizer-2-train-neural-network-using-batch-normalization-and-no-batch-normalization","title":"Objective for this Notebook","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/#1-define-several-neural-networks-criterion-function-optimizer","title":"1. Define Several Neural Networks, Criterion function, Optimizer.","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/#2-train-neural-network-using-batch-normalization-and-no-batch-normalization","title":"2. Train Neural Network using Batch Normalization and no Batch Normalization","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will build a Neural Network using Batch Normalization and compare it to a Neural Network that does not use Batch Normalization. You will use the MNIST dataset to test your network. </p> <ul> <li>Neural Network Module and Training Function</li> <li>Load Data </li> <li>Define Several Neural Networks, Criterion function, Optimizer</li> <li>Train Neural Network using Batch Normalization and no Batch Normalization</li> <li>Analyze Results</li> </ul> <p>Estimated Time Needed: 25 min</p>"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/#Train_Func","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/#define-neural-network-criterion-function-optimizer-and-train-the-model","title":"Define Neural Network, Criterion function, Optimizer and Train the  Model","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/#Train","title":"Train Neural Network using Batch Normalization and no Batch Normalization","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/#Result","title":"Analyze Results","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.1What_is_Convolution/","title":"Convolution Basics","text":"<p>author: Juma Shafara date: \"2024-08-08\" title: Convolution Neural Networks keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will review how to make a prediction in several different ways by using PyTorch.</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>Import the following libraries: </p> <pre><code>import torch \nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import ndimage, misc\n</code></pre> <p></p> <p>Convolution is a linear operation similar to a linear equation, dot product, or matrix multiplication. Convolution has several advantages for analyzing images. As discussed in the video, convolution preserves the relationship between elements, and it requires fewer parameters than other methods.  </p> <p>You can see the relationship between the different methods that you learned:</p> \\[linear \\ equation :y=wx+b$$ $$linear\\ equation\\ with\\ multiple \\ variables \\ where \\ \\mathbf{x} \\ is \\ a \\ vector \\ \\mathbf{y}=\\mathbf{wx}+b$$ $$ \\ matrix\\ multiplication \\ where \\ \\mathbf{X} \\ in \\ a \\ matrix \\ \\mathbf{y}=\\mathbf{wX}+\\mathbf{b} $$ $$\\ convolution \\ where \\ \\mathbf{X} \\ and \\ \\mathbf{Y} \\ is \\ a \\ tensor \\  \\mathbf{Y}=\\mathbf{w}*\\mathbf{X}+\\mathbf{b}\\] <p>In convolution, the parameter w is called a kernel. You can perform convolution on images where you let the variable image denote the variable X and w denote the parameter.</p> <p></p> <p>Create a two-dimensional convolution object by using the constructor Conv2d, the parameter <code>in_channels</code> and <code>out_channels</code> will be used for this section, and the parameter kernel_size will be three.</p> <pre><code>conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\nconv\n</code></pre> <pre>\n<code>Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))</code>\n</pre> <p>Because the parameters in <code>nn.Conv2d</code> are randomly initialized and learned through training, give them some values.</p> <pre><code>conv.state_dict()['weight'][0][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\nconv.state_dict()['bias'][0]=0.0\nconv.state_dict()\n</code></pre> <pre>\n<code>OrderedDict([('weight',\n              tensor([[[[ 1.,  0., -1.],\n                        [ 2.,  0., -2.],\n                        [ 1.,  0., -1.]]]])),\n             ('bias', tensor([0.]))])</code>\n</pre> <p>Create a dummy tensor to represent an image. The shape of the image is (1,1,5,5) where:</p> <p>(number of inputs, number of channels, number of rows, number of columns ) </p> <p>Set the third column to 1:</p> <pre><code>image=torch.zeros(1,1,5,5)\nimage[0,0,:,2]=1\nimage\n</code></pre> <pre>\n<code>tensor([[[[0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.]]]])</code>\n</pre> <p>Call the object <code>conv</code> on the tensor <code>image</code> as an input to perform the convolution and assign the result to the tensor <code>z</code>. </p> <pre><code>z=conv(image)\nz\n</code></pre> <pre>\n<code>tensor([[[[-4.,  0.,  4.],\n          [-4.,  0.,  4.],\n          [-4.,  0.,  4.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</code>\n</pre> <p>The following animation illustrates the process, the kernel performs at the element-level multiplication on every element in the image in the corresponding region. The values are then added together. The kernel is then shifted and the process is repeated. </p> <p></p> <p></p> <p>The size of the output is an important parameter. In this lab, you will assume square images. For rectangular images, the same formula can be used in for each dimension independently.  </p> <p>Let M be the size of the input and K be the size of the kernel. The size of the output is given by the following formula:</p> \\[M_{new}=M-K+1\\] <p>Create a kernel of size 2:</p> <pre><code>K=2\nconv1 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=K)\nconv1.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\nconv1.state_dict()['bias'][0]=0.0\nconv1.state_dict()\nconv1\n</code></pre> <pre>\n<code>Conv2d(1, 1, kernel_size=(2, 2), stride=(1, 1))</code>\n</pre> <p>Create an image of size 2:</p> <pre><code>M=4\nimage1=torch.ones(1,1,M,M)\n</code></pre> <p></p> <p>The following equation provides the output:</p> \\[M_{new}=M-K+1$$ $$M_{new}=4-2+1$$ $$M_{new}=3\\] <p>The following animation illustrates the process: The first iteration of the kernel overlay of the images produces one output. As the kernel is of size K, there are M-K  elements for the kernel to move in the horizontal direction. The same logic applies to the vertical direction.  </p> <p></p> <p>Perform the convolution and verify the size is correct:</p> <pre><code>z1=conv1(image1)\nprint(\"z1:\",z1)\nprint(\"shape:\",z1.shape[2:4])\n</code></pre> <pre>\n<code>z1: tensor([[[[4., 4., 4.],\n          [4., 4., 4.],\n          [4., 4., 4.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\nshape: torch.Size([3, 3])\n</code>\n</pre> <p></p> <p>The parameter stride changes the number of shifts the kernel moves per iteration. As a result, the output size also changes and is given by the following formula:</p> \\[M_{new}=\\dfrac{M-K}{stride}+1\\] <p>Create a convolution object with a stride of 2:</p> <pre><code>conv3 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=2)\n\nconv3.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\nconv3.state_dict()['bias'][0]=0.0\nconv3.state_dict()\n</code></pre> <pre>\n<code>OrderedDict([('weight',\n              tensor([[[[1., 1.],\n                        [1., 1.]]]])),\n             ('bias', tensor([0.]))])</code>\n</pre> <p>For an image with a size of 4, calculate the output size:</p> \\[M_{new}=\\dfrac{M-K}{stride}+1$$ $$M_{new}=\\dfrac{4-2}{2}+1$$ $$M_{new}=2\\] <p>The following animation illustrates the process: The first iteration of the kernel overlay of the images produces one output. Because the kernel is of size K, there are M-K=2 elements. The stride is 2 because it will move 2 elements at a time. As a result, you divide M-K by the stride value 2:</p> <p></p> <p>Perform the convolution and verify the size is correct: </p> <pre><code>z3=conv3(image1)\n\nprint(\"z3:\",z3)\nprint(\"shape:\",z3.shape[2:4])\n</code></pre> <pre>\n<code>z3: tensor([[[[4., 4.],\n          [4., 4.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\nshape: torch.Size([2, 2])\n</code>\n</pre> <p></p> <p>As you apply successive convolutions, the image will shrink. You can apply zero padding to keep the image at a reasonable size, which also holds information at the borders.</p> <p>In addition, you might not get integer values for the size of the kernel. Consider the following image:</p> <pre><code>image1\n</code></pre> <pre>\n<code>tensor([[[[1., 1., 1., 1.],\n          [1., 1., 1., 1.],\n          [1., 1., 1., 1.],\n          [1., 1., 1., 1.]]]])</code>\n</pre> <p>Try performing convolutions with the <code>kernel_size=2</code> and a <code>stride=3</code>. Use these values:</p> \\[M_{new}=\\dfrac{M-K}{stride}+1$$ $$M_{new}=\\dfrac{4-2}{3}+1$$ $$M_{new}=1.666\\] <pre><code>conv4 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3)\nconv4.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\nconv4.state_dict()['bias'][0]=0.0\nconv4.state_dict()\nz4=conv4(image1)\nprint(\"z4:\",z4)\nprint(\"z4:\",z4.shape[2:4])\n</code></pre> <pre>\n<code>z4: tensor([[[[4.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\nz4: torch.Size([1, 1])\n</code>\n</pre> <p>You can add rows and columns of zeros around the image. This is called padding. In the constructor <code>Conv2d</code>, you specify the number of rows or columns of zeros that you want to add with the parameter padding. </p> <p>For a square image, you merely pad an extra column of zeros to the first column and the last column. Repeat the process for the rows. As a result, for a square image, the width and height is the original size plus 2 x the number of padding elements specified. You can then determine the size of the output after subsequent operations accordingly as shown in the following equation where you determine the size of an image after padding and then applying a convolutions kernel of size K.</p> \\[M'=M+2 \\times padding$$ $$M_{new}=M'-K+1\\] <p>Consider the following example:</p> <pre><code>conv5 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3,padding=1)\n\nconv5.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\nconv5.state_dict()['bias'][0]=0.0\nconv5.state_dict()\nz5=conv5(image1)\nprint(\"z5:\",z5)\nprint(\"z5:\",z4.shape[2:4])\n</code></pre> <pre>\n<code>z5: tensor([[[[1., 2.],\n          [2., 4.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\nz5: torch.Size([1, 1])\n</code>\n</pre> <pre><code>\n</code></pre> <p>The process is summarized in the following  animation: </p> <p></p> <p></p> <p>A kernel of zeros with a kernel size=3  is applied to the following image: </p> <pre><code>Image=torch.randn((1,1,4,4))\nImage\n</code></pre> <pre>\n<code>tensor([[[[-0.4460, -0.1425,  1.0888,  0.8292],\n          [ 1.0301, -0.4119, -1.0132, -0.4925],\n          [-1.1662, -0.5480,  1.7078,  0.0230],\n          [-0.1644,  1.8086, -1.1509, -0.2585]]]])</code>\n</pre> <p>Question: Without using the function, determine what the outputs values are as each element:</p> <p>Double-click here for the solution.</p> <p>Question: Use the following convolution object to perform convolution on the tensor   <code>Image</code>:</p> <pre><code>conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\nconv.state_dict()['weight'][0][0]=torch.tensor([[0,0,0],[0,0,0],[0,0.0,0]])\nconv.state_dict()['bias'][0]=0.0\n</code></pre> <p>Double-click here for the solution.</p> <p>Question: You have an image of size 4. The parameters are as follows  kernel_size=2,stride=2. What is the size of the output?</p> <pre><code>\n</code></pre>"},{"location":"Deep%20Learning/Week9-CNNs/9.1What_is_Convolution/#objective-for-this-notebook-learn-about-convolution-leran-determining-the-size-of-output-learn-stride-zero-padding","title":"Objective for this Notebook <ul> <li>Learn about Convolution.</li> <li>Leran Determining  the Size of Output. </li> <li>Learn Stride, Zero Padding</li> </ul>","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.1What_is_Convolution/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will study convolution and review how the different operations change the relationship between input and output.</p> <li>What is Convolution  </li> <li>Determining  the Size of Output</li> <li>Stride</li> <li>Zero Padding </li> <li>Practice Questions </li> <p></p> Estimated Time Needed: 25 min"},{"location":"Deep%20Learning/Week9-CNNs/9.1What_is_Convolution/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.1What_is_Convolution/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.1What_is_Convolution/#what-is-convolution","title":"What is Convolution?","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.1What_is_Convolution/#determining-the-size-of-the-output","title":"Determining  the Size of the Output","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.1What_is_Convolution/#stride-parameter","title":"Stride parameter","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.1What_is_Convolution/#zero-padding","title":"Zero Padding","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.1What_is_Convolution/#practice-question","title":"Practice Question","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.1What_is_Convolution/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.2Activation_max_pooling/","title":"Activation & Pooling","text":"<p>author: Juma Shafara date: \"2024-08-12\" title: Activation function and Maxpooling keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will learn two important components in building a convolutional neural network.</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>Import the following libraries:</p> <pre><code>import torch \nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import ndimage, misc\n</code></pre> <p></p> <p>Just like a neural network, you apply an activation function to the activation map as shown in the following image:</p> <p></p> <p>Create a kernel and image as usual. Set the bias to zero: </p> <pre><code>conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\nGx=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0,-1.0]])\nconv.state_dict()['weight'][0][0]=Gx\nconv.state_dict()['bias'][0]=0.0\nconv.state_dict()\n</code></pre> <pre>\n<code>OrderedDict([('weight',\n              tensor([[[[ 1.,  0., -1.],\n                        [ 2.,  0., -2.],\n                        [ 1.,  0., -1.]]]])),\n             ('bias', tensor([0.]))])</code>\n</pre> <pre><code>image=torch.zeros(1,1,5,5)\nimage[0,0,:,2]=1\nimage\n</code></pre> <pre>\n<code>tensor([[[[0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.]]]])</code>\n</pre> <p>The following image shows the image and kernel: </p> <p></p> <p>Apply convolution to the image: </p> <pre><code>Z=conv(image)\nZ\n</code></pre> <pre>\n<code>tensor([[[[-4.,  0.,  4.],\n          [-4.,  0.,  4.],\n          [-4.,  0.,  4.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</code>\n</pre> <p>Apply the activation function to the activation map. This will apply the activation function to each element in the activation map.</p> <pre><code>A=torch.relu(Z)\nA\n</code></pre> <pre>\n<code>tensor([[[[0., 0., 4.],\n          [0., 0., 4.],\n          [0., 0., 4.]]]], grad_fn=&lt;ReluBackward0&gt;)</code>\n</pre> <pre><code>relu = nn.ReLU()\nrelu(Z)\n</code></pre> <pre>\n<code>tensor([[[[0., 0., 4.],\n          [0., 0., 4.],\n          [0., 0., 4.]]]], grad_fn=&lt;ReluBackward0&gt;)</code>\n</pre> <p>The process is summarized in the the following figure. The Relu function is applied to each element. All the elements less than zero are mapped to zero. The remaining components do not change.</p> <p></p> <p></p> <p>Consider the following image: </p> <pre><code>image1=torch.zeros(1,1,4,4)\nimage1[0,0,0,:]=torch.tensor([1.0,2.0,3.0,-4.0])\nimage1[0,0,1,:]=torch.tensor([0.0,2.0,-3.0,0.0])\nimage1[0,0,2,:]=torch.tensor([0.0,2.0,3.0,1.0])\n\nimage1\n</code></pre> <pre>\n<code>tensor([[[[ 1.,  2.,  3., -4.],\n          [ 0.,  2., -3.,  0.],\n          [ 0.,  2.,  3.,  1.],\n          [ 0.,  0.,  0.,  0.]]]])</code>\n</pre> <p>Max pooling simply takes the maximum value in each region. Consider the following image. For the first region, max pooling simply takes the largest element in a yellow region.   </p> <p></p> <p>The region shifts, and the process is repeated. The process is similar to convolution and is demonstrated in the following figure:</p> <p></p> <p>Create a maxpooling object in 2d as follows and perform max pooling as follows:  </p> <pre><code>max1=torch.nn.MaxPool2d(2,stride=1)\nmax1(image1)\n</code></pre> <pre>\n<code>tensor([[[[2., 3., 3.],\n          [2., 3., 3.],\n          [2., 3., 3.]]]])</code>\n</pre> <p>If the stride is set to None (its defaults setting), the process will simply take the maximum in a prescribed area and shift over accordingly as shown in the following figure:</p> <p></p> <p>Here's the code in Pytorch:  </p>"},{"location":"Deep%20Learning/Week9-CNNs/9.2Activation_max_pooling/#objective-for-this-notebook-1-learn-how-to-apply-an-activation-function-2-learn-about-max-pooling","title":"Objective for this Notebook","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.2Activation_max_pooling/#1-learn-how-to-apply-an-activation-function","title":"1. Learn how to apply an activation function.","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.2Activation_max_pooling/#2-learn-about-max-pooling","title":"2. Learn about max pooling","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.2Activation_max_pooling/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will learn two important components in building a convolutional neural network. The first is applying an activation function, which is analogous to building a regular network. You will also learn about max pooling. Max pooling reduces the number of parameters and makes the network less susceptible to changes in the image. </p> <li>Activation Functions</li> <li>Max Pooling</li> <p></p> <p></p> <p>Estimated Time Needed: 25 min</p>"},{"location":"Deep%20Learning/Week9-CNNs/9.2Activation_max_pooling/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.2Activation_max_pooling/#activation-functions","title":"Activation Functions","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.2Activation_max_pooling/#max-pooling","title":"Max Pooling","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.2Activation_max_pooling/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.3Multiple_Channel_Convolution/","title":"Multiple Channels","text":"<p>author: Juma Shafara date: \"2024-08-12\" title: Multiple Channel Convolutional Network keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will learn two important components in building a convolutional neural network.</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>Import the following libraries:</p> <pre><code>import torch \nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import ndimage, misc\n</code></pre> <p></p> <p>In Pytroch, you can create a <code>Conv2d</code> object with multiple outputs. For each channel, a kernel is created, and each kernel performs a convolution independently. As a result, the number of outputs is equal to the number of channels. This is demonstrated in the following figure. The number 9 is convolved with three kernels: each of a different color. There are three different activation maps represented by the different colors.</p> <p></p> <p>Symbolically, this can be represented as follows:</p> <p></p> <p>Create a <code>Conv2d</code> with three channels:</p> <pre><code>conv1 = nn.Conv2d(in_channels=1, out_channels=3,kernel_size=3)\n</code></pre> <p>Pytorch randomly assigns values to each kernel. However, use kernels that have  been developed to detect edges:</p> <pre><code>Gx=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\nGy=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])\n\nconv1.state_dict()['weight'][0][0]=Gx\nconv1.state_dict()['weight'][1][0]=Gy\nconv1.state_dict()['weight'][2][0]=torch.ones(3,3)\n</code></pre> <p>Each kernel has its own bias, so set them all to zero:</p> <pre><code>conv1.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])\nconv1.state_dict()['bias']\n</code></pre> <pre>\n<code>tensor([0., 0., 0.])</code>\n</pre> <p>Print out each kernel: </p> <pre><code>for x in conv1.state_dict()['weight']:\n    print(x)\n</code></pre> <pre>\n<code>tensor([[[ 1.,  0., -1.],\n         [ 2.,  0., -2.],\n         [ 1.,  0., -1.]]])\ntensor([[[ 1.,  2.,  1.],\n         [ 0.,  0.,  0.],\n         [-1., -2., -1.]]])\ntensor([[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]])\n</code>\n</pre> <p>Create an input <code>image</code> to represent the input X:</p> <pre><code>image=torch.zeros(1,1,5,5)\nimage[0,0,:,2]=1\nimage\n</code></pre> <pre>\n<code>tensor([[[[0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.]]]])</code>\n</pre> <p>Plot it as an image: </p> <pre><code>plt.imshow(image[0,0,:,:].numpy(), interpolation='nearest', cmap=plt.cm.gray)\nplt.colorbar()\nplt.show()\n</code></pre> <p>Perform convolution using each channel: </p> <pre><code>out=conv1(image)\n</code></pre> <p>The result is a 1x3x3x3 tensor. This represents one sample with three channels, and each channel contains a 3x3 image.  The same rules that govern the shape of each image were discussed in the last section.</p> <pre><code>out.shape\n</code></pre> <pre>\n<code>torch.Size([1, 3, 3, 3])</code>\n</pre> <p>Print out each channel as a tensor or an image: </p> <pre><code>for channel,image in enumerate(out[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()\n</code></pre> <pre>\n<code>tensor([[-4.,  0.,  4.],\n        [-4.,  0.,  4.],\n        [-4.,  0.,  4.]], grad_fn=&lt;UnbindBackward0&gt;)\n</code>\n</pre> <pre>\n<code>tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]], grad_fn=&lt;UnbindBackward0&gt;)\n</code>\n</pre> <pre>\n<code>tensor([[3., 3., 3.],\n        [3., 3., 3.],\n        [3., 3., 3.]], grad_fn=&lt;UnbindBackward0&gt;)\n</code>\n</pre> <p>Different kernels can be used to detect various features in an image. You can see that the first channel fluctuates, and the second two channels produce a constant value. The following figure summarizes the process:</p> <p></p> <p>If you use a different image, the result will be different: </p> <pre><code>image1=torch.zeros(1,1,5,5)\nimage1[0,0,2,:]=1\nprint(image1)\nplt.imshow(image1[0,0,:,:].detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\nplt.show()\n</code></pre> <pre>\n<code>tensor([[[[0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.],\n          [1., 1., 1., 1., 1.],\n          [0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.]]]])\n</code>\n</pre> <p>In this case, the second channel fluctuates, and the first and the third channels produce a constant value.</p> <pre><code>out1=conv1(image1)\nfor channel,image in enumerate(out1[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()\n</code></pre> <pre>\n<code>tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]], grad_fn=&lt;UnbindBackward0&gt;)\n</code>\n</pre> <pre>\n<code>tensor([[-4., -4., -4.],\n        [ 0.,  0.,  0.],\n        [ 4.,  4.,  4.]], grad_fn=&lt;UnbindBackward0&gt;)\n</code>\n</pre> <pre>\n<code>tensor([[3., 3., 3.],\n        [3., 3., 3.],\n        [3., 3., 3.]], grad_fn=&lt;UnbindBackward0&gt;)\n</code>\n</pre> <p>The following figure summarizes the process:</p> <p></p> <p></p> <p>For two inputs, you can create two kernels. Each kernel performs a convolution on its associated input channel. The resulting output is added together as shown:  </p> <p></p> <p>Create an input with two channels:</p> <pre><code>image2=torch.zeros(1,2,5,5)\nimage2[0,0,2,:]=-2\nimage2[0,1,2,:]=1\nimage2\n</code></pre> <pre>\n<code>tensor([[[[ 0.,  0.,  0.,  0.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.],\n          [-2., -2., -2., -2., -2.],\n          [ 0.,  0.,  0.,  0.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.]],\n\n         [[ 0.,  0.,  0.,  0.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.],\n          [ 1.,  1.,  1.,  1.,  1.],\n          [ 0.,  0.,  0.,  0.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.]]]])</code>\n</pre> <p>Plot out each image: </p> <pre><code>for channel,image in enumerate(image2[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()\n</code></pre> <pre>\n<code>tensor([[ 0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.],\n        [-2., -2., -2., -2., -2.],\n        [ 0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.]])\n</code>\n</pre> <pre>\n<code>tensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n</code>\n</pre> <p>Create a <code>Conv2d</code> object with two inputs:</p> <pre><code>conv3 = nn.Conv2d(in_channels=2, out_channels=1,kernel_size=3)\n</code></pre> <p>Assign kernel values to make the math a little easier: </p> <pre><code>Gx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])\nconv3.state_dict()['weight'][0][0]=1*Gx1\nconv3.state_dict()['weight'][0][1]=-2*Gx1\nconv3.state_dict()['bias'][:]=torch.tensor([0.0])\n</code></pre> <pre><code>conv3.state_dict()['weight']\n</code></pre> <pre>\n<code>tensor([[[[ 0.,  0.,  0.],\n          [ 0.,  1.,  0.],\n          [ 0.,  0.,  0.]],\n\n         [[-0., -0., -0.],\n          [-0., -2., -0.],\n          [-0., -0., -0.]]]])</code>\n</pre> <p>Perform the convolution:</p> <pre><code>conv3(image2)\n</code></pre> <pre>\n<code>tensor([[[[ 0.,  0.,  0.],\n          [-4., -4., -4.],\n          [ 0.,  0.,  0.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</code>\n</pre> <p>The following images summarize the process. The object performs Convolution.</p> <p></p> <p>Then, it adds the result: </p> <p></p> <p></p> <p>When using multiple inputs and outputs, a kernel is created for each input, and the process is repeated for each output. The process is summarized in the following image. </p> <p>There are two input channels and 3 output channels. For each channel, the input in red and purple is convolved with an individual kernel that is colored differently. As a result, there are three outputs. </p> <p></p> <p>Create an example with two inputs and three outputs and assign the kernel values to make the math a little easier: </p> <pre><code>conv4 = nn.Conv2d(in_channels=2, out_channels=3,kernel_size=3)\nconv4.state_dict()['weight'][0][0]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])\nconv4.state_dict()['weight'][0][1]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])\n\n\nconv4.state_dict()['weight'][1][0]=torch.tensor([[0.0,0.0,0.0],[0,1,0],[0.0,0.0,0.0]])\nconv4.state_dict()['weight'][1][1]=torch.tensor([[0.0,0.0,0.0],[0,-1,0],[0.0,0.0,0.0]])\n\nconv4.state_dict()['weight'][2][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\nconv4.state_dict()['weight'][2][1]=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])\n</code></pre> <p>For each output, there is a bias, so set them all to zero: </p> <pre><code>conv4.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])\n</code></pre> <p>Create a two-channel image and plot the results: </p> <pre><code>image4=torch.zeros(1,2,5,5)\nimage4[0][0]=torch.ones(5,5)\nimage4[0][1][2][2]=1\nfor channel,image in enumerate(image4[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()\n</code></pre> <pre>\n<code>tensor([[1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.]])\n</code>\n</pre> <pre>\n<code>tensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n</code>\n</pre> <p>Perform the convolution:</p> <pre><code>z=conv4(image4)\nz\n</code></pre> <pre>\n<code>tensor([[[[ 0.5000,  0.5000,  0.5000],\n          [ 0.5000,  1.0000,  0.5000],\n          [ 0.5000,  0.5000,  0.5000]],\n\n         [[ 1.0000,  1.0000,  1.0000],\n          [ 1.0000,  0.0000,  1.0000],\n          [ 1.0000,  1.0000,  1.0000]],\n\n         [[-1.0000, -2.0000, -1.0000],\n          [ 0.0000,  0.0000,  0.0000],\n          [ 1.0000,  2.0000,  1.0000]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</code>\n</pre> <p>The output of the first channel is given by: </p> <p></p> <p>The output of the second channel is given by:</p> <p></p> <p>The output of the third channel is given by: </p> <p></p> <p></p> <p>Use the following two convolution objects to produce the same result as two input channel convolution on imageA and imageB as shown in the following image:</p> <pre><code>imageA=torch.zeros(1,1,5,5)\nimageB=torch.zeros(1,1,5,5)\nimageA[0,0,2,:]=-2\nimageB[0,0,2,:]=1\n\n\nconv5 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\nconv6 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\n\n\nGx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])\nconv5.state_dict()['weight'][0][0]=1*Gx1\nconv6.state_dict()['weight'][0][0]=-2*Gx1\nconv5.state_dict()['bias'][:]=torch.tensor([0.0])\nconv6.state_dict()['bias'][:]=torch.tensor([0.0])\n</code></pre> <p></p> <p></p> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week9-CNNs/9.3Multiple_Channel_Convolution/#objective-for-this-notebook-1-learn-on-multiple-input-and-multiple-output-channels","title":"Objective for this Notebook","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.3Multiple_Channel_Convolution/#1-learn-on-multiple-input-and-multiple-output-channels","title":"1. Learn on Multiple Input and Multiple Output Channels.","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.3Multiple_Channel_Convolution/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, you will study convolution and review how the different operations change the relationship between input and output.</p> <li>Multiple Output Channels </li> <li>Multiple Inputs</li> <li>Multiple Input and Multiple Output Channels </li> <li>Practice Questions </li> <p></p> Estimated Time Needed: 25 min"},{"location":"Deep%20Learning/Week9-CNNs/9.3Multiple_Channel_Convolution/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.3Multiple_Channel_Convolution/#multiple-output-channels","title":"Multiple Output Channels","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.3Multiple_Channel_Convolution/#multiple-input-channels","title":"Multiple Input Channels","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.3Multiple_Channel_Convolution/#multiple-input-and-multiple-output-channels","title":"Multiple Input and Multiple Output Channels","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.3Multiple_Channel_Convolution/#practice-questions","title":"Practice Questions","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.3Multiple_Channel_Convolution/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/","title":"Simple CNN","text":"<p>author: Juma Shafara date: \"2024-08-12\" title: Simple Convolutional Neural Network keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, we will use a Convolutional Neral Networks to classify horizontal an vertical Lines</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p></p> <pre><code>import torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd\n</code></pre> <pre><code>torch.manual_seed(4)\n</code></pre> <pre>\n<code>&lt;torch._C.Generator at 0x70243022eeb0&gt;</code>\n</pre> <p>function to plot out the parameters of the Convolutional layers  </p> <pre><code>def plot_channels(W):\n    #number of output channels \n    n_out=W.shape[0]\n    #number of input channels \n    n_in=W.shape[1]\n    w_min=W.min().item()\n    w_max=W.max().item()\n    fig, axes = plt.subplots(n_out,n_in)\n    fig.subplots_adjust(hspace = 0.1)\n    out_index=0\n    in_index=0\n    #plot outputs as rows inputs as columns \n    for ax in axes.flat:\n\n        if in_index&amp;gt;n_in-1:\n            out_index=out_index+1\n            in_index=0\n\n        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        in_index=in_index+1\n\n    plt.show()\n</code></pre> <p><code>show_data</code>: plot out data sample</p> <pre><code>def show_data(dataset,sample):\n\n    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n    plt.title('y='+str(dataset.y[sample].item()))\n    plt.show()\n</code></pre> <p>create some toy data </p> <pre><code>from torch.utils.data import Dataset, DataLoader\nclass Data(Dataset):\n    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n        \"\"\"\n        p:portability that pixel is wight  \n        N_images:number of images \n        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n        \"\"\"\n        if train==True:\n            np.random.seed(1)  \n\n        #make images multiple of 3 \n        N_images=2*(N_images//2)\n        images=np.zeros((N_images,1,11,11))\n        start1=3\n        start2=1\n        self.y=torch.zeros(N_images).type(torch.long)\n\n        for n in range(N_images):\n            if offset&amp;gt;0:\n\n                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n            else:\n                low=4\n                high=1\n\n            if n&amp;lt;=N_images//2:\n                self.y[n]=0\n                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n            elif  n&amp;gt;N_images//2:\n                self.y[n]=1\n                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n\n\n\n        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n        self.len=self.x.shape[0]\n        del(images)\n        np.random.seed(0)\n    def __getitem__(self,index):      \n        return self.x[index],self.y[index]\n    def __len__(self):\n        return self.len\n</code></pre> <p><code>plot_activation</code>: plot out the activations of the Convolutional layers  </p> <pre><code>def plot_activations(A,number_rows= 1,name=\"\"):\n    A=A[0,:,:,:].detach().numpy()\n    n_activations=A.shape[0]\n\n\n    print(n_activations)\n    A_min=A.min().item()\n    A_max=A.max().item()\n\n    if n_activations==1:\n\n        # Plot the image.\n        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n\n    else:\n        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n        fig.subplots_adjust(hspace = 0.4)\n        for i,ax in enumerate(axes.flat):\n            if i&amp;lt; n_activations:\n                # Set the label for the sub-plot.\n                ax.set_xlabel( \"activation:{0}\".format(i+1))\n\n                # Plot the image.\n                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n                ax.set_xticks([])\n                ax.set_yticks([])\n    plt.show()\n</code></pre> <p>Utility function for computing output of convolutions takes a tuple of (h,w) and returns a tuple of (h,w)</p> <pre><code>def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n    #by Duane Nielsen\n    from math import floor\n    if type(kernel_size) is not tuple:\n        kernel_size = (kernel_size, kernel_size)\n    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n    return h, w\n</code></pre> <p></p> <p>Load the training dataset with 10000 samples </p> <pre><code>N_images=10000\ntrain_dataset=Data(N_images=N_images)\n</code></pre> <p>Load the testing dataset</p> <pre><code>validation_dataset=Data(N_images=1000,train=False)\nvalidation_dataset\n</code></pre> <pre>\n<code>&lt;__main__.Data at 0x7023c545ee10&gt;</code>\n</pre> <p>we can see the data type is long </p> <p>Each element in the rectangular  tensor corresponds to a number representing a pixel intensity  as demonstrated by  the following image.</p> <p>We can print out the third label </p> <pre><code>show_data(train_dataset,0)\n</code></pre> <pre><code>show_data(train_dataset,N_images//2+2)\n</code></pre> <p>we can plot the 3rd  sample </p> <p></p> <p>The input image is 11 x11, the following will change the size of the activations:</p> <ul> convolutional layer </ul> <ul> max pooling layer </ul> <ul> convolutional layer  </ul> <ul> max pooling layer  </ul> <p>with the following parameters <code>kernel_size</code>, <code>stride</code> and <code> pad</code>. We use the following  lines of code to change the image before we get tot he fully connected layer </p> <pre><code>out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out)\nout1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out1)\nout2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out2)\n\nout3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out3)\n</code></pre> <pre>\n<code>(10, 10)\n(9, 9)\n(8, 8)\n(7, 7)\n</code>\n</pre> <p>Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.</p> <pre><code>class CNN(nn.Module):\n    def __init__(self,out_1=2,out_2=1):\n\n        super(CNN,self).__init__()\n        #first Convolutional layers \n        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n\n        #second Convolutional layers\n        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n        #max pooling \n\n        #fully connected layer \n        self.fc1=nn.Linear(out_2*7*7,2)\n\n    def forward(self,x):\n        #first Convolutional layers\n        x=self.cnn1(x)\n        #activation function \n        x=torch.relu(x)\n        #max pooling \n        x=self.maxpool1(x)\n        #first Convolutional layers\n        x=self.cnn2(x)\n        #activation function\n        x=torch.relu(x)\n        #max pooling\n        x=self.maxpool2(x)\n        #flatten output \n        x=x.view(x.size(0),-1)\n        #fully connected layer\n        x=self.fc1(x)\n        return x\n\n    def activations(self,x):\n        #outputs activation this is not necessary just for fun \n        z1=self.cnn1(x)\n        a1=torch.relu(z1)\n        out=self.maxpool1(a1)\n\n        z2=self.cnn2(out)\n        a2=torch.relu(z2)\n        out=self.maxpool2(a2)\n        out=out.view(out.size(0),-1)\n        return z1,a1,z2,a2,out        \n</code></pre> <p></p> <p>There are 2 output channels for the first layer, and 1 outputs channel for the second layer </p> <pre><code>model=CNN(2,1)\n</code></pre> <p>we can see the model parameters with the object </p> <pre><code>model\n</code></pre> <pre>\n<code>CNN(\n  (cnn1): Conv2d(1, 2, kernel_size=(2, 2), stride=(1, 1))\n  (maxpool1): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n  (cnn2): Conv2d(2, 1, kernel_size=(2, 2), stride=(1, 1))\n  (maxpool2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=49, out_features=2, bias=True)\n)</code>\n</pre> <p>Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.</p> <pre><code>plot_channels(model.state_dict()['cnn1.weight'])\n</code></pre> <p>Loss function </p> <pre><code>plot_channels(model.state_dict()['cnn2.weight'])\n</code></pre> <p>Define the loss function </p> <pre><code>criterion=nn.CrossEntropyLoss()\n</code></pre> <p>optimizer class </p> <pre><code>learning_rate=0.001\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n</code></pre> <p>Define the optimizer class </p> <pre><code>train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\nvalidation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)\n</code></pre> <p>Train the model and determine validation accuracy technically test accuracy (This may take a long time)</p> <pre><code>n_epochs=10\ncost_list=[]\naccuracy_list=[]\nN_test=len(validation_dataset)\ncost=0\n#n_epochs\nfor epoch in range(n_epochs):\n    cost=0    \n    for x, y in train_loader:\n\n\n        #clear gradient \n        optimizer.zero_grad()\n        #make a prediction \n        z=model(x)\n        # calculate loss \n        loss=criterion(z,y)\n        # calculate gradients of parameters \n        loss.backward()\n        # update parameters \n        optimizer.step()\n        cost+=loss.item()\n    cost_list.append(cost)\n\n\n    correct=0\n    #perform a prediction on the validation  data  \n    for x_test, y_test in validation_loader:\n\n        z=model(x_test)\n        _,yhat=torch.max(z.data,1)\n\n        correct+=(yhat==y_test).sum().item()\n\n\n    accuracy=correct/N_test\n\n    accuracy_list.append(accuracy)\n</code></pre> <p>Plot the loss and accuracy on the validation data:</p> <pre><code>fig, ax1 = plt.subplots()\ncolor = 'tab:red'\nax1.plot(cost_list,color=color)\nax1.set_xlabel('epoch',color=color)\nax1.set_ylabel('total loss',color=color)\nax1.tick_params(axis='y', color=color)\n\nax2 = ax1.twinx()  \ncolor = 'tab:blue'\nax2.set_ylabel('accuracy', color=color)  \nax2.plot( accuracy_list, color=color)\nax2.tick_params(axis='y', labelcolor=color)\nfig.tight_layout()\n</code></pre> <p>View the results of the parameters for the Convolutional layers </p> <pre><code>model.state_dict()['cnn1.weight']\n</code></pre> <pre>\n<code>tensor([[[[ 0.3507,  0.4734],\n          [-0.1160, -0.1536]]],\n\n\n        [[[-0.4187, -0.2707],\n          [ 0.9412,  0.8749]]]])</code>\n</pre> <pre><code>plot_channels(model.state_dict()['cnn1.weight'])\n</code></pre> <pre><code>model.state_dict()['cnn1.weight']\n</code></pre> <pre>\n<code>tensor([[[[ 0.3507,  0.4734],\n          [-0.1160, -0.1536]]],\n\n\n        [[[-0.4187, -0.2707],\n          [ 0.9412,  0.8749]]]])</code>\n</pre> <pre><code>plot_channels(model.state_dict()['cnn2.weight'])\n</code></pre> <p>Consider the following sample </p> <pre><code>show_data(train_dataset,N_images//2+2)\n</code></pre> <p>Determine the activations </p> <pre><code>out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))\nout=model.activations(train_dataset[0][0].view(1,1,11,11))\n</code></pre> <p>Plot them out</p> <pre><code>plot_activations(out[0],number_rows=1,name=\" feature map\")\nplt.show()\n</code></pre> <pre>\n<code>2\n</code>\n</pre> <pre><code>plot_activations(out[2],number_rows=1,name=\"2nd feature map\")\nplt.show()\n</code></pre> <pre>\n<code>1\n</code>\n</pre> <pre><code>plot_activations(out[3],number_rows=1,name=\"first feature map\")\nplt.show()\n</code></pre> <pre>\n<code>1\n</code>\n</pre> <p>we save the output of the activation after flattening  </p> <pre><code>out1=out[4][0].detach().numpy()\n</code></pre> <p>we can do the same for a sample  where y=0 </p> <pre><code>out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\nout0\n</code></pre> <pre>\n<code>array([0.7374982 , 1.7757462 , 2.398145  , 2.4768693 , 2.4768693 ,\n       2.1022153 , 1.0242625 , 0.6254372 , 1.4152323 , 1.9039373 ,\n       2.0423164 , 2.0423164 , 1.8148925 , 1.0581893 , 0.6254372 ,\n       1.4152323 , 1.9821635 , 2.1456885 , 2.1456885 , 1.8400384 ,\n       1.0581893 , 0.67411214, 1.6115171 , 2.1684833 , 2.1684833 ,\n       2.1456885 , 1.8400384 , 0.96484905, 0.7374982 , 1.6366628 ,\n       2.1684833 , 2.1684833 , 2.1105773 , 1.618608  , 0.95454437,\n       0.7374982 , 1.6366628 , 2.0902567 , 2.0902567 , 2.0072055 ,\n       1.8148925 , 1.0581893 , 0.6254372 , 1.4422549 , 2.0730698 ,\n       2.178489  , 2.178489  , 1.99857   , 1.0581893 ], dtype=float32)</code>\n</pre> <pre><code>plt.subplot(2, 1, 1)\nplt.plot( out1, 'b')\nplt.title('Flatted Activation Values  ')\nplt.ylabel('Activation')\nplt.xlabel('index')\nplt.subplot(2, 1, 2)\nplt.plot(out0, 'r')\nplt.xlabel('index')\nplt.ylabel('Activation')\n</code></pre> <pre>\n<code>Text(0, 0.5, 'Activation')</code>\n</pre>"},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/#objective-for-this-notebook-1-learn-convolutional-neral-network-2-define-softmax-criterion-function-optimizer-and-train-the-model","title":"Objective for this Notebook","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/#1-learn-convolutional-neral-network","title":"1. Learn Convolutional Neral Network","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/#2-define-softmax-criterion-function-optimizer-and-train-the-model","title":"2. Define Softmax , Criterion function, Optimizer and Train the  Model","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, we will use a Convolutional Neral Networks to classify horizontal an vertical Lines </p> <li>Helper functions </li> <li> Prepare Data </li> <li>Convolutional Neral Network </li> <li>Define Softmax , Criterion function, Optimizer and Train the  Model</li> <li>Analyse Results</li> <p></p> Estimated Time Needed: 25 min"},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/#helper-functions","title":"Helper functions","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/#prepare-data","title":"Prepare Data","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/#data-visualization","title":"Data Visualization","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/#build-a-convolutional-neral-network-class","title":"Build a Convolutional Neral Network Class","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/#define-the-convolutional-neral-network-classifier-criterion-function-optimizer-and-train-the-model","title":"Define the Convolutional Neral Network Classifier , Criterion function, Optimizer and Train the  Model","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/#_1","title":"Simple CNN","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/#analyse-results","title":"Analyse Results","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/","title":"Small Image CNN","text":"<p>author: Juma Shafara date: \"2024-08-12\" title: Convolutional Neural Network with Small Images keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, we will use a Convolutional Neral Networks to classify horizontal an vertical Lines</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># Import the libraries we need to use in this lab\n\n# Using the following line code to install the torchvision library\n# !mamba install -y torchvision\n\n!pip install torchvision==0.9.1 torch==1.8.1 \nimport torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport matplotlib.pylab as plt\nimport numpy as np\n</code></pre> <pre>\n<code>ERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3\nERROR: Could not find a version that satisfies the requirement torchvision==0.9.1 (from versions: 0.17.0, 0.17.1, 0.17.2, 0.18.0, 0.18.1, 0.19.0)\nERROR: No matching distribution found for torchvision==0.9.1\n</code>\n</pre> <p>Define the function <code>plot_channels</code> to plot out the kernel parameters of  each channel </p> <pre><code># Define the function for plotting the channels\n\ndef plot_channels(W):\n    n_out = W.shape[0]\n    n_in = W.shape[1]\n    w_min = W.min().item()\n    w_max = W.max().item()\n    fig, axes = plt.subplots(n_out, n_in)\n    fig.subplots_adjust(hspace=0.1)\n    out_index = 0\n    in_index = 0\n\n    #plot outputs as rows inputs as columns \n    for ax in axes.flat:\n        if in_index &amp;gt; n_in-1:\n            out_index = out_index + 1\n            in_index = 0\n        ax.imshow(W[out_index, in_index, :, :], vmin=w_min, vmax=w_max, cmap='seismic')\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        in_index = in_index + 1\n\n    plt.show()\n</code></pre> <p>Define the function <code>plot_parameters</code> to plot out the kernel parameters of each channel with Multiple outputs . </p> <pre><code># Define the function for plotting the parameters\n\ndef plot_parameters(W, number_rows=1, name=\"\", i=0):\n    W = W.data[:, i, :, :]\n    n_filters = W.shape[0]\n    w_min = W.min().item()\n    w_max = W.max().item()\n    fig, axes = plt.subplots(number_rows, n_filters // number_rows)\n    fig.subplots_adjust(hspace=0.4)\n\n    for i, ax in enumerate(axes.flat):\n        if i &amp;lt; n_filters:\n            # Set the label for the sub-plot.\n            ax.set_xlabel(\"kernel:{0}\".format(i + 1))\n\n            # Plot the image.\n            ax.imshow(W[i, :], vmin=w_min, vmax=w_max, cmap='seismic')\n            ax.set_xticks([])\n            ax.set_yticks([])\n    plt.suptitle(name, fontsize=10)    \n    plt.show()\n</code></pre> <p>Define the function <code>plot_activation</code> to plot out the activations of the Convolutional layers  </p> <pre><code># Define the function for plotting the activations\n\ndef plot_activations(A, number_rows=1, name=\"\", i=0):\n    A = A[0, :, :, :].detach().numpy()\n    n_activations = A.shape[0]\n    A_min = A.min().item()\n    A_max = A.max().item()\n    fig, axes = plt.subplots(number_rows, n_activations // number_rows)\n    fig.subplots_adjust(hspace = 0.4)\n\n    for i, ax in enumerate(axes.flat):\n        if i &amp;lt; n_activations:\n            # Set the label for the sub-plot.\n            ax.set_xlabel(\"activation:{0}\".format(i + 1))\n\n            # Plot the image.\n            ax.imshow(A[i, :], vmin=A_min, vmax=A_max, cmap='seismic')\n            ax.set_xticks([])\n            ax.set_yticks([])\n    plt.show()\n</code></pre> <p>Define the function <code>show_data</code> to plot out data samples as images.</p> <pre><code>def show_data(data_sample):\n    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n    plt.title('y = '+ str(data_sample[1]))\n</code></pre> <p>we create a transform to resize the image and convert it to a tensor .</p> <pre><code>IMAGE_SIZE = 16\n\ncomposed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])\n</code></pre> <p>Load the training dataset by setting the parameters <code>train </code> to <code>True</code>. We use the transform defined above.</p> <pre><code>train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed)\n</code></pre> <pre>\n<code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n</code>\n</pre> <pre>\n<code>100.0%\n</code>\n</pre> <pre>\n<code>Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n</code>\n</pre> <pre>\n<code>100.0%\n</code>\n</pre> <pre>\n<code>Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n</code>\n</pre> <pre>\n<code>100.0%\n</code>\n</pre> <pre>\n<code>Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n</code>\n</pre> <pre>\n<code>100.0%</code>\n</pre> <pre>\n<code>Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n</code>\n</pre> <pre>\n<code>\n</code>\n</pre> <p>Load the testing dataset by setting the parameters train  <code>False</code>.</p> <pre><code># Make the validating \n\nvalidation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed)\n</code></pre> <p>We can see the data type is long.</p> <pre><code># Show the data type for each element in dataset\n\ntype(train_dataset[0][1])\n</code></pre> <pre>\n<code>int</code>\n</pre> <p>Each element in the rectangular tensor corresponds to a number representing a pixel intensity as demonstrated by the following image.</p> <p></p> <p>Print out the fourth label </p> <pre><code># The label for the fourth data element\n\ntrain_dataset[3][1]\n</code></pre> <pre>\n<code>1</code>\n</pre> <p>Plot the fourth sample </p> <pre><code># The image for the fourth data element\nshow_data(train_dataset[3])\n</code></pre> <p>The fourth sample is a \"1\".</p> <p>Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.</p> <pre><code>class CNN(nn.Module):\n\n    # Contructor\n    def __init__(self, out_1=16, out_2=32):\n        super(CNN, self).__init__()\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n\n        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)\n\n    # Prediction\n    def forward(self, x):\n        x = self.cnn1(x)\n        x = torch.relu(x)\n        x = self.maxpool1(x)\n        x = self.cnn2(x)\n        x = torch.relu(x)\n        x = self.maxpool2(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        return x\n\n    # Outputs in each steps\n    def activations(self, x):\n        #outputs activation this is not necessary\n        z1 = self.cnn1(x)\n        a1 = torch.relu(z1)\n        out = self.maxpool1(a1)\n\n        z2 = self.cnn2(out)\n        a2 = torch.relu(z2)\n        out1 = self.maxpool2(a2)\n        out = out.view(out.size(0),-1)\n        return z1, a1, z2, a2, out1,out\n</code></pre> <p>There are 16 output channels for the first layer, and 32 output channels for the second layer </p> <pre><code># Create the model object using CNN class\n\nmodel = CNN(out_1=16, out_2=32)\n</code></pre> <p>Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.</p> <pre><code># Plot the parameters\n\nplot_parameters(model.state_dict()['cnn1.weight'], number_rows=4, name=\"1st layer kernels before training \")\nplot_parameters(model.state_dict()['cnn2.weight'], number_rows=4, name='2nd layer kernels before training' )\n</code></pre> <p>Define the loss function, the optimizer and the dataset loader </p> <pre><code>criterion = nn.CrossEntropyLoss()\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)\n</code></pre> <p>Train the model and determine validation accuracy technically test accuracy (This may take a long time)</p> <pre><code># Train the model\n\nn_epochs=3\ncost_list=[]\naccuracy_list=[]\nN_test=len(validation_dataset)\nCOST=0\n\ndef train_model(n_epochs):\n    for epoch in range(n_epochs):\n        COST=0\n        for x, y in train_loader:\n            optimizer.zero_grad()\n            z = model(x)\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n            COST+=loss.data\n\n        cost_list.append(COST)\n        correct=0\n        #perform a prediction on the validation  data  \n        for x_test, y_test in validation_loader:\n            z = model(x_test)\n            _, yhat = torch.max(z.data, 1)\n            correct += (yhat == y_test).sum().item()\n        accuracy = correct / N_test\n        accuracy_list.append(accuracy)\n\ntrain_model(n_epochs)\n</code></pre> <p>Plot the loss and accuracy on the validation data:</p> <pre><code># Plot the loss and accuracy\n\nfig, ax1 = plt.subplots()\ncolor = 'tab:red'\nax1.plot(cost_list, color=color)\nax1.set_xlabel('epoch', color=color)\nax1.set_ylabel('Cost', color=color)\nax1.tick_params(axis='y', color=color)\n\nax2 = ax1.twinx()  \ncolor = 'tab:blue'\nax2.set_ylabel('accuracy', color=color) \nax2.set_xlabel('epoch', color=color)\nax2.plot( accuracy_list, color=color)\nax2.tick_params(axis='y', color=color)\nfig.tight_layout()\n</code></pre> <p>View the results of the parameters for the Convolutional layers </p> <pre><code># Plot the channels\n\nplot_channels(model.state_dict()['cnn1.weight'])\nplot_channels(model.state_dict()['cnn2.weight'])\n</code></pre> <p>Consider the following sample </p> <pre><code># Show the second image\n\nshow_data(train_dataset[1])\n</code></pre> <p>Determine the activations </p> <pre><code># Use the CNN activations class to see the steps\n\nout = model.activations(train_dataset[1][0].view(1, 1, IMAGE_SIZE, IMAGE_SIZE))\n</code></pre> <p>Plot out the first set of activations </p> <pre><code># Plot the outputs after the first CNN\n\nplot_activations(out[0], number_rows=4, name=\"Output after the 1st CNN\")\n</code></pre> <p>The image below is the result after applying the relu activation function </p> <pre><code># Plot the outputs after the first Relu\n\nplot_activations(out[1], number_rows=4, name=\"Output after the 1st Relu\")\n</code></pre> <p>The image below is the result of the activation map after the second output layer.</p> <pre><code># Plot the outputs after the second CNN\n\nplot_activations(out[2], number_rows=32 // 4, name=\"Output after the 2nd CNN\")\n</code></pre> <p>The image below is the result of the activation map after applying the second relu  </p> <pre><code># Plot the outputs after the second Relu\n\nplot_activations(out[3], number_rows=4, name=\"Output after the 2nd Relu\")\n</code></pre> <p>We can  see the result for the third sample </p> <pre><code># Show the third image\n\nshow_data(train_dataset[2])\n</code></pre> <pre><code># Use the CNN activations class to see the steps\n\nout = model.activations(train_dataset[2][0].view(1, 1, IMAGE_SIZE, IMAGE_SIZE))\n</code></pre> <pre><code># Plot the outputs after the first CNN\n\nplot_activations(out[0], number_rows=4, name=\"Output after the 1st CNN\")\n</code></pre> <pre><code># Plot the outputs after the first Relu\n\nplot_activations(out[1], number_rows=4, name=\"Output after the 1st Relu\")\n</code></pre> <pre><code># Plot the outputs after the second CNN\n\nplot_activations(out[2], number_rows=32 // 4, name=\"Output after the 2nd CNN\")\n</code></pre> <pre><code># Plot the outputs after the second Relu\n\nplot_activations(out[3], number_rows=4, name=\"Output after the 2nd Relu\")\n</code></pre> <p>Plot the first five mis-classified samples:</p> <pre><code># Plot the mis-classified samples\n\ncount = 0\nfor x, y in torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=1):\n    z = model(x)\n    _, yhat = torch.max(z, 1)\n    if yhat != y:\n        show_data((x, y))\n        plt.show()\n        print(\"yhat: \",yhat)\n        count += 1\n    if count &amp;gt;= 5:\n        break  \n</code></pre> <pre>\n<code>yhat:  tensor([3])\n</code>\n</pre> <pre>\n<code>yhat:  tensor([5])\n</code>\n</pre> <pre>\n<code>yhat:  tensor([2])\n</code>\n</pre> <pre>\n<code>yhat:  tensor([0])\n</code>\n</pre> <pre>\n<code>yhat:  tensor([4])\n</code>\n</pre>"},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/#objective-for-this-notebook-1-learn-how-to-use-a-convolutional-neural-network-to-classify-handwritten-digits-from-the-mnist-database-2-learn-hot-to-reshape-the-images-to-make-them-faster-to-process","title":"Objective for this Notebook","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/#1-learn-how-to-use-a-convolutional-neural-network-to-classify-handwritten-digits-from-the-mnist-database","title":"1. Learn how to use a Convolutional Neural Network to classify handwritten digits from the MNIST database","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/#2-learn-hot-to-reshape-the-images-to-make-them-faster-to-process","title":"2. Learn hot to reshape the images to make them faster to process","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, we will use a Convolutional Neural Network to classify handwritten digits from the MNIST database. We will reshape the images to make them faster to process </p> <ul> <li>Get Some Data</li> <li>Convolutional Neural Network</li> <li>Define Softmax, Criterion function, Optimizer and Train the Model</li> <li>Analyze Results</li> </ul> <p>Estimated Time Needed: 25 min 14 min to train model </p>"},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/#Makeup_Data","title":"Get the Data","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/#CNN","title":"Build a Convolutional Neural Network Class","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/#Train","title":"Define the Convolutional Neural Network Classifier, Criterion function, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/#Result","title":"Analyze Results","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/","title":"Batch Processing","text":"<p>author: Juma Shafara date: \"2024-08-12\" title: Convolutional Neural Network with Batch-Normalization keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, we will compare a Convolutional Neural Network using Batch Normalization with a regular Convolutional Neural Network</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>Like a fully connected network, we create a <code>BatchNorm2d</code> object, but we apply it to the 2D convolution object. First, we create objects <code>Conv2d</code> object; we require the number of output channels, specified by the variable <code>OUT</code>.  </p> <p><code>self.cnn1 = nn.Conv2d(in_channels=1, out_channels=OUT, kernel_size=5, padding=2) </code></p> <p>We then create a Batch Norm  object for 2D convolution as follows:</p> <p><code>self.conv1_bn = nn.BatchNorm2d(OUT)</code></p> <p>The parameter out is the number of channels in the output. We can then apply batch norm  after  the convolution operation :</p> <p><code>x = self.cnn1(x)</code></p> <p></p> <p><code> x=self.conv1_bn(x)</code></p> <pre><code># Import the libraries we need to use in this lab\n\n# Using the following line code to install the torchvision library\n# !mamba install -y torchvision\n\n!pip install torchvision==0.9.1 torch==1.8.1 \nimport torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport matplotlib.pylab as plt\nimport numpy as np\ndef show_data(data_sample):\n    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n    plt.title('y = '+ str(data_sample[1]))\n</code></pre> <p>we create a transform to resize the image and convert it to a tensor :</p> <pre><code>IMAGE_SIZE = 16\n\ncomposed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])\n</code></pre> <p>Load the training dataset by setting the parameters <code>train </code> to <code>True</code>. We use the transform defined above.</p> <pre><code>train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed)\n</code></pre> <p>Load the testing dataset by setting the parameters train  <code>False</code>.</p> <pre><code># Make the validating \n\nvalidation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed)\n</code></pre> <p>We can see the data type is long.</p> <pre><code># Show the data type for each element in dataset\n\ntype(train_dataset[0][1])\n</code></pre> <p>Each element in the rectangular tensor corresponds to a number representing a pixel intensity as demonstrated by the following image.</p> <p></p> <p>Print out the fourth label </p> <pre><code># The label for the fourth data element\n\ntrain_dataset[3][1]\n</code></pre> <p>Plot the fourth sample </p> <pre><code># The image for the fourth data element\nshow_data(train_dataset[3])\n</code></pre> <p>The fourth sample is a \"1\".</p> <p>Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.</p> <pre><code>class CNN(nn.Module):\n\n    # Contructor\n    def __init__(self, out_1=16, out_2=32):\n        super(CNN, self).__init__()\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n\n        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)\n\n    # Prediction\n    def forward(self, x):\n        x = self.cnn1(x)\n        x = torch.relu(x)\n        x = self.maxpool1(x)\n        x = self.cnn2(x)\n        x = torch.relu(x)\n        x = self.maxpool2(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        return x\n</code></pre> <p>Build a Convolutional Network class with two Convolutional layers and one fully connected layer. But we add Batch Norm for the convolutional layers. </p> <pre><code>class CNN_batch(nn.Module):\n\n    # Contructor\n    def __init__(self, out_1=16, out_2=32,number_of_classes=10):\n        super(CNN_batch, self).__init__()\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n        self.conv1_bn = nn.BatchNorm2d(out_1)\n\n        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n\n        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n        self.conv2_bn = nn.BatchNorm2d(out_2)\n\n        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n        self.fc1 = nn.Linear(out_2 * 4 * 4, number_of_classes)\n        self.bn_fc1 = nn.BatchNorm1d(10)\n\n    # Prediction\n    def forward(self, x):\n        x = self.cnn1(x)\n        x=self.conv1_bn(x)\n        x = torch.relu(x)\n        x = self.maxpool1(x)\n        x = self.cnn2(x)\n        x=self.conv2_bn(x)\n        x = torch.relu(x)\n        x = self.maxpool2(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x=self.bn_fc1(x)\n        return x\n</code></pre> <p>Function to train the model </p> <pre><code>def train_model(model,train_loader,validation_loader,optimizer,n_epochs=4):\n\n    #global variable \n    N_test=len(validation_dataset)\n    accuracy_list=[]\n    loss_list=[]\n    for epoch in range(n_epochs):\n        for x, y in train_loader:\n            model.train()\n            optimizer.zero_grad()\n            z = model(x)\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n            loss_list.append(loss.data)\n\n        correct=0\n        #perform a prediction on the validation  data  \n        for x_test, y_test in validation_loader:\n            model.eval()\n            z = model(x_test)\n            _, yhat = torch.max(z.data, 1)\n            correct += (yhat == y_test).sum().item()\n        accuracy = correct / N_test\n        accuracy_list.append(accuracy)\n\n    return accuracy_list, loss_list\n</code></pre> <p>There are 16 output channels for the first layer, and 32 output channels for the second layer </p> <pre><code># Create the model object using CNN class\nmodel = CNN(out_1=16, out_2=32)\n</code></pre> <p>Define the loss function, the optimizer and the dataset loader </p> <pre><code>criterion = nn.CrossEntropyLoss()\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)\n</code></pre> <p>Train the model and determine validation accuracy technically test accuracy (This may take a long time)</p> <pre><code># Train the model\naccuracy_list_normal, loss_list_normal=train_model(model=model,n_epochs=10,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)\n</code></pre> <p>Repeat the Process for the model with  batch norm </p> <pre><code>model_batch=CNN_batch(out_1=16, out_2=32)\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(model_batch.parameters(), lr = learning_rate)\naccuracy_list_batch, loss_list_batch=train_model(model=model_batch,n_epochs=10,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)\n</code></pre> <p>Plot the loss with both networks.</p> <pre><code># Plot the loss and accuracy\n\nplt.plot(loss_list_normal, 'b',label='loss normal cnn ')\nplt.plot(loss_list_batch,'r',label='loss batch cnn')\nplt.xlabel('iteration')\nplt.title(\"loss\")\nplt.legend()\n</code></pre> <pre><code>plt.plot(accuracy_list_normal, 'b',label=' normal CNN')\nplt.plot(accuracy_list_batch,'r',label=' CNN with Batch Norm')\nplt.xlabel('Epoch')\nplt.title(\"Accuracy \")\nplt.legend()\nplt.show()\n</code></pre> <p>We see the CNN with batch norm performers better, with faster convergence.</p>"},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/#objective-for-this-notebook-1-learn-how-to-compare-a-convolutional-neural-network-using-batch-normalization-with-a-regular-convolutional-neural-network-to-classify-handwritten-digits-from-the-mnist-database","title":"Objective for this Notebook","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/#1-learn-how-to-compare-a-convolutional-neural-network-using-batch-normalization-with-a-regular-convolutional-neural-network-to-classify-handwritten-digits-from-the-mnist-database","title":"1. Learn how to compare a Convolutional Neural Network using Batch Normalization with a regular Convolutional Neural Network  to classify handwritten digits from the MNIST database..","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/#table-of-contents","title":"Table of Contents","text":"<p>This lab takes a long time to run so the results are given. You can run the notebook your self but it may take a long time.</p> <p>In this lab, we will compare a Convolutional Neural Network using Batch Normalization with a regular Convolutional Neural Network  to classify handwritten digits from the MNIST database. We will reshape the images to make them faster to process. </p> <ul> <li>Read me Batch Norm for Convolution Operation  </li> <li>Get Some Data</li> <li>Two Types of Convolutional Neural Network</li> <li>Define Criterion function, Optimizer and Train the Model</li> <li>Analyze Results</li> </ul> <p>Estimated Time Needed: 25 min</p>"},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/#read_me","title":"Read me Batch Norm for Convolution Operation","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/#preparation","title":"Preparation","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/#Makeup_Data","title":"Get the Data","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/#CNN","title":"Build a Two Convolutional Neural Network Class","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/#Train","title":"Define the Convolutional Neural Network Classifier, Criterion function, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/#Result","title":"Analyze Results","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Extras/classification_metrics/","title":"Classification Metrics","text":"<p>title: Classification Metrics Practice author: Juma Shafara date: \"2024-02\" date-modified: \"2024-07-25\" keywords: [machine learning, machine learning classification, machine learning classification metrics, decision trees, python, precision, recall, f1 score, weighted, accuracy, linear regression] description: Learn Programming for Data Science. Demonstrate loading, preparing, training, and evaluating a machine learning model using the Iris dataset  </p> <p></p> <p>In this notebook, we'll walk through the process of building and evaluating a decision tree classifier using Scikit-Learn. We'll use the Iris dataset for demonstration and then provide an exercise to apply the same steps to the Wine dataset.</p> Don't Miss Any Updates! <p> To be among the first to hear about future updates of the course materials, simply enter your email below, follow us on   (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n</code></pre> <ul> <li><code>numpy</code> and <code>pandas</code> are imported for data manipulation.</li> <li><code>load_iris</code> from <code>sklearn.datasets</code> is imported to load the Iris dataset.</li> </ul> <pre><code>iris = load_iris()\n</code></pre> <p>The Iris dataset is loaded and stored in the variable iris.</p> <pre><code>## uncomment and run to read the data description\n# print(iris['DESCR'])\n</code></pre> <pre><code>X = iris['data']\ny = iris['target']\n</code></pre> <ul> <li>X contains the feature data (sepal length, sepal width, petal length, petal width).</li> <li>y contains the target data (class labels: 0, 1, 2).</li> </ul> <pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <p><code>train_test_split</code> is imported to split the data into training and testing sets.</p> <pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n</code></pre> <p>The dataset is split into training (70%) and testing (30%) sets.</p> <pre><code>from sklearn.tree import DecisionTreeClassifier\n</code></pre> <pre><code>classifier = DecisionTreeClassifier()\n</code></pre> <pre><code>classifier.fit(X_train, y_train)\n</code></pre> <pre>DecisionTreeClassifier()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. <p>\u00a0\u00a0DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFitted<pre>DecisionTreeClassifier()</pre> <p> </p> <pre><code>preds = classifier.predict(X_test)\n</code></pre> <pre><code>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n</code></pre> <pre><code>accuracy_score(y_test, preds)\n</code></pre> <pre>\n<code>0.9777777777777777</code>\n</pre> <pre><code>precision_score(y_test, preds, average='weighted')\n</code></pre> <pre>\n<code>0.9794871794871796</code>\n</pre> <pre><code>recall_score(y_test, preds, average='weighted')\n</code></pre> <pre>\n<code>0.9777777777777777</code>\n</pre> <pre><code>f1_score(y_test, preds, average='weighted')\n</code></pre> <pre>\n<code>0.977863799283154</code>\n</pre> <pre><code>from sklearn.metrics import classification_report\n</code></pre> <pre><code>classification_report = classification_report(y_test, preds)\nprint(classification_report)\n</code></pre> <pre>\n<code>              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        17\n           1       1.00      0.94      0.97        16\n           2       0.92      1.00      0.96        12\n\n    accuracy                           0.98        45\n   macro avg       0.97      0.98      0.98        45\nweighted avg       0.98      0.98      0.98        45\n\n</code>\n</pre> <p>The results show how well the model performs in classifying the iris species, with metrics providing insights into different aspects of the model's performance.</p> <pre><code>from sklearn.metrics import confusion_matrix\n</code></pre> <pre><code>conf_matrix = confusion_matrix(y_test, preds)\nconf_matrix = pd.DataFrame(conf_matrix, index=[0, 1, 2], columns=[0, 1, 2])\n# print(\"Confusion Matrix:\\n\", conf_matrix)\nconf_matrix\n</code></pre> 0 1 2 0 17 0 0 1 0 15 1 2 0 0 12"},{"location":"Extras/classification_metrics/#importing-necessary-libraries","title":"Importing Necessary Libraries","text":"<p>First, we import the necessary libraries for data manipulation and loading the dataset.</p>"},{"location":"Extras/classification_metrics/#loading-the-iris-dataset","title":"Loading the Iris Dataset","text":""},{"location":"Extras/classification_metrics/#displaying-dataset-description","title":"Displaying Dataset Description","text":"<p>For a better understanding of the dataset, we can uncomment the following line to print the description of the Iris dataset.</p>"},{"location":"Extras/classification_metrics/#extracting-features-and-target-variables","title":"Extracting Features and Target Variables","text":""},{"location":"Extras/classification_metrics/#importing-train-test-split-function","title":"Importing Train-Test Split Function","text":""},{"location":"Extras/classification_metrics/#splitting-the-data","title":"Splitting the Data","text":""},{"location":"Extras/classification_metrics/#importing-decision-tree-classifier","title":"Importing Decision Tree Classifier","text":"<p>Next, we import the Decision Tree classifier from Scikit-Learn.</p>"},{"location":"Extras/classification_metrics/#initializing-the-classifier","title":"Initializing the Classifier","text":"<p>We create an instance of the Decision Tree classifier</p>"},{"location":"Extras/classification_metrics/#training-the-classifier","title":"Training the Classifier","text":"<p>We train the classifier using the training data.</p>"},{"location":"Extras/classification_metrics/#making-predictions","title":"Making Predictions","text":"<p>We then make predictions on the test data using the the <code>predict()</code> method on the model</p>"},{"location":"Extras/classification_metrics/#importing-metrics-for-evaluation","title":"Importing Metrics for Evaluation","text":"<p>To evaluate our model, we import various metrics from Scikit-Learn.</p>"},{"location":"Extras/classification_metrics/#calculating-accuracy","title":"Calculating Accuracy","text":"<p>Accuracy refers to the proportion of correctly predicted instances out of the total instances.</p>"},{"location":"Extras/classification_metrics/#calculating-precision","title":"Calculating Precision","text":"<p>Precision is the ratio of correctly predicted positive observations to the total predicted positives.</p>"},{"location":"Extras/classification_metrics/#calculating-recall","title":"Calculating Recall","text":"<p>Recall is the ratio of correctly predicted positive observations to all the actual positives.</p>"},{"location":"Extras/classification_metrics/#calculating-f1-score","title":"Calculating F1 Score","text":"<p>The f1 score refers to the Harmonic mean of Precision and Recall.</p>"},{"location":"Extras/classification_metrics/#displaying-the-classification-report","title":"Displaying the Classification Report","text":"<p>We can print the classification report, which provides precision, recall, F1-score, and support for each class.</p>"},{"location":"Extras/classification_metrics/#exercise","title":"Exercise:","text":"<p>Perform the steps above using the wine dataset from sklearn</p>"},{"location":"Extras/classification_metrics/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Extras/customer_analysis/","title":"Customer Analysis","text":"<p>title: Customer Analysis keywords: [Customer Analysis, Data Visualization, Matplotlib, Pandas, Opendatasets] description: In this notebook, I want to observe any trends related to customers. author: Juma Shafara date: \"2024-07-08\"</p> <p>In this project, I want to look at customer data pulled from github and create some visuals in my jupyter notebook to observe any trends related to customers.</p> Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on   (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code>!pip install dataidea --upgrade --quiet\n</code></pre> <pre><code>import opendatasets as od\n\n# download the dataset\ndataset_url = 'https://raw.githubusercontent.com/Kaushik-Varma/Marketing_Data_Analysis/master/Marketing_Analysis.csv'\nod.download(dataset_url)\n</code></pre> <pre>\n<code>Using downloaded and verified file: ./Marketing_Analysis.csv\n</code>\n</pre> <pre><code>#import the useful libraries.\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Read the data set of \"Marketing Analysis\" in data.\ndata= pd.read_csv(\"Marketing_Analysis.csv\", low_memory=False)\n\n# Printing the data\ndata.head()\n</code></pre> banking marketing Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15 Unnamed: 16 Unnamed: 17 Unnamed: 18 0 customer id and age. NaN Customer salary and balance. NaN Customer marital status and job with education... NaN particular customer before targeted or not NaN Loan types: loans or housing loans NaN Contact type NaN month of contact duration of call NaN NaN NaN outcome of previous contact response of customer after call happned 1 customerid age salary balance marital jobedu targeted default housing loan contact day month duration campaign pdays previous poutcome response 2 1 58 100000 2143 married management,tertiary yes no yes no unknown 5 may, 2017 261 sec 1 -1 0 unknown no 3 2 44 60000 29 single technician,secondary yes no yes no unknown 5 may, 2017 151 sec 1 -1 0 unknown no 4 3 33 120000 2 married entrepreneur,secondary yes no yes yes unknown 5 may, 2017 76 sec 1 -1 0 unknown no <pre><code>#import the useful libraries.\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Read the file in data without first two rows as it is of no use.\ndata = pd.read_csv(\"Marketing_Analysis.csv\",skiprows = 2)\n\n#print the head of the data frame.\ndata.head()\n</code></pre> customerid age salary balance marital jobedu targeted default housing loan contact day month duration campaign pdays previous poutcome response 0 1 58.0 100000 2143 married management,tertiary yes no yes no unknown 5 may, 2017 261 sec 1 -1 0 unknown no 1 2 44.0 60000 29 single technician,secondary yes no yes no unknown 5 may, 2017 151 sec 1 -1 0 unknown no 2 3 33.0 120000 2 married entrepreneur,secondary yes no yes yes unknown 5 may, 2017 76 sec 1 -1 0 unknown no 3 4 47.0 20000 1506 married blue-collar,unknown no no yes no unknown 5 may, 2017 92 sec 1 -1 0 unknown no 4 5 33.0 0 1 single unknown,unknown no no no no unknown 5 may, 2017 198 sec 1 -1 0 unknown no <pre><code># Drop the customer id as it is of no use.\ndata.drop('customerid', axis = 1, inplace = True)\n\n#Extract job  &amp;amp; Education in newly from \"jobedu\" column.\ndata['job']= data[\"jobedu\"].apply(lambda x: x.split(\",\")[0])\ndata['education']= data[\"jobedu\"].apply(lambda x: x.split(\",\")[1])\n\n# Drop the \"jobedu\" column from the dataframe.\ndata.drop('jobedu', axis = 1, inplace = True)\n\n# Printing the Dataset\ndata.sample(n=5)\n</code></pre> age salary balance marital targeted default housing loan contact day month duration campaign pdays previous poutcome response job education 7369 28.0 60000 1180 married yes no yes no unknown 29 may, 2017 637 sec 3 -1 0 unknown no technician secondary 31281 44.0 100000 483 single no no no no cellular 6 mar, 2017 3.45 min 2 199 6 success yes management tertiary 736 40.0 20000 -7 married yes no yes no unknown 6 may, 2017 410 sec 2 -1 0 unknown no blue-collar primary 45207 71.0 55000 1729 divorced yes no no no cellular 17 nov, 2017 7.6 min 2 -1 0 unknown yes retired primary 6297 53.0 60000 6 married yes no yes no unknown 27 may, 2017 233 sec 2 -1 0 unknown no self-employed primary <pre><code># Checking the missing values\ndata.isnull().sum()\n</code></pre> <pre>\n<code>age          20\nsalary        0\nbalance       0\nmarital       0\ntargeted      0\ndefault       0\nhousing       0\nloan          0\ncontact       0\nday           0\nmonth        50\nduration      0\ncampaign      0\npdays         0\nprevious      0\npoutcome      0\nresponse     30\njob           0\neducation     0\ndtype: int64</code>\n</pre> <pre><code># Dropping the records with age missing in data dataframe.\ndata = data[~data.age.isnull()].copy()\n\n# Checking the missing values in the dataset.\ndata.isnull().sum()\n</code></pre> <pre>\n<code>age           0\nsalary        0\nbalance       0\nmarital       0\ntargeted      0\ndefault       0\nhousing       0\nloan          0\ncontact       0\nday           0\nmonth        50\nduration      0\ncampaign      0\npdays         0\nprevious      0\npoutcome      0\nresponse     30\njob           0\neducation     0\ndtype: int64</code>\n</pre> <pre><code># Find the mode of month in data\nmonth_mode = data.month.mode()[0]\n\n# Fill the missing values with mode value of month in data.\ndata.month.fillna(month_mode, inplace = True)\n\n# Let's see the null values in the month column.\ndata.month.isnull().sum()\n</code></pre> <pre>\n<code>/tmp/ipykernel_39902/3697544734.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  data.month.fillna(month_mode, inplace = True)\n</code>\n</pre> <pre>\n<code>0</code>\n</pre> <pre><code>#drop the records with response missing in data.\ndata = data[~data.response.isnull()].copy()\n# Calculate the missing values in each column of data frame\ndata.isnull().sum()\n</code></pre> <pre>\n<code>age          0\nsalary       0\nbalance      0\nmarital      0\ntargeted     0\ndefault      0\nhousing      0\nloan         0\ncontact      0\nday          0\nmonth        0\nduration     0\ncampaign     0\npdays        0\nprevious     0\npoutcome     0\nresponse     0\njob          0\neducation    0\ndtype: int64</code>\n</pre> <pre><code># Let's calculate the percentage of each job status category.\ndata.job.value_counts(normalize=True)\n\n#plot the bar graph of percentage job categories\ndata.job.value_counts(normalize=True).plot.barh()\nplt.show()\n</code></pre> <pre><code>#calculate the percentage of each education category.\ndata.education.value_counts(normalize=True)\n\n#plot the pie chart of education categories\ndata.education.value_counts(normalize=True).plot.pie()\nplt.show()\n</code></pre> <pre><code>data.salary.describe()\n</code></pre> <pre>\n<code>count     45161.000000\nmean      57004.849317\nstd       32087.698810\nmin           0.000000\n25%       20000.000000\n50%       60000.000000\n75%       70000.000000\nmax      120000.000000\nName: salary, dtype: float64</code>\n</pre> <pre><code>#plot the scatter plot of balance and salary variable in data\nplt.scatter(data.salary,data.balance)\nplt.show()\n\n#plot the scatter plot of balance and age variable in data\ndata.plot.scatter(x=\"age\",y=\"balance\")\nplt.show()\n</code></pre> <pre><code>#plot the pair plot of salary, balance and age in data dataframe.\nsns.pairplot(data = data, vars=['salary','balance','age'])\nplt.show()\n</code></pre> <pre><code># Creating a matrix using age, salary, balance as rows and columns\ndata[['age','salary','balance']].corr()\n\n#plot the correlation matrix of salary, balance and age in data dataframe.\nsns.heatmap(data[['age','salary','balance']].corr(), annot=True, cmap = 'Greens')\nplt.show()\n</code></pre> <pre><code>#create response_rate of numerical data type where response \"yes\"= 1, \"no\"= 0\ndata['response_rate'] = np.where(data.response=='yes',1,0)\ndata.response_rate.value_counts()\n</code></pre> <pre>\n<code>response_rate\n0    39876\n1     5285\nName: count, dtype: int64</code>\n</pre> <pre><code>#plot the bar graph of marital status with average value of response_rate\ndata.groupby('marital')['response_rate'].mean().plot.bar()\nplt.show()\n</code></pre> <pre><code>result = pd.pivot_table(data=data, index='education', columns='marital',values='response_rate')\nprint(result)\n\n#create heat map of to show correlations betwenn education vs marital vs response_rate\nsns.heatmap(result, annot=True, cmap = 'RdYlGn', center=0.117)\nplt.show()\n</code></pre> <pre>\n<code>marital    divorced   married    single\neducation                              \nprimary    0.138852  0.075601  0.106808\nsecondary  0.103559  0.094650  0.129271\ntertiary   0.137415  0.129835  0.183737\nunknown    0.142012  0.122519  0.162879\n</code>\n</pre> <pre><code>#plot the bar graph of age groups with average salary for that group\nbins = [18, 30, 40, 50, 60, 70, 120]\nlabels = ['18-29', '30-39', '40-49', '50-59', '60-69', '70+']\ndata['agerange'] = pd.cut(data.age, bins, labels = labels,include_lowest = True)\n\n#plot the bar graph of average salary per age group\ndata.groupby('agerange')['salary'].mean().plot.bar()\nplt.title('Avg Salary per Age',fontsize = 12)\nplt.show()\n</code></pre> <pre>\n<code>/tmp/ipykernel_39902/506738209.py:7: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  data.groupby('agerange')['salary'].mean().plot.bar()\n</code>\n</pre> <p>Let us save and upload our work to Jovian before finishing up.</p> <pre><code>\n</code></pre>"},{"location":"Extras/customer_analysis/#downloading-the-dataset","title":"Downloading the Dataset:","text":"<p>First we download our data sets from github.</p>"},{"location":"Extras/customer_analysis/#data-preparation-and-cleaning","title":"Data Preparation and Cleaning","text":"<p>Get our dataset into a data frame, examine the tables to check for incorrect, inconsistent, or invalid entries. Handle other cleaning steps as necessary.</p>"},{"location":"Extras/customer_analysis/#cleaning-the-data","title":"Cleaning the Data","text":"<p>Here we need to fix some of the columns/rows to make the data easier to use.</p>"},{"location":"Extras/customer_analysis/#exploratory-analysis-and-visualization","title":"Exploratory Analysis and Visualization","text":"<p>Now we apply some data manipulation steps and explore some of the findings through the use of visuals. Hopefully we can then gain some useful insights from our data. </p>"},{"location":"Extras/customer_analysis/#what-kind-of-employment-is-most-common-in-our-data","title":"What kind of employment is most common in our data?","text":""},{"location":"Extras/customer_analysis/#what-is-the-education-level","title":"What is the education level?","text":""},{"location":"Extras/customer_analysis/#what-are-the-balances-for-individuals-based-on-their-age","title":"What are the balances for individuals based on their age?","text":""},{"location":"Extras/customer_analysis/#what-is-correlating-with-balance","title":"What is correlating with balance?","text":""},{"location":"Extras/customer_analysis/#what-is-the-salary-range-and-averages-for-both-response-types","title":"What is the salary range and averages for both response types?","text":""},{"location":"Extras/customer_analysis/#what-marital-status-has-the-highest-response-rate","title":"What marital status has the highest response rate?","text":""},{"location":"Extras/customer_analysis/#what-combination-of-education-and-marital-status-has-the-largest-response-rate","title":"What combination of education and marital status has the largest response rate?","text":""},{"location":"Extras/customer_analysis/#what-is-the-average-salary-for-each-age-group-in-the-data","title":"What is the average salary for each age group in the data?","text":""},{"location":"Extras/customer_analysis/#conclusions","title":"Conclusions","text":"<p>What we can say from the visuals above are the following:</p> <ol> <li>Approx. 60% of our customers are in the technician/management/blue collar category of work.</li> <li>Half are high school graduates, and less than a third have higher education.</li> <li>For people under 65, the balance is typically between 0-20000. For over 65, we see 0-10000 is the range. </li> <li>Heatmap supports the age-balance correlation to be stronger than salary-balance.</li> <li>Reponse rate is highest for single highly educated and lowest for married and less educated individuals. </li> </ol>"},{"location":"Extras/customer_analysis/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Extras/handling_imbalanced_data/","title":"Handling Imbalanced Data","text":"<p>title: Imbalanced Datasets keywords: [imbalanced datasets, how to handle dataset imbalance, data analysis, data science, data wrangling] description:  Let\u2019s go through a simple example using the popular Iris dataset, which we\u2019ll artificially imbalance for demonstration purposes date: \"2024-02\" author: Juma Shafara</p> <p></p> <p>Handling imbalanced datasets is a common challenge in machine learning, especially in classification tasks where one class significantly outnumbers the other(s). Let's go through a simple example using the popular Iris dataset, which we'll artificially imbalance for demonstration purposes. </p> <p>The Iris dataset consists of 150 samples, each belonging to one of three classes: Iris Setosa, Iris Versicolour, and Iris Virginica. We'll create an imbalanced version of this dataset where one class is underrepresented.</p> <p>First, let's load the dataset and create the imbalance: </p> <pre><code># !pip install imbalanced-learn\n# !pip install --upgrade dataidea\n</code></pre> <pre><code>from dataidea.packages import pd, plt, np\nfrom sklearn.datasets import load_iris\n</code></pre> <pre><code># Load Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Convert to DataFrame for manipulation\ndf = pd.DataFrame(data=np.c_[X, y], \n                  columns=iris.feature_names + ['target'])\n\ndf.head()\n</code></pre> sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0.0 1 4.9 3.0 1.4 0.2 0.0 2 4.7 3.2 1.3 0.2 0.0 3 4.6 3.1 1.5 0.2 0.0 4 5.0 3.6 1.4 0.2 0.0 Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates, simply enter your email below, follow us on   (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># Introduce imbalance by removing samples from one class\nclass_to_remove = 2  # Iris Virginica\nimbalance_ratio = 0.5  # Ratio of samples to be removed\nindices_to_remove = np.random.choice(df[df['target'] == class_to_remove].index,\n                                     size=int(imbalance_ratio * len(df[df['target'] == class_to_remove])),\n                                     replace=False)\ndf_imbalanced = df.drop(indices_to_remove)\n\n# Check the class distribution\nvalue_counts = df_imbalanced['target'].value_counts()\nprint(value_counts)\n</code></pre> <pre>\n<code>target\n0.0    50\n1.0    50\n2.0    25\nName: count, dtype: int64\n</code>\n</pre> <pre><code>plt.bar(value_counts.index, \n        value_counts.values, \n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n</code></pre> <p>Now, <code>df_imbalanced</code> contains the imbalanced dataset. Next, we'll demonstrate a few techniques to handle this imbalance:</p> <ol> <li>Resampling Methods:</li> <li>Oversampling: Randomly duplicate samples from the minority class.</li> <li>Undersampling: Randomly remove samples from the majority class.</li> <li>Synthetic Sampling Methods:</li> <li>SMOTE (Synthetic Minority Over-sampling Technique): Generates synthetic samples for the minority class.</li> <li>Algorithmic Techniques:</li> <li>Algorithm Tuning: Adjusting class weights in the algorithm.</li> <li>Ensemble Methods: Using ensemble techniques like bagging or boosting.</li> </ol> <pre><code>from imblearn.over_sampling import RandomOverSampler\n\n# Separate features and target\nX_imbalanced = df_imbalanced.drop('target', axis=1)\ny_imbalanced = df_imbalanced['target']\n\n# Apply Random Over-Sampling\noversample = RandomOverSampler(sampling_strategy='auto', \n                               random_state=42)\nX_resampled, y_resampled = oversample.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after oversampling\noversampled_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(oversampled_data_value_counts)\n</code></pre> <pre>\n<code>target\n0.0    50\n1.0    50\n2.0    50\nName: count, dtype: int64\n</code>\n</pre> <pre><code>plt.bar(oversampled_data_value_counts.index, \n        oversampled_data_value_counts.values, \n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n</code></pre> <pre><code>from imblearn.under_sampling import RandomUnderSampler\n\n# Apply Random Under-Sampling\nundersample = RandomUnderSampler(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = undersample.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after undersampling\nundersampled_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(undersampled_data_value_counts)\n</code></pre> <pre>\n<code>target\n0.0    25\n1.0    25\n2.0    25\nName: count, dtype: int64\n</code>\n</pre> <pre><code>plt.bar(undersampled_data_value_counts.index, \n        undersampled_data_value_counts.values, \n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n</code></pre> <pre><code>from imblearn.over_sampling import SMOTE\n\n# Apply SMOTE\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after SMOTE\nsmoted_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(smoted_data_value_counts)\n</code></pre> <pre>\n<code>target\n0.0    50\n1.0    50\n2.0    50\nName: count, dtype: int64\n</code>\n</pre> <pre><code>plt.bar(smoted_data_value_counts.index, \n        smoted_data_value_counts.values, \n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n</code></pre> <p>By using SMOTE, you can generate synthetic samples for the minority class, effectively increasing its representation in the dataset. This can help to mitigate the class imbalance issue and improve the performance of your machine learning model.</p> <pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n</code></pre> <pre><code># Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_imbalanced, y_imbalanced, \n                                                    test_size=0.2, random_state=42)\n\n# Define the logistic regression classifier with class weights\nclass_weights = {0: 1, 1: 1, 2: 20}  # Penalize the minority class more heavily\nlog_reg = LogisticRegression(class_weight=class_weights)\n\n# Train the model\nlog_reg.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = log_reg.predict(X_test)\n\n# display classification report\npd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()\n</code></pre> precision recall f1-score support 0.0 1.000000 1.000000 1.000000 13.00 1.0 1.000000 0.750000 0.857143 8.00 2.0 0.666667 1.000000 0.800000 4.00 accuracy 0.920000 0.920000 0.920000 0.92 macro avg 0.888889 0.916667 0.885714 25.00 weighted avg 0.946667 0.920000 0.922286 25.00 <p>In this example, the class weight for the minority class is increased to penalize misclassifications more heavily.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Define and train Random Forest classifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_classifier.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_rf = rf_classifier.predict(X_test)\n\n# Evaluate the performance\nprint(\"Random Forest Classifier:\")\npd.DataFrame(data=classification_report(y_test, y_pred_rf, output_dict=True)).transpose()\n</code></pre> <pre>\n<code>Random Forest Classifier:\n</code>\n</pre> precision recall f1-score support 0.0 1.000000 1.000000 1.000000 13.00 1.0 1.000000 0.875000 0.933333 8.00 2.0 0.800000 1.000000 0.888889 4.00 accuracy 0.960000 0.960000 0.960000 0.96 macro avg 0.933333 0.958333 0.940741 25.00 weighted avg 0.968000 0.960000 0.960889 25.00 <p>Ensemble methods like Random Forest build multiple decision trees and combine their predictions to make a final prediction. This can often lead to better generalization and performance, even in the presence of imbalanced classes.</p> <pre><code>from sklearn.ensemble import AdaBoostClassifier\n\n# Define and train AdaBoost classifier\nada_classifier = AdaBoostClassifier(n_estimators=100, random_state=42)\nada_classifier.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_ada = ada_classifier.predict(X_test)\n\n# Evaluate the performance\nprint(\"AdaBoost Classifier:\")\npd.DataFrame(data=classification_report(y_test, y_pred_ada, output_dict=True)).transpose()\n</code></pre> <pre>\n<code>AdaBoost Classifier:\n</code>\n</pre> <pre>\n<code>/home/jumashafara/venvs/dataidea/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n  warnings.warn(\n</code>\n</pre> precision recall f1-score support 0.0 1.000000 1.000000 1.000000 13.00 1.0 1.000000 0.875000 0.933333 8.00 2.0 0.800000 1.000000 0.888889 4.00 accuracy 0.960000 0.960000 0.960000 0.96 macro avg 0.933333 0.958333 0.940741 25.00 weighted avg 0.968000 0.960000 0.960889 25.00 <p>By utilizing ensemble methods like Random Forest and AdaBoost, you can often achieve better performance on imbalanced datasets compared to individual classifiers, as these methods inherently mitigate the effects of class imbalance through their construction.</p> <p>These are just a few techniques for handling imbalanced datasets. It's crucial to experiment with different methods and evaluate their performance using appropriate evaluation metrics to find the best approach for your specific problem.</p>"},{"location":"Extras/handling_imbalanced_data/#handling-imbalanced-dataset","title":"Handling Imbalanced Dataset","text":""},{"location":"Extras/handling_imbalanced_data/#introducing-imbalance","title":"Introducing Imbalance","text":""},{"location":"Extras/handling_imbalanced_data/#resampling","title":"Resampling","text":""},{"location":"Extras/handling_imbalanced_data/#oversampling","title":"Oversampling","text":"<p>Let's implement oversampling using the <code>imbalanced-learn</code> library:</p>"},{"location":"Extras/handling_imbalanced_data/#undersampling","title":"Undersampling:","text":"<p>Undersampling involves reducing the number of samples in the majority class to balance the dataset. Here's how you can apply random undersampling using the <code>imbalanced-learn</code> library:</p>"},{"location":"Extras/handling_imbalanced_data/#smote-synthetic-minority-over-sampling-technique","title":"SMOTE (Synthetic Minority Over-sampling Technique)","text":"<p>SMOTE generates synthetic samples for the minority class by interpolating between existing minority class samples.</p> <p>Here's how you can apply SMOTE using the <code>imbalanced-learn</code> library:</p>"},{"location":"Extras/handling_imbalanced_data/#algorithmic-techniques","title":"Algorithmic Techniques","text":""},{"location":"Extras/handling_imbalanced_data/#algorithm-tuning","title":"Algorithm Tuning:","text":"<p>Many algorithms allow you to specify class weights to penalize misclassifications of the minority class more heavily. Here's an example using the <code>class_weight</code> parameter in a logistic regression classifier:</p>"},{"location":"Extras/handling_imbalanced_data/#ensemble-methods","title":"Ensemble Methods","text":"<p>Ensemble methods can also be effective for handling imbalanced datasets. Techniques such as bagging and boosting can improve the performance of classifiers, especially when dealing with imbalanced classes.</p> <p>Here's an example of using ensemble methods like Random Forest, a popular bagging algorithm, with an imbalanced dataset:</p>"},{"location":"Extras/handling_imbalanced_data/#adaboost-classifier","title":"AdaBoost Classifier","text":"<p>Another ensemble method that specifically addresses class imbalance is AdaBoost (Adaptive Boosting). AdaBoost focuses more on those training instances that were previously misclassified, thus giving higher weight to the minority class instances.</p>"},{"location":"Extras/handling_imbalanced_data/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Extras/how_KNN_works/","title":"How KNN Works","text":"<p>title: Understanding K-Nearest Neighbors (KNN) Regression keywords: [KNN Regression, Distance Metric, Weighted KNN, Distance Calculation, Find Nearest Neighbors, Predict the Target Value, Visualizing KNN Regression, Advantages and Disadvantages of KNN] description: K-Nearest Neighbors (KNN) regression is a type of instance-based learning algorithm used for regression problems author: Juma Shafara date: \"2024-07-08\"</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Extras/how_KNN_works/#introduction-to-knn-regression","title":"Introduction to KNN Regression","text":"<p>K-Nearest Neighbors (KNN) regression is a type of instance-based learning algorithm used for regression problems. It makes predictions based on the \\(k\\) most similar instances (neighbors) in the training dataset. The algorithm is non-parametric, meaning it makes predictions without assuming any underlying data distribution.</p>"},{"location":"Extras/how_KNN_works/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Extras/how_KNN_works/#key-concepts","title":"Key Concepts","text":"<ol> <li>Distance Metric: The method used to calculate the distance between instances. Common metrics include Euclidean, Manhattan, and Minkowski distances.</li> <li>k: The number of neighbors to consider when making a prediction. Choosing the right \\(k\\) is crucial for the algorithm's performance.</li> <li>Weighted KNN: In some variants, closer neighbors have a higher influence on the prediction than more distant ones, often implemented by assigning weights inversely proportional to the distance.</li> </ol>"},{"location":"Extras/how_KNN_works/#how-knn-regression-works","title":"How KNN Regression Works","text":""},{"location":"Extras/how_KNN_works/#step-by-step-process","title":"Step-by-Step Process","text":"<ol> <li>Load the Data: Start with a dataset consisting of feature vectors and their corresponding target values.</li> <li>Choose the Number of Neighbors (k): Select the number of nearest neighbors to consider for making predictions.</li> <li>Distance Calculation: For a new data point, calculate the distance between this point and all points in the training dataset.</li> <li>Find Nearest Neighbors: Identify the \\(k\\) points in the training data that are closest to the new point.</li> <li>Predict the Target Value: Compute the average (or a weighted average) of the target values of the \\(k\\) nearest neighbors.</li> </ol>"},{"location":"Extras/how_KNN_works/#example","title":"Example","text":"<p>Let's walk through an example with a simple dataset.</p> <p>Dataset:</p> Feature (X) Target (Y) 1.0 2.0 2.0 3.0 3.0 4.5 4.0 6.0 5.0 7.5 <p>New Point: \\(X_{new} = 3.5\\)</p> <ol> <li>Choose \\(k\\): Let's select \\(k = 3\\).</li> <li>Calculate Distances:</li> <li>Distance to (1.0, 2.0): \\(\\sqrt{(3.5-1.0)^2} = 2.5\\)</li> <li>Distance to (2.0, 3.0): \\(\\sqrt{(3.5-2.0)^2} = 1.5\\)</li> <li>Distance to (3.0, 4.5): \\(\\sqrt{(3.5-3.0)^2} = 0.5\\)</li> <li>Distance to (4.0, 6.0): \\(\\sqrt{(3.5-4.0)^2} = 0.5\\)</li> <li>Distance to (5.0, 7.5): \\(\\sqrt{(3.5-5.0)^2} = 1.5\\)</li> <li>Find Nearest Neighbors:</li> <li>Neighbors: (3.0, 4.5), (4.0, 6.0), and (2.0, 3.0) (distances 0.5, 0.5, and 1.5 respectively)</li> <li>Predict the Target Value:</li> <li>Average the target values of the nearest neighbors: \\(\\frac{4.5 + 6.0 + 3.0}{3} = \\frac{13.5}{3} = 4.5\\)</li> </ol> <p>So, the predicted target value for \\(X_{new} = 3.5\\) is 4.5.</p>"},{"location":"Extras/how_KNN_works/#visualizing-knn-regression","title":"Visualizing KNN Regression","text":"<p>Below is a visual representation of the KNN regression process:</p> <p></p> <ul> <li>The blue points represent the training data.</li> <li>The red point is the new input for which we want to predict the target value.</li> <li>The green points are the nearest neighbors considered for the prediction.</li> </ul>"},{"location":"Extras/how_KNN_works/#advantages-and-disadvantages","title":"Advantages and Disadvantages","text":""},{"location":"Extras/how_KNN_works/#advantages","title":"Advantages:","text":"<ul> <li>Simplicity: Easy to understand and implement.</li> <li>No Training Phase: The algorithm stores the training dataset and makes predictions at runtime.</li> </ul>"},{"location":"Extras/how_KNN_works/#disadvantages","title":"Disadvantages:","text":"<ul> <li>Computationally Intensive: Requires computing the distance to all training points for each prediction, which can be slow for large datasets.</li> <li>Choosing \\( k \\): Selecting the optimal \\( k \\) can be challenging and often requires cross-validation.</li> <li>Curse of Dimensionality: Performance can degrade in high-dimensional spaces as distances become less meaningful.</li> </ul>"},{"location":"Extras/how_KNN_works/#conclusion","title":"Conclusion","text":"<p>KNN regression is a straightforward and intuitive algorithm for making predictions based on the similarity of data points. Despite its simplicity, it can be quite powerful, especially for smaller datasets. However, it can become computationally expensive for large datasets and high-dimensional data, and it requires careful selection of the number of neighbors \\( k \\).</p> <p>By understanding and visualizing the KNN regression process, you can better appreciate its applications and limitations, allowing you to apply it effectively in your machine learning projects.</p>"},{"location":"Extras/how_KNN_works/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Extras/interview_questions/","title":"Interview questions","text":""},{"location":"Extras/interview_questions/#technical-skills","title":"Technical Skills","text":"<ol> <li>Data Manipulation:</li> <li>How do you handle missing data in a dataset? Can you walk us through the steps you would take in cleaning and preprocessing data?</li> <li> <p>Which Python libraries do you frequently use for data manipulation, and why?</p> </li> <li> <p>Exploratory Data Analysis (EDA):</p> </li> <li> <p>Can you describe a recent project where you conducted exploratory data analysis (EDA)? What insights did you uncover, and how did they impact decision-making?</p> </li> <li> <p>Predictive Modeling:</p> </li> <li>Walk us through a machine learning model you developed. What problem were you solving, and how did you evaluate and optimize the model's performance?</li> <li> <p>How do you decide which machine learning algorithm to use for a particular problem? Can you provide examples?</p> </li> <li> <p>Statistical Knowledge:</p> </li> <li> <p>How do you apply statistical techniques to solve business problems? Can you provide an example where you used statistical models to generate actionable insights?</p> </li> <li> <p>A/B Testing &amp; Experimentation:</p> </li> <li> <p>Have you ever designed or analyzed an A/B test? What challenges did you face, and how did you interpret the results?</p> </li> <li> <p>Data Visualization:</p> </li> <li> <p>What tools do you prefer for data visualization, and how do you ensure that your visualizations are accessible to non-technical stakeholders?</p> </li> <li> <p>Cloud Platforms:</p> </li> <li> <p>Describe your experience using cloud platforms like AWS, GCP, or Azure for deploying machine learning models. What challenges did you encounter?</p> </li> <li> <p>Model Deployment:</p> </li> <li>Can you share a project where you deployed a machine learning model into production? How did you monitor and maintain it post-deployment?</li> </ol>"},{"location":"Extras/interview_questions/#problem-solving-research","title":"Problem-Solving &amp; Research","text":"<ol> <li>R&amp;D in Data Science:</li> <li> <p>Describe a situation where you had to perform research and development in data science to improve a model or methodology. How did your findings enhance your project?</p> </li> <li> <p>Complex Problem Solving:</p> <ul> <li>Tell us about a time you faced a complex data problem. How did you approach it, and what was the outcome?</li> </ul> </li> </ol>"},{"location":"Extras/interview_questions/#collaboration-communication","title":"Collaboration &amp; Communication","text":"<ol> <li> <p>Stakeholder Engagement:</p> <ul> <li>How do you communicate technical findings to non-technical stakeholders? Can you give an example of how you presented data insights to influence a key decision?</li> </ul> </li> <li> <p>Cross-functional Collaboration:</p> <ul> <li>Describe a project where you worked closely with different teams (e.g., program, field teams) to integrate data insights into program strategies.</li> </ul> </li> <li> <p>Mentoring:</p> <ul> <li>Have you ever mentored junior data scientists? What strategies do you use to help them improve their data science skills?</li> </ul> </li> </ol>"},{"location":"Extras/interview_questions/#nonprofit-social-impact-focus","title":"Nonprofit &amp; Social Impact Focus","text":"<ol> <li> <p>Nonprofit Sector Experience:</p> <ul> <li>What experience do you have working in the nonprofit sector, and how does it influence your approach to data science?</li> </ul> </li> <li> <p>Theories of Change:</p> <ul> <li>How do you apply Theories of Change (TOC) in data science work? Can you provide an example where TOC guided your data analysis?</li> </ul> </li> <li> <p>Participatory Research:</p> <ul> <li>Have you been involved in participatory research with last-mile communities? How did you approach data collection and analysis in such contexts?</li> </ul> </li> </ol>"},{"location":"Extras/interview_questions/#behavioral-soft-skills","title":"Behavioral &amp; Soft Skills","text":"<ol> <li> <p>Adaptability:</p> <ul> <li>Tell us about a time when you had to adapt to a significant change in project priorities. How did you manage the shift?</li> </ul> </li> <li> <p>Problem-Solving Under Pressure:</p> <ul> <li>Describe a situation where you had to solve a data problem under tight deadlines. How did you manage your time and resources?</li> </ul> </li> <li> <p>Team Player:</p> <ul> <li>How do you balance independent work with collaboration when working on data science projects? Can you give an example?</li> </ul> </li> </ol>"},{"location":"Extras/interview_questions/#continuous-learning","title":"Continuous Learning","text":"<ol> <li>Staying Updated:<ul> <li>How do you stay current with the latest developments in data science and machine learning? What\u2019s the most recent tool or technique you\u2019ve learned?</li> </ul> </li> </ol> <p>These questions will help assess the technical skills, problem-solving capabilities, collaboration style, and nonprofit experience required for the role.</p>"},{"location":"Git/01_introduction/","title":"Introduction to Git and GitHub","text":"<p>Learn version control with Git and collaborate with GitHub - essential skills for modern software development and data science.</p>"},{"location":"Git/01_introduction/#what-is-git","title":"What is Git?","text":"<p>Git is a distributed version control system that tracks changes in your code over time. It allows you to:</p> <ul> <li>Save snapshots of your project at different points in time</li> <li>Collaborate with others without overwriting each other's work</li> <li>Experiment with new features without breaking working code</li> <li>Revert to previous versions when things go wrong</li> <li>Track who made what changes and when</li> </ul>"},{"location":"Git/01_introduction/#what-is-github","title":"What is GitHub?","text":"<p>GitHub is a cloud-based hosting service for Git repositories. It provides:</p> <ul> <li>Remote storage for your Git repositories</li> <li>Collaboration tools (pull requests, issues, discussions)</li> <li>Code review features</li> <li>Project management tools</li> <li>CI/CD integration</li> <li>Portfolio for showcasing your work</li> </ul>"},{"location":"Git/01_introduction/#why-learn-git","title":"Why Learn Git?","text":""},{"location":"Git/01_introduction/#for-data-scientists","title":"For Data Scientists","text":"<ul> <li>Track changes to data analysis notebooks</li> <li>Collaborate on machine learning projects</li> <li>Version datasets and model configurations</li> <li>Share reproducible research</li> <li>Manage experiment tracking</li> </ul>"},{"location":"Git/01_introduction/#for-developers","title":"For Developers","text":"<ul> <li>Industry standard for version control</li> <li>Required for most tech jobs</li> <li>Essential for open-source contribution</li> <li>Enables team collaboration</li> <li>Facilitates code review processes</li> </ul>"},{"location":"Git/01_introduction/#key-concepts","title":"Key Concepts","text":""},{"location":"Git/01_introduction/#repository-repo","title":"Repository (Repo)","text":"<p>A folder that Git tracks. Contains all your project files and the complete history of changes.</p>"},{"location":"Git/01_introduction/#commit","title":"Commit","text":"<p>A snapshot of your project at a specific point in time. Like saving your game progress.</p>"},{"location":"Git/01_introduction/#branch","title":"Branch","text":"<p>An independent line of development. Allows you to work on features without affecting the main code.</p>"},{"location":"Git/01_introduction/#remote","title":"Remote","text":"<p>A version of your repository hosted on the internet (usually on GitHub).</p>"},{"location":"Git/01_introduction/#clone","title":"Clone","text":"<p>Creating a local copy of a remote repository on your computer.</p>"},{"location":"Git/01_introduction/#push","title":"Push","text":"<p>Uploading your local commits to a remote repository.</p>"},{"location":"Git/01_introduction/#pull","title":"Pull","text":"<p>Downloading changes from a remote repository to your local machine.</p>"},{"location":"Git/01_introduction/#git-vs-github","title":"Git vs GitHub","text":"Git GitHub Version control system Hosting service for Git repositories Installed on your computer Cloud-based platform Command-line tool Web interface + additional features Works offline Requires internet connection Free and open-source Free for public repos, paid for advanced features"},{"location":"Git/01_introduction/#installing-git","title":"Installing Git","text":""},{"location":"Git/01_introduction/#windows","title":"Windows","text":"<ol> <li>Download from git-scm.com</li> <li>Run the installer</li> <li>Use default settings (or customize as needed)</li> <li>Verify: Open Git Bash and run <code>git --version</code></li> </ol>"},{"location":"Git/01_introduction/#macos","title":"macOS","text":"<pre><code># Using Homebrew\nbrew install git\n\n# Or download from git-scm.com\n</code></pre> <p>Verify installation: <pre><code>git --version\n</code></pre></p>"},{"location":"Git/01_introduction/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<pre><code>sudo apt-get update\nsudo apt-get install git\n</code></pre> <p>Verify installation: <pre><code>git --version\n</code></pre></p>"},{"location":"Git/01_introduction/#initial-git-configuration","title":"Initial Git Configuration","text":"<p>After installing Git, configure your identity:</p> <pre><code># Set your name\ngit config --global user.name \"Your Name\"\n\n# Set your email\ngit config --global user.email \"your.email@example.com\"\n\n# Check your configuration\ngit config --list\n</code></pre> <p>Use Your GitHub Email</p> <p>If you plan to use GitHub, use the same email address you'll use for your GitHub account.</p>"},{"location":"Git/01_introduction/#creating-a-github-account","title":"Creating a GitHub Account","text":"<ol> <li>Go to github.com</li> <li>Click \"Sign up\"</li> <li>Follow the registration process</li> <li>Verify your email address</li> <li>Complete your profile</li> </ol>"},{"location":"Git/01_introduction/#basic-workflow-overview","title":"Basic Workflow Overview","text":"<p>Here's a typical Git workflow:</p> <pre><code>1. Initialize or clone a repository\n   \u2193\n2. Make changes to files\n   \u2193\n3. Stage changes (git add)\n   \u2193\n4. Commit changes (git commit)\n   \u2193\n5. Push to remote (git push)\n   \u2193\n6. Repeat steps 2-5\n</code></pre>"},{"location":"Git/01_introduction/#your-first-git-repository","title":"Your First Git Repository","text":"<p>Let's create your first Git repository:</p> <pre><code># Create a new folder\nmkdir my-first-repo\ncd my-first-repo\n\n# Initialize Git\ngit init\n\n# Check status\ngit status\n</code></pre> <p>You should see: <pre><code>Initialized empty Git repository in /path/to/my-first-repo/.git/\n</code></pre></p>"},{"location":"Git/01_introduction/#creating-your-first-commit","title":"Creating Your First Commit","text":"<pre><code># Create a file\necho \"# My First Repo\" &gt; README.md\n\n# Check status\ngit status\n\n# Stage the file\ngit add README.md\n\n# Commit\ngit commit -m \"Initial commit: Add README\"\n\n# View commit history\ngit log\n</code></pre>"},{"location":"Git/01_introduction/#common-git-terms-explained","title":"Common Git Terms Explained","text":""},{"location":"Git/01_introduction/#working-directory","title":"Working Directory","text":"<p>The folder on your computer where you're currently working. Contains your actual files.</p>"},{"location":"Git/01_introduction/#staging-area-index","title":"Staging Area (Index)","text":"<p>A intermediate area where you prepare files before committing. Think of it as a \"preview\" of your next commit.</p>"},{"location":"Git/01_introduction/#local-repository","title":"Local Repository","text":"<p>The <code>.git</code> folder in your project that stores all commits and history on your computer.</p>"},{"location":"Git/01_introduction/#remote-repository","title":"Remote Repository","text":"<p>A version of your project hosted on a server (like GitHub).</p>"},{"location":"Git/01_introduction/#head","title":"HEAD","text":"<p>A pointer to your current branch and commit. Think of it as \"you are here.\"</p>"},{"location":"Git/01_introduction/#git-workflow-diagram","title":"Git Workflow Diagram","text":"<pre><code>Working Directory  \u2192  Staging Area  \u2192  Local Repository  \u2192  Remote Repository\n                   git add          git commit          git push\n\n                   \u2190                 \u2190                   \u2190\n                   git restore      git reset           git pull\n</code></pre>"},{"location":"Git/01_introduction/#best-practices","title":"Best Practices","text":"<ol> <li>Commit Often - Make small, focused commits</li> <li>Write Clear Messages - Explain what and why, not how</li> <li>Pull Before Push - Always sync before sharing your changes</li> <li>Use Branches - Don't work directly on main/master</li> <li>Review Changes - Check <code>git status</code> and <code>git diff</code> before committing</li> <li>.gitignore - Don't commit sensitive data or generated files</li> </ol>"},{"location":"Git/01_introduction/#what-not-to-commit","title":"What Not to Commit","text":"<p>Never commit: - Passwords or API keys - Large binary files (use Git LFS instead) - Dependencies (node_modules, venv, etc.) - Personal configuration files - Temporary files - Compiled code (unless necessary)</p> <p>Create a <code>.gitignore</code> file to exclude these automatically.</p>"},{"location":"Git/01_introduction/#getting-help","title":"Getting Help","text":"<pre><code># General help\ngit help\n\n# Help for a specific command\ngit help commit\ngit commit --help\n\n# Quick reference\ngit &lt;command&gt; -h\n</code></pre>"},{"location":"Git/01_introduction/#resources","title":"Resources","text":"<ul> <li>Official Git Documentation</li> <li>GitHub Guides</li> <li>Pro Git Book (Free online)</li> <li>Git Cheat Sheet</li> </ul>"},{"location":"Git/01_introduction/#whats-next","title":"What's Next?","text":"<p>In the following tutorials, you'll learn:</p> <ol> <li>Basic Git Commands - add, commit, status, log</li> <li>Branching and Merging - Create and manage branches</li> <li>GitHub Basics - Push, pull, clone, fork</li> <li>Collaboration - Pull requests, code review</li> <li>Advanced Topics - Rebase, cherry-pick, stash</li> </ol> <p>You're Ready!</p> <p>You now understand the basics of Git and GitHub. Let's dive deeper into the commands and workflows.</p> <p>Next Tutorial: Basic Git Commands</p>"},{"location":"Git/02_basic_commands/","title":"Basic Git Commands","text":"<p>Master the essential Git commands for everyday version control.</p>"},{"location":"Git/02_basic_commands/#repository-initialization","title":"Repository Initialization","text":""},{"location":"Git/02_basic_commands/#creating-a-new-repository","title":"Creating a New Repository","text":"<pre><code># Create and navigate to project folder\nmkdir my-project\ncd my-project\n\n# Initialize Git\ngit init\n</code></pre> <p>This creates a hidden <code>.git</code> folder that stores all Git data.</p>"},{"location":"Git/02_basic_commands/#cloning-an-existing-repository","title":"Cloning an Existing Repository","text":"<pre><code># Clone from GitHub\ngit clone https://github.com/username/repo-name.git\n\n# Clone to a specific folder\ngit clone https://github.com/username/repo-name.git my-folder\n\n# Clone a specific branch\ngit clone -b branch-name https://github.com/username/repo-name.git\n</code></pre>"},{"location":"Git/02_basic_commands/#checking-repository-status","title":"Checking Repository Status","text":""},{"location":"Git/02_basic_commands/#git-status","title":"git status","text":"<p>Shows the current state of your working directory and staging area.</p> <pre><code>git status\n</code></pre> <p>Example output: <pre><code>On branch main\nYour branch is up to date with 'origin/main'.\n\nChanges not staged for commit:\n  modified:   file1.py\n\nUntracked files:\n  file2.py\n\nno changes added to commit\n</code></pre></p> <p>Short format: <pre><code>git status -s\n</code></pre></p> <p>Output: <pre><code>M  file1.py\n?? file2.py\n</code></pre></p> <p>Symbols: - <code>??</code> - Untracked file - <code>M</code> - Modified file - <code>A</code> - Added file - <code>D</code> - Deleted file</p>"},{"location":"Git/02_basic_commands/#staging-changes","title":"Staging Changes","text":""},{"location":"Git/02_basic_commands/#git-add","title":"git add","text":"<p>Add files to the staging area (prepare for commit).</p> <pre><code># Stage a specific file\ngit add filename.py\n\n# Stage multiple files\ngit add file1.py file2.py\n\n# Stage all changes in current directory\ngit add .\n\n# Stage all changes in repository\ngit add -A\n\n# Stage all Python files\ngit add *.py\n\n# Stage all files in a folder\ngit add folder/\n</code></pre>"},{"location":"Git/02_basic_commands/#interactive-staging","title":"Interactive Staging","text":"<pre><code># Choose what to stage interactively\ngit add -p filename.py\n</code></pre> <p>This lets you stage parts of a file (hunks) individually.</p>"},{"location":"Git/02_basic_commands/#committing-changes","title":"Committing Changes","text":""},{"location":"Git/02_basic_commands/#git-commit","title":"git commit","text":"<p>Save staged changes to the repository.</p> <pre><code># Commit with inline message\ngit commit -m \"Add user authentication feature\"\n\n# Commit with detailed message (opens editor)\ngit commit\n\n# Stage and commit in one command\ngit commit -am \"Fix bug in data processing\"\n</code></pre> <p>The -a flag</p> <p><code>git commit -a</code> only stages modified files, not new (untracked) files.</p>"},{"location":"Git/02_basic_commands/#writing-good-commit-messages","title":"Writing Good Commit Messages","text":"<p>Bad: <pre><code>git commit -m \"fixed stuff\"\ngit commit -m \"updates\"\ngit commit -m \"asdfgh\"\n</code></pre></p> <p>Good: <pre><code>git commit -m \"Fix null pointer exception in user login\"\ngit commit -m \"Add data validation for email field\"\ngit commit -m \"Update README with installation instructions\"\n</code></pre></p> <p>Best Practice Format: <pre><code>Short summary (50 chars or less)\n\nDetailed explanation if needed. Wrap at 72 characters.\nExplain WHAT changed and WHY, not HOW.\n\n- Bullet points are okay\n- Use present tense: \"Add\" not \"Added\"\n- Reference issues: Fixes #123\n</code></pre></p>"},{"location":"Git/02_basic_commands/#viewing-history","title":"Viewing History","text":""},{"location":"Git/02_basic_commands/#git-log","title":"git log","text":"<p>View commit history.</p> <pre><code># Full log\ngit log\n\n# One line per commit\ngit log --oneline\n\n# Last 5 commits\ngit log -5\n\n# Show file changes\ngit log --stat\n\n# Show actual changes\ngit log -p\n\n# Graphical view\ngit log --graph --oneline --all\n\n# Filter by author\ngit log --author=\"John Doe\"\n\n# Filter by date\ngit log --since=\"2024-01-01\"\ngit log --after=\"2 weeks ago\"\n</code></pre> <p>Example output: <pre><code>commit a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0\nAuthor: Your Name &lt;your.email@example.com&gt;\nDate:   Sat Jan 20 10:30:00 2024 +0000\n\n    Add user authentication feature\n</code></pre></p>"},{"location":"Git/02_basic_commands/#viewing-a-specific-commit","title":"Viewing a Specific Commit","text":"<pre><code># Show commit details\ngit show a1b2c3d\n\n# Show files changed\ngit show --name-only a1b2c3d\n\n# Show changes in a file\ngit show a1b2c3d:path/to/file.py\n</code></pre>"},{"location":"Git/02_basic_commands/#viewing-changes","title":"Viewing Changes","text":""},{"location":"Git/02_basic_commands/#git-diff","title":"git diff","text":"<p>See what changed in your files.</p> <pre><code># Changes in working directory (not staged)\ngit diff\n\n# Changes in staging area (staged but not committed)\ngit diff --staged\n# or\ngit diff --cached\n\n# Compare specific files\ngit diff filename.py\n\n# Compare commits\ngit diff commit1 commit2\n\n# Compare branches\ngit diff main feature-branch\n</code></pre> <p>Example output: <pre><code>diff --git a/app.py b/app.py\nindex 1234567..abcdefg 100644\n--- a/app.py\n+++ b/app.py\n@@ -10,7 +10,7 @@ def hello():\n-    return \"Hello World\"\n+    return \"Hello, World!\"\n</code></pre></p> <p>Legend: - <code>-</code> Red line (removed) - <code>+</code> Green line (added) - Context lines (unchanged)</p>"},{"location":"Git/02_basic_commands/#undoing-changes","title":"Undoing Changes","text":""},{"location":"Git/02_basic_commands/#unstaging-files","title":"Unstaging Files","text":"<pre><code># Unstage a file (keep changes)\ngit restore --staged filename.py\n\n# Unstage all files\ngit restore --staged .\n</code></pre>"},{"location":"Git/02_basic_commands/#discarding-changes","title":"Discarding Changes","text":"<pre><code># Discard changes in working directory\ngit restore filename.py\n\n# Discard all changes\ngit restore .\n</code></pre> <p>Warning</p> <p><code>git restore</code> (without --staged) permanently discards changes that aren't committed!</p>"},{"location":"Git/02_basic_commands/#amending-the-last-commit","title":"Amending the Last Commit","text":"<pre><code># Fix the last commit message\ngit commit --amend -m \"New commit message\"\n\n# Add forgotten files to last commit\ngit add forgotten-file.py\ngit commit --amend --no-edit\n</code></pre> <p>Don't Amend Pushed Commits</p> <p>Only amend commits that haven't been pushed to a remote repository.</p>"},{"location":"Git/02_basic_commands/#removing-files","title":"Removing Files","text":""},{"location":"Git/02_basic_commands/#git-rm","title":"git rm","text":"<p>Remove files from Git and your working directory.</p> <pre><code># Remove a file\ngit rm filename.py\n\n# Remove from Git but keep locally\ngit rm --cached filename.py\n\n# Remove all .log files\ngit rm *.log\n\n# Remove folder\ngit rm -r folder-name/\n</code></pre> <p>Then commit: <pre><code>git commit -m \"Remove old log files\"\n</code></pre></p>"},{"location":"Git/02_basic_commands/#movingrenaming-files","title":"Moving/Renaming Files","text":""},{"location":"Git/02_basic_commands/#git-mv","title":"git mv","text":"<pre><code># Rename a file\ngit mv old-name.py new-name.py\n\n# Move a file\ngit mv file.py folder/file.py\n</code></pre> <p>This is equivalent to: <pre><code>mv old-name.py new-name.py\ngit rm old-name.py\ngit add new-name.py\n</code></pre></p>"},{"location":"Git/02_basic_commands/#ignoring-files","title":"Ignoring Files","text":""},{"location":"Git/02_basic_commands/#gitignore","title":".gitignore","text":"<p>Create a <code>.gitignore</code> file to specify files Git should ignore.</p> <p>Example .gitignore: <pre><code># Python\n*.pyc\n__pycache__/\n.venv/\nvenv/\n*.egg-info/\n\n# Jupyter\n.ipynb_checkpoints/\n\n# Environment\n.env\n.env.local\n\n# OS\n.DS_Store\nThumbs.db\n\n# IDE\n.vscode/\n.idea/\n\n# Data\n*.csv\ndata/raw/\n!data/processed/important.csv\n</code></pre></p> <p>Patterns: - <code>*.ext</code> - All files with extension - <code>folder/</code> - Entire folder - <code>!file</code> - Exception (don't ignore this) - <code>#</code> - Comments</p>"},{"location":"Git/02_basic_commands/#ignoring-already-tracked-files","title":"Ignoring Already Tracked Files","text":"<pre><code># Stop tracking but keep file\ngit rm --cached filename\n\n# Update .gitignore\necho \"filename\" &gt;&gt; .gitignore\n\n# Commit\ngit commit -m \"Stop tracking filename\"\n</code></pre>"},{"location":"Git/02_basic_commands/#working-with-remotes","title":"Working with Remotes","text":""},{"location":"Git/02_basic_commands/#git-remote","title":"git remote","text":"<p>Manage remote repositories.</p> <pre><code># View remotes\ngit remote\n\n# View remote URLs\ngit remote -v\n\n# Add a remote\ngit remote add origin https://github.com/username/repo.git\n\n# Change remote URL\ngit remote set-url origin https://github.com/username/new-repo.git\n\n# Remove a remote\ngit remote remove origin\n\n# Rename a remote\ngit remote rename old-name new-name\n</code></pre>"},{"location":"Git/02_basic_commands/#practical-examples","title":"Practical Examples","text":""},{"location":"Git/02_basic_commands/#example-1-starting-a-new-project","title":"Example 1: Starting a New Project","text":"<pre><code># Initialize\nmkdir data-analysis-project\ncd data-analysis-project\ngit init\n\n# Create files\necho \"# Data Analysis Project\" &gt; README.md\necho \"*.csv\" &gt; .gitignore\n\n# First commit\ngit add .\ngit commit -m \"Initial commit: Add README and gitignore\"\n</code></pre>"},{"location":"Git/02_basic_commands/#example-2-making-changes","title":"Example 2: Making Changes","text":"<pre><code># Create a new file\necho \"import pandas as pd\" &gt; analysis.py\n\n# Check status\ngit status\n\n# Stage and commit\ngit add analysis.py\ngit commit -m \"Add initial analysis script\"\n\n# Make more changes\necho \"df = pd.read_csv('data.csv')\" &gt;&gt; analysis.py\n\n# View changes\ngit diff\n\n# Commit\ngit add analysis.py\ngit commit -m \"Add data loading code\"\n</code></pre>"},{"location":"Git/02_basic_commands/#example-3-fixing-a-mistake","title":"Example 3: Fixing a Mistake","text":"<pre><code># Oops, made a typo\necho \"improt pandas\" &gt; analysis.py\n\n# Check status\ngit status\n\n# Discard the mistake\ngit restore analysis.py\n\n# Or if you already staged it\ngit add analysis.py\ngit restore --staged analysis.py\ngit restore analysis.py\n</code></pre>"},{"location":"Git/02_basic_commands/#practice-exercises","title":"Practice Exercises","text":"<p>Try these exercises in a test repository:</p> <ol> <li>Create a new repository</li> <li>Add a README.md file and commit it</li> <li>Create a Python file and commit it</li> <li>Modify the Python file and view the diff</li> <li>Stage and commit the changes</li> <li>View the commit history</li> <li>Create a .gitignore file for Python projects</li> <li>Add and commit the .gitignore file</li> </ol> Solutions <pre><code># 1. Create repository\nmkdir git-practice\ncd git-practice\ngit init\n\n# 2. Add README\necho \"# Practice Repository\" &gt; README.md\ngit add README.md\ngit commit -m \"Add README\"\n\n# 3. Create and commit Python file\necho \"print('Hello Git')\" &gt; hello.py\ngit add hello.py\ngit commit -m \"Add hello script\"\n\n# 4. Modify and view diff\necho \"print('Learning Git')\" &gt;&gt; hello.py\ngit diff\n\n# 5. Stage and commit\ngit add hello.py\ngit commit -m \"Add second print statement\"\n\n# 6. View history\ngit log --oneline\n\n# 7. Create .gitignore\ncat &gt; .gitignore &lt;&lt; EOF\n*.pyc\n__pycache__/\n.venv/\nEOF\n\n# 8. Commit .gitignore\ngit add .gitignore\ngit commit -m \"Add Python gitignore\"\n</code></pre>"},{"location":"Git/02_basic_commands/#common-workflows","title":"Common Workflows","text":""},{"location":"Git/02_basic_commands/#daily-workflow","title":"Daily Workflow","text":"<pre><code># Start of day: Get latest changes\ngit pull\n\n# Work on files\n# ... make changes ...\n\n# Check what changed\ngit status\ngit diff\n\n# Stage and commit\ngit add .\ngit commit -m \"Descriptive message\"\n\n# Share your work\ngit push\n</code></pre>"},{"location":"Git/02_basic_commands/#before-going-home","title":"Before Going Home","text":"<pre><code># Save all work\ngit add .\ngit commit -m \"WIP: End of day checkpoint\"\ngit push\n</code></pre> <p>Commit Early, Commit Often</p> <p>Don't wait until you have \"perfect\" code. Commit logical chunks of work frequently. You can always reorganize commits later if needed.</p> <p>Previous: Introduction | Next: Branching and Merging</p>"},{"location":"Git/03_branching/","title":"Branching and Merging","text":"<p>Learn how to use Git branches to work on multiple features simultaneously without conflicts.</p>"},{"location":"Git/03_branching/#what-is-a-branch","title":"What is a Branch?","text":"<p>A branch is an independent line of development. Think of it as a separate timeline where you can make changes without affecting the main codebase.</p>"},{"location":"Git/03_branching/#why-use-branches","title":"Why Use Branches?","text":"<ul> <li>Isolate features - Work on new features without breaking main code</li> <li>Experiment safely - Try new ideas without risk</li> <li>Parallel development - Multiple people work simultaneously</li> <li>Organize work - Separate bug fixes from new features</li> <li>Code review - Submit changes for review before merging</li> </ul>"},{"location":"Git/03_branching/#understanding-branches","title":"Understanding Branches","text":"<pre><code>main:     A --- B --- C --- D\n                   \\\nfeature:            E --- F\n</code></pre> <ul> <li><code>main</code> is the default branch (formerly called <code>master</code>)</li> <li><code>feature</code> branched off from commit <code>B</code></li> <li>Changes on <code>feature</code> don't affect <code>main</code></li> </ul>"},{"location":"Git/03_branching/#branch-basics","title":"Branch Basics","text":""},{"location":"Git/03_branching/#viewing-branches","title":"Viewing Branches","text":"<pre><code># List local branches\ngit branch\n\n# List all branches (including remote)\ngit branch -a\n\n# List remote branches only\ngit branch -r\n\n# Show current branch\ngit branch --show-current\n</code></pre> <p>Example output: <pre><code>* main\n  feature-login\n  bugfix-header\n</code></pre></p> <p>The <code>*</code> indicates your current branch.</p>"},{"location":"Git/03_branching/#creating-branches","title":"Creating Branches","text":"<pre><code># Create a new branch\ngit branch feature-name\n\n# Create and switch to new branch\ngit checkout -b feature-name\n\n# Modern alternative (Git 2.23+)\ngit switch -c feature-name\n</code></pre>"},{"location":"Git/03_branching/#switching-branches","title":"Switching Branches","text":"<pre><code># Switch to existing branch\ngit checkout branch-name\n\n# Modern alternative\ngit switch branch-name\n\n# Switch to previous branch\ngit switch -\n</code></pre> <p>Clean Working Directory</p> <p>Commit or stash your changes before switching branches to avoid conflicts.</p>"},{"location":"Git/03_branching/#deleting-branches","title":"Deleting Branches","text":"<pre><code># Delete a merged branch\ngit branch -d branch-name\n\n# Force delete (even if not merged)\ngit branch -D branch-name\n\n# Delete remote branch\ngit push origin --delete branch-name\n</code></pre>"},{"location":"Git/03_branching/#practical-branching-workflow","title":"Practical Branching Workflow","text":""},{"location":"Git/03_branching/#example-adding-a-new-feature","title":"Example: Adding a New Feature","text":"<pre><code># 1. Make sure you're on main and up-to-date\ngit switch main\ngit pull\n\n# 2. Create feature branch\ngit switch -c feature-user-authentication\n\n# 3. Make changes\necho \"def login():\" &gt; auth.py\ngit add auth.py\ngit commit -m \"Add login function\"\n\n# 4. More changes\necho \"def logout():\" &gt;&gt; auth.py\ngit add auth.py\ngit commit -m \"Add logout function\"\n\n# 5. View branch history\ngit log --oneline --graph\n</code></pre>"},{"location":"Git/03_branching/#merging-branches","title":"Merging Branches","text":"<p>Merging combines changes from one branch into another.</p>"},{"location":"Git/03_branching/#fast-forward-merge","title":"Fast-Forward Merge","text":"<p>When no new commits exist on the target branch:</p> <pre><code># Switch to main\ngit switch main\n\n# Merge feature branch\ngit merge feature-user-authentication\n</code></pre> <p>Before merge: <pre><code>main:    A --- B\n              \\\nfeature:       C --- D\n</code></pre></p> <p>After merge: <pre><code>main:    A --- B --- C --- D\n</code></pre></p>"},{"location":"Git/03_branching/#three-way-merge","title":"Three-Way Merge","text":"<p>When both branches have new commits:</p> <pre><code>git switch main\ngit merge feature-user-authentication\n</code></pre> <p>Before merge: <pre><code>main:    A --- B --- E\n              \\\nfeature:       C --- D\n</code></pre></p> <p>After merge: <pre><code>main:    A --- B --- E --- M\n              \\           /\nfeature:       C --- D --\n</code></pre></p> <p><code>M</code> is a merge commit that combines both histories.</p>"},{"location":"Git/03_branching/#merge-with-message","title":"Merge with Message","text":"<pre><code>git merge feature-name -m \"Merge feature: user authentication\"\n</code></pre>"},{"location":"Git/03_branching/#handling-merge-conflicts","title":"Handling Merge Conflicts","text":"<p>Conflicts occur when the same lines were changed in both branches.</p>"},{"location":"Git/03_branching/#example-conflict","title":"Example Conflict","text":"<pre><code># Try to merge\ngit merge feature-branch\n</code></pre> <p>Output: <pre><code>Auto-merging file.py\nCONFLICT (content): Merge conflict in file.py\nAutomatic merge failed; fix conflicts and then commit the result.\n</code></pre></p>"},{"location":"Git/03_branching/#conflict-markers","title":"Conflict Markers","text":"<p>Git adds conflict markers to the file:</p> <pre><code>def calculate_total(items):\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n    return sum(item.price for item in items)\n=======\n    return sum(item.cost * item.quantity for item in items)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature-branch\n</code></pre> <ul> <li><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</code> - Your current branch changes</li> <li><code>=======</code> - Separator</li> <li><code>&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature-branch</code> - Incoming branch changes</li> </ul>"},{"location":"Git/03_branching/#resolving-conflicts","title":"Resolving Conflicts","text":"<ol> <li>Open the conflicted file</li> <li>Choose which changes to keep (or combine both)</li> <li>Remove conflict markers</li> <li>Stage the resolved file</li> <li>Commit the merge</li> </ol> <pre><code># Edit file to resolve conflict\n# Remove &lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt; markers\n\n# Stage resolved file\ngit add file.py\n\n# Complete the merge\ngit commit -m \"Merge feature-branch, resolve pricing conflicts\"\n</code></pre>"},{"location":"Git/03_branching/#aborting-a-merge","title":"Aborting a Merge","text":"<p>If things go wrong:</p> <pre><code># Cancel the merge\ngit merge --abort\n</code></pre>"},{"location":"Git/03_branching/#branch-management-strategies","title":"Branch Management Strategies","text":""},{"location":"Git/03_branching/#feature-branch-workflow","title":"Feature Branch Workflow","text":"<pre><code># Always branch from main\ngit switch main\ngit pull\ngit switch -c feature-new-chart\n\n# Work on feature\n# ... commits ...\n\n# Merge back to main\ngit switch main\ngit pull\ngit merge feature-new-chart\ngit push\n\n# Delete feature branch\ngit branch -d feature-new-chart\n</code></pre>"},{"location":"Git/03_branching/#hotfix-workflow","title":"Hotfix Workflow","text":"<p>For urgent bug fixes:</p> <pre><code># Create hotfix branch from main\ngit switch main\ngit switch -c hotfix-login-bug\n\n# Fix the bug\n# ... make changes ...\ngit commit -am \"Fix login validation bug\"\n\n# Merge to main\ngit switch main\ngit merge hotfix-login-bug\ngit push\n\n# Delete hotfix branch\ngit branch -d hotfix-login-bug\n</code></pre>"},{"location":"Git/03_branching/#viewing-branch-differences","title":"Viewing Branch Differences","text":"<pre><code># See commits in feature not in main\ngit log main..feature-branch\n\n# See all differences\ngit diff main..feature-branch\n\n# See file list\ngit diff --name-only main..feature-branch\n</code></pre>"},{"location":"Git/03_branching/#renaming-branches","title":"Renaming Branches","text":"<pre><code># Rename current branch\ngit branch -m new-name\n\n# Rename a different branch\ngit branch -m old-name new-name\n\n# Update remote after renaming\ngit push origin -u new-name\ngit push origin --delete old-name\n</code></pre>"},{"location":"Git/03_branching/#branch-naming-conventions","title":"Branch Naming Conventions","text":"<p>Good branch names are descriptive and follow a pattern:</p> <p>Common patterns: <pre><code>feature/user-authentication\nbugfix/header-alignment\nhotfix/critical-security-patch\nrefactor/database-queries\ndocs/api-documentation\n</code></pre></p> <p>Examples: <pre><code>feature/add-dark-mode\nbugfix/fix-csv-export\nhotfix/security-vulnerability\nrefactor/optimize-queries\ntest/add-unit-tests\n</code></pre></p>"},{"location":"Git/03_branching/#advanced-branching","title":"Advanced Branching","text":""},{"location":"Git/03_branching/#listing-merged-branches","title":"Listing Merged Branches","text":"<pre><code># Branches merged into current branch\ngit branch --merged\n\n# Branches not yet merged\ngit branch --no-merged\n\n# Clean up merged branches\ngit branch --merged | grep -v \"main\" | xargs git branch -d\n</code></pre>"},{"location":"Git/03_branching/#branch-from-specific-commit","title":"Branch from Specific Commit","text":"<pre><code># Create branch from a commit\ngit branch new-branch commit-hash\n\n# Create and switch\ngit switch -c new-branch commit-hash\n</code></pre>"},{"location":"Git/03_branching/#cherry-pick","title":"Cherry-Pick","text":"<p>Apply a specific commit from one branch to another:</p> <pre><code># Switch to target branch\ngit switch main\n\n# Cherry-pick a commit\ngit cherry-pick abc1234\n</code></pre>"},{"location":"Git/03_branching/#common-workflows","title":"Common Workflows","text":""},{"location":"Git/03_branching/#workflow-1-parallel-feature-development","title":"Workflow 1: Parallel Feature Development","text":"<pre><code># Developer 1: User authentication\ngit switch -c feature-auth\n# ... work and commits ...\n\n# Developer 2: Dashboard UI (simultaneously)\ngit switch main\ngit switch -c feature-dashboard\n# ... work and commits ...\n\n# Merge both features\ngit switch main\ngit merge feature-auth\ngit merge feature-dashboard\n</code></pre>"},{"location":"Git/03_branching/#workflow-2-experiment-and-decide","title":"Workflow 2: Experiment and Decide","text":"<pre><code># Try approach A\ngit switch -c experiment-approach-a\n# ... code ...\n\n# Try approach B\ngit switch main\ngit switch -c experiment-approach-b\n# ... code ...\n\n# Approach A was better\ngit switch main\ngit merge experiment-approach-a\ngit branch -D experiment-approach-b\n</code></pre>"},{"location":"Git/03_branching/#practical-example","title":"Practical Example","text":"<p>Complete workflow for adding a feature:</p> <pre><code># 1. Start fresh\ngit switch main\ngit pull origin main\n\n# 2. Create feature branch\ngit switch -c feature-data-export\n\n# 3. Implement feature\ncat &gt; export.py &lt;&lt; EOF\ndef export_to_csv(data, filename):\n    import csv\n    with open(filename, 'w') as f:\n        writer = csv.writer(f)\n        writer.writerows(data)\nEOF\n\ngit add export.py\ngit commit -m \"Add CSV export function\"\n\n# 4. Add tests\ncat &gt; test_export.py &lt;&lt; EOF\ndef test_export():\n    assert export_to_csv([[1,2]], 'test.csv')\nEOF\n\ngit add test_export.py\ngit commit -m \"Add export tests\"\n\n# 5. Update documentation\necho \"## Export Feature\" &gt;&gt; README.md\ngit add README.md\ngit commit -m \"Document export feature\"\n\n# 6. Merge to main\ngit switch main\ngit merge feature-data-export\n\n# 7. Push to remote\ngit push origin main\n\n# 8. Clean up\ngit branch -d feature-data-export\n</code></pre>"},{"location":"Git/03_branching/#practice-exercises","title":"Practice Exercises","text":"<p>Try these in a test repository:</p> <ol> <li>Create a new branch called <code>feature-calculator</code></li> <li>Add a calculator function and commit</li> <li>Switch back to main</li> <li>Create another branch <code>feature-display</code></li> <li>Add a display function and commit</li> <li>Merge both branches into main</li> <li>Create a conflict by modifying the same line in two branches</li> <li>Resolve the conflict</li> </ol> Solutions <pre><code># 1-2. Create and work on feature-calculator\ngit switch -c feature-calculator\necho \"def add(a, b): return a + b\" &gt; calc.py\ngit add calc.py\ngit commit -m \"Add calculator function\"\n\n# 3-5. Create and work on feature-display\ngit switch main\ngit switch -c feature-display\necho \"def display(result): print(result)\" &gt; display.py\ngit add display.py\ngit commit -m \"Add display function\"\n\n# 6. Merge both\ngit switch main\ngit merge feature-calculator\ngit merge feature-display\n\n# 7. Create conflict\ngit switch -c branch-a\necho \"version = 1\" &gt; config.py\ngit add config.py\ngit commit -m \"Set version to 1\"\n\ngit switch main\ngit switch -c branch-b\necho \"version = 2\" &gt; config.py\ngit add config.py\ngit commit -m \"Set version to 2\"\n\ngit switch main\ngit merge branch-a\ngit merge branch-b  # Conflict!\n\n# 8. Resolve\n# Edit config.py to choose a version\necho \"version = 2\" &gt; config.py\ngit add config.py\ngit commit -m \"Resolve version conflict\"\n</code></pre>"},{"location":"Git/03_branching/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ol> <li>Branch often - Create branches for every feature or bug fix</li> <li>Keep branches short-lived - Merge or delete within days, not weeks</li> <li>Update regularly - Merge main into your feature branch frequently</li> <li>Use descriptive names - <code>feature-user-login</code> not <code>my-branch</code></li> <li>One feature per branch - Don't mix multiple features</li> <li>Delete merged branches - Keep your branch list clean</li> <li>Review before merging - Check <code>git diff</code> before merging</li> </ol> <p>Previous: Basic Commands | Next: GitHub Basics</p>"},{"location":"Git/04_github/","title":"GitHub Basics","text":"<p>Learn how to use GitHub to host your code, collaborate with others, and showcase your projects.</p>"},{"location":"Git/04_github/#connecting-to-github","title":"Connecting to GitHub","text":""},{"location":"Git/04_github/#creating-a-repository-on-github","title":"Creating a Repository on GitHub","text":"<ol> <li>Go to github.com and sign in</li> <li>Click the + icon \u2192 New repository</li> <li>Fill in repository details:</li> <li>Name: <code>my-awesome-project</code></li> <li>Description: Brief description</li> <li>Public or Private</li> <li>Initialize with README (optional)</li> <li>Click Create repository</li> </ol>"},{"location":"Git/04_github/#connecting-local-repository-to-github","title":"Connecting Local Repository to GitHub","text":""},{"location":"Git/04_github/#option-1-push-an-existing-repository","title":"Option 1: Push an Existing Repository","text":"<pre><code># Add GitHub as remote\ngit remote add origin https://github.com/username/repo-name.git\n\n# Push to GitHub\ngit push -u origin main\n</code></pre>"},{"location":"Git/04_github/#option-2-clone-from-github","title":"Option 2: Clone from GitHub","text":"<pre><code># Clone the repository\ngit clone https://github.com/username/repo-name.git\n\n# Navigate into it\ncd repo-name\n\n# Start working\n</code></pre>"},{"location":"Git/04_github/#ssh-vs-https","title":"SSH vs HTTPS","text":"<p>HTTPS URL: <pre><code>https://github.com/username/repo-name.git\n</code></pre></p> <p>SSH URL: <pre><code>git@github.com:username/repo-name.git\n</code></pre></p> <p>Setting up SSH:</p> <pre><code># Generate SSH key\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n\n# Copy public key\ncat ~/.ssh/id_ed25519.pub\n\n# Add to GitHub:\n# Settings \u2192 SSH and GPG keys \u2192 New SSH key\n</code></pre> <p>Change remote to SSH: <pre><code>git remote set-url origin git@github.com:username/repo-name.git\n</code></pre></p>"},{"location":"Git/04_github/#basic-github-workflow","title":"Basic GitHub Workflow","text":""},{"location":"Git/04_github/#pushing-changes","title":"Pushing Changes","text":"<pre><code># Make changes locally\necho \"New feature\" &gt;&gt; feature.py\ngit add feature.py\ngit commit -m \"Add new feature\"\n\n# Push to GitHub\ngit push origin main\n</code></pre>"},{"location":"Git/04_github/#pulling-changes","title":"Pulling Changes","text":"<pre><code># Get latest changes from GitHub\ngit pull origin main\n\n# Or in two steps\ngit fetch origin\ngit merge origin/main\n</code></pre>"},{"location":"Git/04_github/#fetching-vs-pulling","title":"Fetching vs Pulling","text":"Command What it does <code>git fetch</code> Downloads changes but doesn't merge them <code>git pull</code> Downloads and merges changes (fetch + merge) <pre><code># Fetch only\ngit fetch origin\n\n# See what changed\ngit log origin/main\n\n# Merge when ready\ngit merge origin/main\n</code></pre>"},{"location":"Git/04_github/#forking-repositories","title":"Forking Repositories","text":"<p>A fork is your personal copy of someone else's repository.</p>"},{"location":"Git/04_github/#how-to-fork","title":"How to Fork","text":"<ol> <li>Go to the repository you want to fork</li> <li>Click Fork button (top right)</li> <li>Choose your account</li> <li>Clone your fork:</li> </ol> <pre><code>git clone https://github.com/YOUR-USERNAME/repo-name.git\n</code></pre>"},{"location":"Git/04_github/#keeping-fork-updated","title":"Keeping Fork Updated","text":"<pre><code># Add original repo as upstream\ngit remote add upstream https://github.com/ORIGINAL-OWNER/repo-name.git\n\n# Fetch upstream changes\ngit fetch upstream\n\n# Merge into your main\ngit checkout main\ngit merge upstream/main\n\n# Push to your fork\ngit push origin main\n</code></pre>"},{"location":"Git/04_github/#pull-requests-prs","title":"Pull Requests (PRs)","text":"<p>Pull requests let you propose changes to a repository.</p>"},{"location":"Git/04_github/#creating-a-pull-request","title":"Creating a Pull Request","text":"<p>Step 1: Create a feature branch <pre><code>git switch -c feature-add-validation\n</code></pre></p> <p>Step 2: Make changes and push <pre><code># Make changes\necho \"def validate_email(email):\" &gt; validation.py\ngit add validation.py\ngit commit -m \"Add email validation function\"\n\n# Push branch to GitHub\ngit push origin feature-add-validation\n</code></pre></p> <p>Step 3: Open PR on GitHub 1. Go to your repository on GitHub 2. Click Pull requests \u2192 New pull request 3. Select your feature branch 4. Add title and description 5. Click Create pull request</p>"},{"location":"Git/04_github/#pr-description-template","title":"PR Description Template","text":"<pre><code>## What does this PR do?\nBrief description of changes\n\n## Why is this needed?\nExplain the problem this solves\n\n## How was it tested?\n- [ ] Unit tests added\n- [ ] Manual testing completed\n\n## Screenshots (if applicable)\n[Add screenshots]\n\n## Related Issues\nFixes #123\n</code></pre>"},{"location":"Git/04_github/#reviewing-pull-requests","title":"Reviewing Pull Requests","text":"<pre><code># Fetch PR locally to test\ngit fetch origin pull/123/head:pr-123\ngit switch pr-123\n\n# Test the changes\npython test.py\n\n# Switch back\ngit switch main\n</code></pre>"},{"location":"Git/04_github/#merging-pull-requests","title":"Merging Pull Requests","text":"<p>On GitHub, you can merge PRs in three ways:</p> <ol> <li>Merge commit - Preserves all commits and creates merge commit</li> <li>Squash and merge - Combines all commits into one</li> <li>Rebase and merge - Applies commits individually without merge commit</li> </ol>"},{"location":"Git/04_github/#github-issues","title":"GitHub Issues","text":"<p>Issues track bugs, features, and tasks.</p>"},{"location":"Git/04_github/#creating-an-issue","title":"Creating an Issue","text":"<ol> <li>Go to repository \u2192 Issues \u2192 New issue</li> <li>Add title and description</li> <li>Assign labels (bug, enhancement, documentation)</li> <li>Assign people or milestones</li> <li>Click Submit new issue</li> </ol>"},{"location":"Git/04_github/#issue-template-example","title":"Issue Template Example","text":"<pre><code>## Bug Report\n\n**Describe the bug**\nA clear description of what the bug is.\n\n**To Reproduce**\nSteps to reproduce:\n1. Go to '...'\n2. Click on '...'\n3. See error\n\n**Expected behavior**\nWhat should happen instead?\n\n**Screenshots**\nIf applicable, add screenshots.\n\n**Environment:**\n- OS: [e.g. macOS 13]\n- Python version: [e.g. 3.10]\n- Package version: [e.g. 1.2.3]\n</code></pre>"},{"location":"Git/04_github/#referencing-issues","title":"Referencing Issues","text":"<p>In commits and PRs:</p> <pre><code># Reference issue\ngit commit -m \"Add validation, addresses #45\"\n\n# Close issue automatically\ngit commit -m \"Fix login bug, closes #45\"\ngit commit -m \"Resolve #45\"\n</code></pre>"},{"location":"Git/04_github/#github-actions-cicd","title":"GitHub Actions (CI/CD)","text":"<p>Automate testing, building, and deployment.</p>"},{"location":"Git/04_github/#simple-python-test-workflow","title":"Simple Python Test Workflow","text":"<p>Create <code>.github/workflows/tests.yml</code>:</p> <pre><code>name: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.10'\n\n    - name: Install dependencies\n      run: |\n        pip install -r requirements.txt\n        pip install pytest\n\n    - name: Run tests\n      run: pytest tests/\n</code></pre> <p>This automatically runs tests on every push and PR.</p>"},{"location":"Git/04_github/#readme-best-practices","title":"README Best Practices","text":"<p>A good README includes:</p> <pre><code># Project Name\n\nBrief description of what this project does.\n\n## Features\n\n- Feature 1\n- Feature 2\n- Feature 3\n\n## Installation\n\n\\```bash\npip install package-name\n\\```\n\n## Usage\n\n\\```python\nfrom package import function\n\nresult = function()\n\\```\n\n## Examples\n\n[Link to examples folder]\n\n## Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md)\n\n## License\n\nMIT License - see [LICENSE](LICENSE)\n\n## Contact\n\nYour Name - [@twitter](https://twitter.com/handle)\nProject Link: https://github.com/username/repo\n</code></pre>"},{"location":"Git/04_github/#badges","title":"Badges","text":"<p>Add status badges to your README:</p> <pre><code>![Tests](https://github.com/username/repo/workflows/Tests/badge.svg)\n![Python](https://img.shields.io/badge/python-3.8+-blue.svg)\n![License](https://img.shields.io/badge/license-MIT-green.svg)\n</code></pre>"},{"location":"Git/04_github/#github-pages","title":"GitHub Pages","text":"<p>Host websites directly from your repository.</p>"},{"location":"Git/04_github/#quick-setup","title":"Quick Setup","text":"<ol> <li>Go to repository Settings \u2192 Pages</li> <li>Select source: <code>main</code> branch, <code>/docs</code> folder</li> <li>Save</li> <li>Your site will be at: <code>https://username.github.io/repo-name</code></li> </ol>"},{"location":"Git/04_github/#example-project-documentation","title":"Example: Project Documentation","text":"<p>Create <code>docs/index.html</code>:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;My Project&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Welcome to My Project&lt;/h1&gt;\n    &lt;p&gt;Documentation goes here&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Push and it's live!</p>"},{"location":"Git/04_github/#releases-and-tags","title":"Releases and Tags","text":""},{"location":"Git/04_github/#creating-a-release","title":"Creating a Release","text":"<p>1. Tag a version: <pre><code>git tag -a v1.0.0 -m \"First release\"\ngit push origin v1.0.0\n</code></pre></p> <p>2. Create release on GitHub: 1. Go to Releases \u2192 Draft a new release 2. Choose tag <code>v1.0.0</code> 3. Add release notes 4. Attach binaries if needed 5. Click Publish release</p>"},{"location":"Git/04_github/#semantic-versioning","title":"Semantic Versioning","text":"<p>Use <code>MAJOR.MINOR.PATCH</code> format:</p> <ul> <li><code>MAJOR</code> - Breaking changes (v2.0.0)</li> <li><code>MINOR</code> - New features (v1.1.0)</li> <li><code>PATCH</code> - Bug fixes (v1.0.1)</li> </ul> <p>Examples: - <code>v1.0.0</code> - Initial release - <code>v1.0.1</code> - Bug fix - <code>v1.1.0</code> - New feature - <code>v2.0.0</code> - Breaking changes</p>"},{"location":"Git/04_github/#github-repository-settings","title":"GitHub Repository Settings","text":""},{"location":"Git/04_github/#protecting-branches","title":"Protecting Branches","text":"<ol> <li>Settings \u2192 Branches \u2192 Add rule</li> <li>Branch name pattern: <code>main</code></li> <li>Enable:</li> <li>\u2705 Require pull request reviews</li> <li>\u2705 Require status checks to pass</li> <li>\u2705 Require branches to be up to date</li> </ol>"},{"location":"Git/04_github/#collaborators","title":"Collaborators","text":"<p>Add collaborators: 1. Settings \u2192 Collaborators 2. Click Add people 3. Enter GitHub username 4. Choose permission level</p>"},{"location":"Git/04_github/#gitignore-templates","title":".gitignore Templates","text":"<p>GitHub provides templates when creating repos:</p> <ul> <li>Python</li> <li>Node</li> <li>Java</li> <li>Ruby</li> <li>etc.</li> </ul> <p>Or use gitignore.io</p>"},{"location":"Git/04_github/#github-cli","title":"GitHub CLI","text":"<p>Command-line tool for GitHub operations.</p>"},{"location":"Git/04_github/#installation","title":"Installation","text":"<p>macOS: <pre><code>brew install gh\n</code></pre></p> <p>Windows: <pre><code>winget install GitHub.cli\n</code></pre></p> <p>Linux: <pre><code>sudo apt install gh\n</code></pre></p>"},{"location":"Git/04_github/#authentication","title":"Authentication","text":"<pre><code>gh auth login\n</code></pre>"},{"location":"Git/04_github/#common-commands","title":"Common Commands","text":"<pre><code># Create repo\ngh repo create my-project --public\n\n# Clone repo\ngh repo clone username/repo\n\n# Create issue\ngh issue create --title \"Bug report\" --body \"Description\"\n\n# Create PR\ngh pr create --title \"Add feature\" --body \"Description\"\n\n# View PR\ngh pr view 123\n\n# List PRs\ngh pr list\n\n# Check PR status\ngh pr status\n\n# Merge PR\ngh pr merge 123\n</code></pre>"},{"location":"Git/04_github/#practical-examples","title":"Practical Examples","text":""},{"location":"Git/04_github/#example-1-contributing-to-open-source","title":"Example 1: Contributing to Open Source","text":"<pre><code># 1. Fork repository on GitHub\n\n# 2. Clone your fork\ngit clone https://github.com/YOUR-USERNAME/repo-name.git\ncd repo-name\n\n# 3. Add upstream remote\ngit remote add upstream https://github.com/ORIGINAL-OWNER/repo-name.git\n\n# 4. Create feature branch\ngit switch -c fix-typo-in-readme\n\n# 5. Make changes\n# ... edit files ...\n\n# 6. Commit and push\ngit add README.md\ngit commit -m \"Fix typo in installation instructions\"\ngit push origin fix-typo-in-readme\n\n# 7. Create PR on GitHub\ngh pr create --title \"Fix README typo\" --body \"Fixed typo in installation section\"\n</code></pre>"},{"location":"Git/04_github/#example-2-team-collaboration","title":"Example 2: Team Collaboration","text":"<pre><code># 1. Clone team repository\ngit clone https://github.com/team/project.git\ncd project\n\n# 2. Create feature branch\ngit switch -c feature-user-dashboard\n\n# 3. Work and commit regularly\ngit add dashboard.py\ngit commit -m \"Add dashboard layout\"\n\n# 4. Push to get feedback\ngit push origin feature-user-dashboard\n\n# 5. Create draft PR\ngh pr create --draft --title \"WIP: User dashboard\"\n\n# 6. Continue working\n# ... more commits ...\n\n# 7. Mark as ready for review\ngh pr ready 123\n\n# 8. After approval, merge\ngh pr merge 123\n</code></pre>"},{"location":"Git/04_github/#best-practices","title":"Best Practices","text":""},{"location":"Git/04_github/#commit-messages","title":"Commit Messages","text":"<p>Follow conventions: <pre><code>type(scope): subject\n\nbody\n\nfooter\n</code></pre></p> <p>Types: - <code>feat:</code> New feature - <code>fix:</code> Bug fix - <code>docs:</code> Documentation - <code>style:</code> Formatting - <code>refactor:</code> Code restructuring - <code>test:</code> Tests - <code>chore:</code> Maintenance</p> <p>Example: <pre><code>feat(auth): add password reset functionality\n\nImplement password reset via email with token expiration.\nUsers can request a reset link that expires after 1 hour.\n\nCloses #45\n</code></pre></p>"},{"location":"Git/04_github/#project-organization","title":"Project Organization","text":"<pre><code>my-project/\n\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u2514\u2500\u2500 tests.yml\n\u251c\u2500\u2500 docs/\n\u2502   \u2514\u2500\u2500 index.md\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 main.py\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_main.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"Git/04_github/#security","title":"Security","text":"<p>Never commit: - API keys - Passwords - Private keys - Database credentials</p> <p>Use: - Environment variables - <code>.env</code> files (in <code>.gitignore</code>) - GitHub Secrets (for Actions)</p> <p>Previous: Branching and Merging | Next: Collaboration Workflows</p>"},{"location":"Git/05_collaboration/","title":"Collaboration Workflows","text":"<p>Learn professional workflows for collaborating with teams using Git and GitHub.</p>"},{"location":"Git/05_collaboration/#git-workflows-overview","title":"Git Workflows Overview","text":"<p>Different teams use different workflows based on their needs. Here are the most common:</p> Workflow Best For Complexity Centralized Small teams, simple projects Low Feature Branch Most teams Medium Gitflow Release-based projects High Forking Open source Medium Trunk-based CI/CD-heavy teams Low"},{"location":"Git/05_collaboration/#centralized-workflow","title":"Centralized Workflow","text":"<p>Everyone works on the <code>main</code> branch.</p>"},{"location":"Git/05_collaboration/#how-it-works","title":"How It Works","text":"<pre><code># Morning: Get latest changes\ngit pull origin main\n\n# Work on features\n# ... make changes ...\ngit add .\ngit commit -m \"Add new feature\"\n\n# Share changes\ngit pull origin main  # Get any new changes\ngit push origin main\n</code></pre>"},{"location":"Git/05_collaboration/#pros-and-cons","title":"Pros and Cons","text":"<p>Pros: - Simple and straightforward - Easy to learn - Good for small teams</p> <p>Cons: - Risk of breaking main branch - No isolation for features - Difficult code review</p> <p>Use when: - Team size: 1-3 people - Simple projects - Rapid prototyping</p>"},{"location":"Git/05_collaboration/#feature-branch-workflow","title":"Feature Branch Workflow","text":"<p>Each feature gets its own branch.</p>"},{"location":"Git/05_collaboration/#how-it-works_1","title":"How It Works","text":"<pre><code># 1. Start from main\ngit switch main\ngit pull origin main\n\n# 2. Create feature branch\ngit switch -c feature-user-profile\n\n# 3. Work on feature\necho \"def get_user_profile():\" &gt; profile.py\ngit add profile.py\ngit commit -m \"Add user profile function\"\n\n# 4. Push feature branch\ngit push origin feature-user-profile\n\n# 5. Create pull request on GitHub\n\n# 6. After review and approval, merge\ngit switch main\ngit pull origin main\ngit merge feature-user-profile\ngit push origin main\n\n# 7. Delete feature branch\ngit branch -d feature-user-profile\ngit push origin --delete feature-user-profile\n</code></pre>"},{"location":"Git/05_collaboration/#best-practices","title":"Best Practices","text":"<ul> <li>One feature per branch</li> <li>Keep branches short-lived (days, not weeks)</li> <li>Update from main regularly <pre><code>git switch feature-branch\ngit merge main  # Or rebase\n</code></pre></li> <li>Delete merged branches</li> </ul>"},{"location":"Git/05_collaboration/#pros-and-cons_1","title":"Pros and Cons","text":"<p>Pros: - Features isolated - Easy code review - Safe experimentation</p> <p>Cons: - Slightly more complex - Merge conflicts possible</p> <p>Use when: - Most professional projects - Need code review - Team size: 3+</p>"},{"location":"Git/05_collaboration/#gitflow-workflow","title":"Gitflow Workflow","text":"<p>Structured workflow with multiple long-lived branches.</p>"},{"location":"Git/05_collaboration/#branch-structure","title":"Branch Structure","text":"<pre><code>main (production)\n  \u2191\ndevelop (integration)\n  \u2191\nfeature/* (new features)\nrelease/* (release prep)\nhotfix/* (urgent fixes)\n</code></pre>"},{"location":"Git/05_collaboration/#branch-purposes","title":"Branch Purposes","text":"<ul> <li><code>main</code> - Production-ready code</li> <li><code>develop</code> - Integration branch</li> <li><code>feature/*</code> - New features</li> <li><code>release/*</code> - Release preparation</li> <li><code>hotfix/*</code> - Critical bug fixes</li> </ul>"},{"location":"Git/05_collaboration/#how-it-works_2","title":"How It Works","text":"<p>Starting a feature: <pre><code># Branch from develop\ngit switch develop\ngit pull origin develop\ngit switch -c feature/user-analytics\n\n# Work on feature\n# ... commits ...\n\n# Merge back to develop\ngit switch develop\ngit merge feature/user-analytics\ngit push origin develop\ngit branch -d feature/user-analytics\n</code></pre></p> <p>Creating a release: <pre><code># Branch from develop\ngit switch develop\ngit switch -c release/1.2.0\n\n# Bug fixes and prep\n# ... commits ...\n\n# Merge to main and develop\ngit switch main\ngit merge release/1.2.0\ngit tag -a v1.2.0 -m \"Release version 1.2.0\"\n\ngit switch develop\ngit merge release/1.2.0\n\n# Cleanup\ngit branch -d release/1.2.0\n</code></pre></p> <p>Hotfix: <pre><code># Branch from main\ngit switch main\ngit switch -c hotfix/critical-security\n\n# Fix the issue\n# ... commits ...\n\n# Merge to both main and develop\ngit switch main\ngit merge hotfix/critical-security\ngit tag -a v1.2.1 -m \"Hotfix 1.2.1\"\n\ngit switch develop\ngit merge hotfix/critical-security\n\n# Cleanup\ngit branch -d hotfix/critical-security\n</code></pre></p>"},{"location":"Git/05_collaboration/#pros-and-cons_2","title":"Pros and Cons","text":"<p>Pros: - Clear structure - Supports multiple releases - Organized release process</p> <p>Cons: - Complex for beginners - Overhead for simple projects - Longer feedback loops</p> <p>Use when: - Scheduled releases - Multiple versions supported - Large teams</p>"},{"location":"Git/05_collaboration/#forking-workflow","title":"Forking Workflow","text":"<p>Everyone has their own fork of the repository.</p>"},{"location":"Git/05_collaboration/#how-it-works_3","title":"How It Works","text":"<p>1. Fork the repository on GitHub</p> <p>2. Clone your fork: <pre><code>git clone https://github.com/YOUR-USERNAME/repo.git\ncd repo\n</code></pre></p> <p>3. Add upstream remote: <pre><code>git remote add upstream https://github.com/ORIGINAL-OWNER/repo.git\n</code></pre></p> <p>4. Create feature branch: <pre><code>git switch -c feature-new-algorithm\n</code></pre></p> <p>5. Work and commit: <pre><code># ... make changes ...\ngit add .\ngit commit -m \"Implement new sorting algorithm\"\n</code></pre></p> <p>6. Push to your fork: <pre><code>git push origin feature-new-algorithm\n</code></pre></p> <p>7. Create pull request: - From your fork to original repo - Original maintainers review</p> <p>8. Keep fork updated: <pre><code>git fetch upstream\ngit switch main\ngit merge upstream/main\ngit push origin main\n</code></pre></p>"},{"location":"Git/05_collaboration/#pros-and-cons_3","title":"Pros and Cons","text":"<p>Pros: - Contributors don't need write access - Perfect for open source - Maintainers have full control</p> <p>Cons: - More complex setup - Fork can get out of sync</p> <p>Use when: - Open source projects - External contributors - No direct write access</p>"},{"location":"Git/05_collaboration/#trunk-based-development","title":"Trunk-Based Development","text":"<p>Everyone commits to main frequently.</p>"},{"location":"Git/05_collaboration/#how-it-works_4","title":"How It Works","text":"<pre><code># 1. Pull latest\ngit pull origin main\n\n# 2. Create short-lived branch\ngit switch -c quick-fix\n\n# 3. Make small change\n# ... one small change ...\ngit add .\ngit commit -m \"Fix typo in header\"\n\n# 4. Merge immediately\ngit switch main\ngit pull origin main\ngit merge quick-fix\ngit push origin main\n\n# 5. Delete branch\ngit branch -d quick-fix\n</code></pre>"},{"location":"Git/05_collaboration/#key-principles","title":"Key Principles","text":"<ul> <li>Very short-lived branches (hours, not days)</li> <li>Small, frequent commits</li> <li>Feature flags for incomplete features</li> <li>Strong CI/CD</li> <li>Automated testing</li> </ul>"},{"location":"Git/05_collaboration/#feature-flags-example","title":"Feature Flags Example","text":"<pre><code># Use feature flags for incomplete features\nENABLE_NEW_DASHBOARD = os.getenv('FEATURE_NEW_DASHBOARD', 'false') == 'true'\n\nif ENABLE_NEW_DASHBOARD:\n    return render_new_dashboard()\nelse:\n    return render_old_dashboard()\n</code></pre>"},{"location":"Git/05_collaboration/#pros-and-cons_4","title":"Pros and Cons","text":"<p>Pros: - Fast integration - Reduced merge conflicts - Continuous deployment</p> <p>Cons: - Requires discipline - Needs strong testing - Feature flags add complexity</p> <p>Use when: - Strong CI/CD pipeline - Experienced team - Continuous deployment</p>"},{"location":"Git/05_collaboration/#code-review-best-practices","title":"Code Review Best Practices","text":""},{"location":"Git/05_collaboration/#for-authors","title":"For Authors","text":"<p>Before creating PR: <pre><code># Self-review your changes\ngit diff main..feature-branch\n\n# Check for common issues\n# - Remove debug code\n# - Add tests\n# - Update documentation\n</code></pre></p> <p>PR Description: <pre><code>## What\nAdd email validation to signup form\n\n## Why\nUsers were entering invalid emails causing 500 errors\n\n## How\n- Added regex validation\n- Added unit tests\n- Updated error messages\n\n## Testing\n- [x] Unit tests pass\n- [x] Manually tested with 10+ email formats\n- [x] Checked error handling\n\n## Screenshots\n[Add if UI changes]\n</code></pre></p>"},{"location":"Git/05_collaboration/#for-reviewers","title":"For Reviewers","text":"<p>Review checklist: - [ ] Code is understandable - [ ] Tests included - [ ] Documentation updated - [ ] No security issues - [ ] Follows coding standards - [ ] Performance considered</p> <p>Good review comments: <pre><code>Good catch on the edge case! \ud83d\udc4d\n\nConsider using a constant instead:\n\\```python\nMAX_RETRIES = 3\n\\```\n\nThis could be simplified:\n\\```python\n# Instead of:\nif x == True:\n    return True\nelse:\n    return False\n\n# Try:\nreturn x\n\\```\n</code></pre></p>"},{"location":"Git/05_collaboration/#responding-to-reviews","title":"Responding to Reviews","text":"<pre><code># Make requested changes\n# ... edit files ...\n\n# Commit with descriptive message\ngit add .\ngit commit -m \"Address review: Add input validation\"\n\n# Push to update PR\ngit push origin feature-branch\n</code></pre>"},{"location":"Git/05_collaboration/#handling-merge-conflicts-in-teams","title":"Handling Merge Conflicts in Teams","text":""},{"location":"Git/05_collaboration/#minimizing-conflicts","title":"Minimizing Conflicts","text":"<p>1. Pull frequently: <pre><code># Daily or even more often\ngit switch main\ngit pull origin main\ngit switch feature-branch\ngit merge main\n</code></pre></p> <p>2. Communicate: - Announce when working on shared files - Use draft PRs early - Break large features into smaller PRs</p> <p>3. Keep PRs small: - Easier to review - Less chance of conflicts - Faster to merge</p>"},{"location":"Git/05_collaboration/#resolving-team-conflicts","title":"Resolving Team Conflicts","text":"<p>1. Fetch latest: <pre><code>git fetch origin main\n</code></pre></p> <p>2. Try to merge: <pre><code>git merge origin/main\n</code></pre></p> <p>3. If conflicts, communicate: <pre><code># In PR or Slack\n\"Hey @teammate, our changes conflict in auth.py.\nLet's hop on a call to resolve together?\"\n</code></pre></p> <p>4. Resolve together: - Understand both changes - Decide on best approach - Test thoroughly</p> <p>5. Commit resolution: <pre><code>git add resolved-file.py\ngit commit -m \"Merge main, resolve auth.py conflicts with @teammate\"\n</code></pre></p>"},{"location":"Git/05_collaboration/#team-conventions","title":"Team Conventions","text":""},{"location":"Git/05_collaboration/#commit-message-convention","title":"Commit Message Convention","text":"<p>Agree on a format:</p> <pre><code>type(scope): subject\n\nbody\n\nfooter\n</code></pre> <p>Example: <pre><code>feat(auth): add two-factor authentication\n\nImplement TOTP-based 2FA using pyotp library.\nUsers can enable in settings.\n\nCloses #234\n</code></pre></p>"},{"location":"Git/05_collaboration/#branch-naming-convention","title":"Branch Naming Convention","text":"<pre><code>type/short-description\n\nExamples:\nfeature/user-dashboard\nbugfix/login-redirect\nhotfix/security-patch\ndocs/api-documentation\ntest/integration-tests\n</code></pre>"},{"location":"Git/05_collaboration/#pr-size-guidelines","title":"PR Size Guidelines","text":"<pre><code>Small:    &lt; 100 lines (ideal)\nMedium:   100-300 lines\nLarge:    300-500 lines\nToo big:  &gt; 500 lines (split it!)\n</code></pre>"},{"location":"Git/05_collaboration/#practical-team-scenarios","title":"Practical Team Scenarios","text":""},{"location":"Git/05_collaboration/#scenario-1-simultaneous-feature-development","title":"Scenario 1: Simultaneous Feature Development","text":"<p>Developer A: <pre><code>git switch -c feature-payments\n# ... work on payments ...\n</code></pre></p> <p>Developer B: <pre><code>git switch -c feature-notifications\n# ... work on notifications ...\n</code></pre></p> <p>Both push and create PRs</p> <p>Team lead reviews and merges: <pre><code># Merge feature-payments\ngh pr merge 101\n\n# Merge feature-notifications\ngh pr merge 102\n</code></pre></p>"},{"location":"Git/05_collaboration/#scenario-2-emergency-hotfix","title":"Scenario 2: Emergency Hotfix","text":"<p>Developer on-call: <pre><code># Pull latest\ngit switch main\ngit pull origin main\n\n# Create hotfix branch\ngit switch -c hotfix/critical-data-leak\n\n# Fix issue\n# ... make fix ...\ngit add .\ngit commit -m \"hotfix: patch data exposure vulnerability\"\n\n# Push and create urgent PR\ngit push origin hotfix/critical-data-leak\ngh pr create --title \"URGENT: Fix data leak\" --assignee @security-team\n\n# After fast review, merge\ngh pr merge --squash\n</code></pre></p>"},{"location":"Git/05_collaboration/#scenario-3-long-running-feature","title":"Scenario 3: Long-Running Feature","text":"<p>Week 1: <pre><code>git switch -c feature-ai-recommendations\n# ... initial work ...\ngit push origin feature-ai-recommendations\ngh pr create --draft --title \"WIP: AI recommendations\"\n</code></pre></p> <p>Week 2: <pre><code># Keep updated with main\ngit switch feature-ai-recommendations\ngit fetch origin\ngit merge origin/main\n\n# Continue work\n# ... more commits ...\ngit push origin feature-ai-recommendations\n</code></pre></p> <p>Week 3: <pre><code># Mark ready for review\ngh pr ready\n\n# After review, merge\ngh pr merge feature-ai-recommendations\n</code></pre></p>"},{"location":"Git/05_collaboration/#tools-for-collaboration","title":"Tools for Collaboration","text":""},{"location":"Git/05_collaboration/#git-aliases","title":"Git Aliases","text":"<p>Add to <code>~/.gitconfig</code>:</p> <pre><code>[alias]\n    co = checkout\n    br = branch\n    ci = commit\n    st = status\n    unstage = reset HEAD --\n    last = log -1 HEAD\n    visual = log --graph --oneline --all\n    sync = !git fetch origin &amp;&amp; git merge origin/main\n</code></pre> <p>Usage: <pre><code>git st        # instead of git status\ngit co main   # instead of git checkout main\ngit visual    # pretty branch visualization\n</code></pre></p>"},{"location":"Git/05_collaboration/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit framework:</p> <pre><code>pip install pre-commit\n</code></pre> <p>Create <code>.pre-commit-config.yaml</code>:</p> <pre><code>repos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n\n  - repo: https://github.com/psf/black\n    rev: 23.1.0\n    hooks:\n      - id: black\n</code></pre> <p>Install: <pre><code>pre-commit install\n</code></pre></p> <p>Now hooks run automatically before each commit!</p>"},{"location":"Git/05_collaboration/#summary-choosing-a-workflow","title":"Summary: Choosing a Workflow","text":"Project Type Recommended Workflow Solo project Centralized or Feature Branch Small team (2-5) Feature Branch Medium team (5-20) Feature Branch or Gitflow Large team (20+) Gitflow Open source Forking Continuous deployment Trunk-based <p>Start simple and evolve - Begin with Feature Branch workflow and adapt as needs grow.</p> <p>Previous: GitHub Basics | Next: Advanced Git</p>"},{"location":"Git/06_advanced/","title":"Advanced Git Techniques","text":"<p>Master advanced Git commands and techniques for power users.</p>"},{"location":"Git/06_advanced/#git-rebase","title":"Git Rebase","text":"<p>Rebase moves commits to a new base, creating a linear history.</p>"},{"location":"Git/06_advanced/#basic-rebase","title":"Basic Rebase","text":"<pre><code># Start on feature branch\ngit switch feature-branch\n\n# Rebase onto main\ngit rebase main\n</code></pre> <p>Before rebase: <pre><code>main:     A --- B --- C\n               \\\nfeature:        D --- E\n</code></pre></p> <p>After rebase: <pre><code>main:     A --- B --- C\n                       \\\nfeature:                D' --- E'\n</code></pre></p> <p>Note: <code>D'</code> and <code>E'</code> are new commits (different hash) with same changes.</p>"},{"location":"Git/06_advanced/#interactive-rebase","title":"Interactive Rebase","text":"<p>Modify commit history:</p> <pre><code># Rebase last 3 commits\ngit rebase -i HEAD~3\n</code></pre> <p>Editor opens: <pre><code>pick abc123 Add feature A\npick def456 Fix typo\npick ghi789 Update docs\n\n# Commands:\n# p, pick = use commit\n# r, reword = change commit message\n# e, edit = edit commit\n# s, squash = combine with previous commit\n# f, fixup = like squash, but discard message\n# d, drop = remove commit\n</code></pre></p>"},{"location":"Git/06_advanced/#common-rebase-tasks","title":"Common Rebase Tasks","text":"<p>1. Squash commits: <pre><code>pick abc123 Add feature A\nsquash def456 Fix typo in feature A\nsquash ghi789 Add tests for feature A\n</code></pre></p> <p>Results in one clean commit.</p> <p>2. Reword message: <pre><code>pick abc123 Add feature A\nreword def456 Fix typo\npick ghi789 Update docs\n</code></pre></p> <p>3. Reorder commits: <pre><code>pick ghi789 Update docs\npick abc123 Add feature A\npick def456 Fix typo\n</code></pre></p>"},{"location":"Git/06_advanced/#rebase-vs-merge","title":"Rebase vs Merge","text":"Aspect Merge Rebase History Preserves all history Creates linear history Commits Keeps original commits Creates new commits Use case Public branches Private branches Conflicts Resolved once May need multiple resolutions <p>Golden Rule: Never rebase public branches (shared with others)!</p>"},{"location":"Git/06_advanced/#git-stash","title":"Git Stash","text":"<p>Temporarily save changes without committing.</p>"},{"location":"Git/06_advanced/#basic-stash","title":"Basic Stash","text":"<pre><code># Save changes\ngit stash\n\n# Or with a message\ngit stash save \"Work in progress on login feature\"\n\n# List stashes\ngit stash list\n\n# Apply most recent stash\ngit stash apply\n\n# Apply and remove from stash list\ngit stash pop\n\n# Apply specific stash\ngit stash apply stash@{2}\n</code></pre>"},{"location":"Git/06_advanced/#stash-options","title":"Stash Options","text":"<pre><code># Stash including untracked files\ngit stash -u\n\n# Stash including ignored files\ngit stash -a\n\n# Create branch from stash\ngit stash branch feature-name stash@{0}\n\n# View stash contents\ngit stash show -p stash@{0}\n\n# Delete a stash\ngit stash drop stash@{0}\n\n# Clear all stashes\ngit stash clear\n</code></pre>"},{"location":"Git/06_advanced/#practical-stash-example","title":"Practical Stash Example","text":"<pre><code># Working on feature\necho \"def new_feature():\" &gt; feature.py\n\n# Urgent bug reported!\ngit stash save \"WIP: new feature implementation\"\n\n# Fix bug\ngit switch main\necho \"fix bug\" &gt; bugfix.py\ngit add bugfix.py\ngit commit -m \"Fix critical bug\"\n\n# Back to feature\ngit switch feature-branch\ngit stash pop\n\n# Continue working\n</code></pre>"},{"location":"Git/06_advanced/#cherry-pick","title":"Cherry-Pick","text":"<p>Apply specific commits from one branch to another.</p>"},{"location":"Git/06_advanced/#basic-cherry-pick","title":"Basic Cherry-Pick","text":"<pre><code># On target branch\ngit switch main\n\n# Cherry-pick a commit\ngit cherry-pick abc1234\n\n# Cherry-pick multiple commits\ngit cherry-pick abc1234 def5678\n\n# Cherry-pick a range\ngit cherry-pick abc1234..def5678\n</code></pre>"},{"location":"Git/06_advanced/#cherry-pick-with-edit","title":"Cherry-Pick with Edit","text":"<pre><code># Cherry-pick but don't commit yet\ngit cherry-pick -n abc1234\n\n# Make changes\n# ... edit files ...\n\n# Commit when ready\ngit commit -m \"Cherry-picked and modified abc1234\"\n</code></pre>"},{"location":"Git/06_advanced/#when-to-use-cherry-pick","title":"When to Use Cherry-Pick","text":"<ul> <li> <p>Hotfix to multiple branches <pre><code># Fix on main\ngit switch main\ngit commit -m \"Fix security issue\"\n\n# Apply to release branch\ngit switch release-2.0\ngit cherry-pick main\n</code></pre></p> </li> <li> <p>Pull single feature from experimental branch</p> </li> <li>Apply bug fix to older versions</li> </ul>"},{"location":"Git/06_advanced/#git-reflog","title":"Git Reflog","text":"<p>History of all Git operations (safety net).</p>"},{"location":"Git/06_advanced/#viewing-reflog","title":"Viewing Reflog","text":"<pre><code># Show recent operations\ngit reflog\n\n# Show for specific branch\ngit reflog show feature-branch\n\n# Show last 10 entries\ngit reflog -10\n</code></pre> <p>Example output: <pre><code>abc1234 HEAD@{0}: commit: Add new feature\ndef5678 HEAD@{1}: checkout: moving from main to feature\nghi9012 HEAD@{2}: commit: Fix bug\n</code></pre></p>"},{"location":"Git/06_advanced/#recovering-lost-commits","title":"Recovering Lost Commits","text":"<p>Scenario: Accidentally deleted a branch</p> <pre><code># Oops!\ngit branch -D important-feature\n\n# Find the commit\ngit reflog\n\n# Output shows:\n# abc1234 HEAD@{5}: commit: Important feature complete\n\n# Recover\ngit switch -c important-feature abc1234\n</code></pre>"},{"location":"Git/06_advanced/#undoing-a-reset","title":"Undoing a Reset","text":"<pre><code># Accidentally reset\ngit reset --hard HEAD~3\n\n# Find previous state\ngit reflog\n# Output: def5678 HEAD@{1}: reset: moving to HEAD~3\n\n# Recover\ngit reset --hard HEAD@{1}\n</code></pre> <p>Reflog Expiration</p> <p>Reflog entries expire after 90 days by default. Use it soon after mistakes!</p>"},{"location":"Git/06_advanced/#git-reset","title":"Git Reset","text":"<p>Move branch pointer and optionally modify working directory.</p>"},{"location":"Git/06_advanced/#reset-modes","title":"Reset Modes","text":"<pre><code># Soft: Move HEAD, keep changes staged\ngit reset --soft HEAD~1\n\n# Mixed (default): Move HEAD, unstage changes\ngit reset HEAD~1\ngit reset --mixed HEAD~1\n\n# Hard: Move HEAD, discard all changes\ngit reset --hard HEAD~1\n</code></pre>"},{"location":"Git/06_advanced/#visual-comparison","title":"Visual Comparison","text":"<p>Before: <pre><code>A --- B --- C (HEAD)\n        Staged: changes.py\n        Working: more-changes.py\n</code></pre></p> <p>After <code>git reset --soft HEAD~1</code>: <pre><code>A --- B (HEAD)\n    Staged: changes.py, more-changes.py\n</code></pre></p> <p>After <code>git reset --mixed HEAD~1</code>: <pre><code>A --- B (HEAD)\n    Working: changes.py, more-changes.py\n</code></pre></p> <p>After <code>git reset --hard HEAD~1</code>: <pre><code>A --- B (HEAD)\n    (all changes lost!)\n</code></pre></p>"},{"location":"Git/06_advanced/#common-reset-uses","title":"Common Reset Uses","text":"<p>1. Undo last commit (keep changes): <pre><code>git reset --soft HEAD~1\n</code></pre></p> <p>2. Unstage files: <pre><code>git reset filename.py\n</code></pre></p> <p>3. Discard all changes: <pre><code>git reset --hard HEAD\n</code></pre></p> <p>4. Move to specific commit: <pre><code>git reset --hard abc1234\n</code></pre></p>"},{"location":"Git/06_advanced/#git-revert","title":"Git Revert","text":"<p>Create new commits that undo previous commits.</p>"},{"location":"Git/06_advanced/#basic-revert","title":"Basic Revert","text":"<pre><code># Revert a commit (creates new commit)\ngit revert abc1234\n\n# Revert without committing yet\ngit revert -n abc1234\n\n# Revert a merge commit\ngit revert -m 1 merge-commit-hash\n</code></pre>"},{"location":"Git/06_advanced/#reset-vs-revert","title":"Reset vs Revert","text":"Aspect Reset Revert Changes history Yes No Creates commit No Yes Safe for public branches No Yes Use when Fixing local mistakes Undoing public commits <p>Reset: \"This never happened\" Revert: \"Let's undo that with a new commit\"</p>"},{"location":"Git/06_advanced/#git-bisect","title":"Git Bisect","text":"<p>Find which commit introduced a bug using binary search.</p>"},{"location":"Git/06_advanced/#basic-bisect","title":"Basic Bisect","text":"<pre><code># Start bisect\ngit bisect start\n\n# Mark current commit as bad\ngit bisect bad\n\n# Mark last known good commit\ngit bisect good abc1234\n\n# Git checks out a middle commit\n# Test if bug exists\n\n# If bug exists:\ngit bisect bad\n\n# If bug doesn't exist:\ngit bisect good\n\n# Repeat until Git finds the culprit\n\n# End bisect\ngit bisect reset\n</code></pre>"},{"location":"Git/06_advanced/#automated-bisect","title":"Automated Bisect","text":"<pre><code># Use a test script\ngit bisect start HEAD abc1234\ngit bisect run python test.py\n\n# Git automatically finds the bad commit\n</code></pre> <p>test.py example: <pre><code>import sys\n\ndef test_feature():\n    # Your test here\n    assert feature_works() == True\n\nif __name__ == \"__main__\":\n    try:\n        test_feature()\n        sys.exit(0)  # Good\n    except:\n        sys.exit(1)  # Bad\n</code></pre></p>"},{"location":"Git/06_advanced/#git-submodules","title":"Git Submodules","text":"<p>Include other Git repositories in your project.</p>"},{"location":"Git/06_advanced/#adding-submodule","title":"Adding Submodule","text":"<pre><code># Add submodule\ngit submodule add https://github.com/user/repo.git external/repo\n\n# Commit\ngit add .gitmodules external/repo\ngit commit -m \"Add submodule\"\n</code></pre>"},{"location":"Git/06_advanced/#cloning-with-submodules","title":"Cloning with Submodules","text":"<pre><code># Clone and initialize submodules\ngit clone --recurse-submodules https://github.com/user/main-repo.git\n\n# Or if already cloned\ngit submodule init\ngit submodule update\n</code></pre>"},{"location":"Git/06_advanced/#updating-submodules","title":"Updating Submodules","text":"<pre><code># Update to latest\ngit submodule update --remote\n\n# Or update specific submodule\ngit submodule update --remote external/repo\n</code></pre>"},{"location":"Git/06_advanced/#git-worktree","title":"Git Worktree","text":"<p>Multiple working directories from one repository.</p>"},{"location":"Git/06_advanced/#basic-worktree","title":"Basic Worktree","text":"<pre><code># Create new worktree\ngit worktree add ../project-hotfix hotfix-branch\n\n# Now you have two directories:\n# ./project (main)\n# ../project-hotfix (hotfix-branch)\n\n# Work in hotfix directory\ncd ../project-hotfix\n# ... make changes ...\n\n# Remove worktree when done\ngit worktree remove ../project-hotfix\n</code></pre>"},{"location":"Git/06_advanced/#use-cases","title":"Use Cases","text":"<p>1. Review PR while continuing work: <pre><code># Create worktree for PR review\ngit worktree add ../review-pr-123 pr-123\n\n# Review in separate directory\ncd ../review-pr-123\n\n# Continue working in main directory\n</code></pre></p> <p>2. Test different branches simultaneously</p>"},{"location":"Git/06_advanced/#git-filter-branch","title":"Git Filter-Branch","text":"<p>Rewrite history (use carefully!).</p>"},{"location":"Git/06_advanced/#remove-file-from-history","title":"Remove File from History","text":"<pre><code># Remove sensitive file from all commits\ngit filter-branch --tree-filter 'rm -f passwords.txt' HEAD\n\n# Better: use BFG Repo Cleaner instead\n# https://rtyley.github.io/bfg-repo-cleaner/\n</code></pre>"},{"location":"Git/06_advanced/#change-author-email","title":"Change Author Email","text":"<pre><code>git filter-branch --env-filter '\nif [ \"$GIT_AUTHOR_EMAIL\" = \"old@email.com\" ]; then\n    export GIT_AUTHOR_EMAIL=\"new@email.com\"\nfi\n' HEAD\n</code></pre> <p>Danger Zone</p> <p>Filter-branch rewrites history. Use only when necessary and never on public branches!</p>"},{"location":"Git/06_advanced/#advanced-git-configuration","title":"Advanced Git Configuration","text":""},{"location":"Git/06_advanced/#useful-config-settings","title":"Useful Config Settings","text":"<pre><code># Better diffs for Python\ngit config --global diff.python.xfuncname \"^[ \\t]*def .*\"\n\n# Auto-correct typos\ngit config --global help.autocorrect 20\n\n# Reuse recorded conflict resolutions\ngit config --global rerere.enabled true\n\n# Better merge conflict style\ngit config --global merge.conflictstyle diff3\n\n# Default pull behavior\ngit config --global pull.rebase true\n\n# Default push behavior\ngit config --global push.default simple\n\n# Prune on fetch\ngit config --global fetch.prune true\n</code></pre>"},{"location":"Git/06_advanced/#git-aliases","title":"Git Aliases","text":"<pre><code># View pretty log\ngit config --global alias.lg \"log --graph --oneline --all --decorate\"\n\n# Undo last commit\ngit config --global alias.undo \"reset --soft HEAD~1\"\n\n# Show aliases\ngit config --global alias.aliases \"config --get-regexp ^alias\\.\"\n\n# Quick commit\ngit config --global alias.cm \"commit -m\"\n\n# Push with force-with-lease (safer)\ngit config --global alias.pushf \"push --force-with-lease\"\n</code></pre>"},{"location":"Git/06_advanced/#performance-tips","title":"Performance Tips","text":""},{"location":"Git/06_advanced/#shallow-clone","title":"Shallow Clone","text":"<pre><code># Clone only recent history\ngit clone --depth 1 https://github.com/user/repo.git\n\n# Fetch more history later\ngit fetch --depth 100\n</code></pre>"},{"location":"Git/06_advanced/#sparse-checkout","title":"Sparse Checkout","text":"<p>Work with only part of a repository:</p> <pre><code>git clone --filter=blob:none --sparse https://github.com/user/repo.git\ncd repo\ngit sparse-checkout init --cone\ngit sparse-checkout set docs tests\n</code></pre>"},{"location":"Git/06_advanced/#maintenance","title":"Maintenance","text":"<pre><code># Optimize repository\ngit gc\n\n# Aggressive optimization\ngit gc --aggressive\n\n# Verify repository integrity\ngit fsck\n\n# Clean untracked files\ngit clean -fdx\n</code></pre>"},{"location":"Git/06_advanced/#best-practices","title":"Best Practices","text":"<ol> <li>Never rewrite public history</li> <li>Use rebase for local cleanup</li> <li>Use merge for integrating features</li> <li>Stash for quick context switches</li> <li>Cherry-pick sparingly</li> <li>Keep commits atomic</li> <li>Write descriptive commit messages</li> </ol>"},{"location":"Git/06_advanced/#emergency-commands","title":"Emergency Commands","text":"<pre><code># Undo almost anything\ngit reflog\ngit reset --hard HEAD@{1}\n\n# Recover deleted branch\ngit reflog\ngit switch -c recovered-branch abc1234\n\n# Abort operations\ngit merge --abort\ngit rebase --abort\ngit cherry-pick --abort\n\n# Save work quickly\ngit stash --include-untracked\n</code></pre> <p>Previous: Collaboration Workflows</p>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/","title":"Overview of Machine Learning","text":"<p>title: Overview of Machine Learning author: Juma Shafara date: \"2024-01\" date-modified: \"2024-09-06\" description: Machine learning (ML) is a branch of artificial intelligence (AI) and computer science that focuses on using data and algorithms to enable AI to imitate the way that humans learn, and gradually improve. keywords: [What is Machine Learning, ]</p> <p></p> <pre><code>import pandas as pd\nfrom sklearn.datasets import load_iris\n\n# Load Iris dataset\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = pd.Series(iris.target, name='species')\n\n# Display the first 5 rows of the dataset\nX.head()\n</code></pre> sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 <pre><code>from sklearn.model_selection import train_test_split\n\n# Split data into train and test sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Check for missing values\nprint(X.isnull().sum())\n</code></pre> <pre>\n<code>sepal length (cm)    0\nsepal width (cm)     0\npetal length (cm)    0\npetal width (cm)     0\ndtype: int64\n</code>\n</pre> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Pairplot to visualize relationships between features\nsns.pairplot(pd.concat([X, y], axis=1), hue=\"species\")\nplt.show()\n</code></pre> <pre><code>from sklearn.linear_model import LogisticRegression\n\n# Initialize and train the model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n</code></pre> <pre>LogisticRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. <p>\u00a0\u00a0LogisticRegression?Documentation for LogisticRegressioniFitted<pre>LogisticRegression()</pre> <p> </p>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#1-introduction-to-machine-learning","title":"1. Introduction to Machine Learning","text":"<p>Machine learning (ML) is a branch of artificial intelligence that involves training algorithms to make predictions or decisions based on data. It's widely used in many fields, such as healthcare, finance, and marketing.</p>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#2-types-of-machine-learning","title":"2. Types of Machine Learning","text":"<ul> <li>Supervised Learning: Learn from labeled data.</li> <li>Unsupervised Learning: Discover patterns in unlabeled data.</li> <li>Reinforcement Learning: Agents learn to make decisions by interacting with the environment.</li> </ul>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#3-key-concepts-in-machine-learning","title":"3. Key Concepts in Machine Learning","text":"<ul> <li>Feature: A measurable property of the data.</li> <li>Label: The target variable (what you're predicting).</li> <li>Training Set: The data used to train the model.</li> <li>Test Set: The data used to evaluate the model's performance.</li> </ul>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#4-steps-in-a-machine-learning-workflow","title":"4. Steps in a Machine Learning Workflow","text":"<ol> <li>Data Collection</li> <li>Data Preprocessing</li> <li>Feature Engineering</li> <li>Model Selection</li> <li>Model Training</li> <li>Model Evaluation</li> <li>Hyperparameter Tuning</li> </ol>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#5-real-world-application-with-iris-dataset","title":"5. Real-world Application with Iris Dataset","text":"<pre><code>from sklearn.metrics import accuracy_score, confusion_matrix\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n</code></pre> <pre>\n<code>Accuracy: 1.0\nConfusion Matrix:\n[[15  0  0]\n [ 0 11  0]\n [ 0  0 12]]\n</code>\n</pre> <pre><code>from sklearn.model_selection import GridSearchCV\n\n# Define parameter grid\nparam_grid = {'C': [0.1, 1, 10], 'solver': ['lbfgs', 'liblinear']}\n\n# Grid search\ngrid_search = GridSearchCV(LogisticRegression(max_iter=200), param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best Parameters: \", grid_search.best_params_)\n</code></pre> <pre>\n<code>Best Parameters:  {'C': 1, 'solver': 'lbfgs'}\n</code>\n</pre>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#a-data-loading","title":"(a) Data Loading","text":"<p>We will load the Iris dataset using <code>sklearn.datasets</code>.</p>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#b-data-preprocessing","title":"(b) Data Preprocessing","text":"<p>We check for missing values, and split the data into training and testing sets.</p>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#c-exploratory-data-analysis-eda","title":"(c) Exploratory Data Analysis (EDA)","text":"<p>We'll visualize the relationship between features and the target label.</p>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#d-model-training","title":"(d) Model Training","text":"<p>We will train a Logistic Regression model, which is a simple yet effective supervised learning algorithm.</p>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#e-model-evaluation","title":"(e) Model Evaluation","text":"<p>We evaluate the model's performance using accuracy and confusion matrix.</p>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#f-hyperparameter-tuning","title":"(f) Hyperparameter Tuning","text":"<p>We use <code>GridSearchCV</code> to find the best hyperparameters for our model.</p>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#g-conclusion","title":"(g) Conclusion","text":"<p>Congratulations on reaching the end of the tutorial, with the simple Logistic Regression model, we achieved a good accuracy on the Iris dataset. This example shows how to:</p> <ul> <li> Load and preprocess data</li> <li> Perform EDA</li> <li> Train and evaluate a model</li> <li> Tune hyperparameters for better performance</li> </ul>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Machine%20Learning/42_training_models_meaning/","title":"Training Models","text":"<p>title: Training a Model, Meaning author: Juma Shafara date: \"2024-02\" date-modified: \"2024-07-25\" description: Training a model means finding the best parameters that describe the relationship between our input data and the output we want to predict. keywords: [Understanding Linear Regression, Fitting a Model]</p> <p></p> <p>In this lesson you will learn what it means to train a machine learning for:</p> <ul> <li> Linear Regression</li> <li> Logistic Regression</li> </ul> <pre><code># Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n</code></pre> <pre><code># Creating sample data\n # Independent variable (feature)\nX = np.array([[1], [2], [3], [4], [5]]) \n\n# Dependent variable (target)\ny = np.array([2, 4, 5, 4, 5])             \n</code></pre> <pre><code># Creating a linear regression model\nlinear_regression_model = LinearRegression()\n\n# Fitting the linear_regression_model to our data\nlinear_regression_model.fit(X, y)\n</code></pre> <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. <p>\u00a0\u00a0LinearRegression?Documentation for LinearRegressioniFitted<pre>LinearRegression()</pre> <p> </p> <pre><code># Printing the slope (coefficient) and intercept of the best-fitting line\nprint(\"Slope (m):\", linear_regression_model.coef_[0])\nprint(\"Intercept (b):\", linear_regression_model.intercept_)\n</code></pre> <pre>\n<code>Slope (m): 0.6\nIntercept (b): 2.2\n</code>\n</pre> <pre><code># Predictions\ny_predicted = linear_regression_model.predict(X)\n</code></pre> <pre><code>y_predicted\n</code></pre> <pre>\n<code>array([2.8, 3.4, 4. , 4.6, 5.2])</code>\n</pre> <ul> <li>In this code:<ul> <li><code>X</code> represents the independent variable (feature), which is a column vector in this case.</li> <li><code>y</code> represents the dependent variable (target).</li> <li>We create a <code>LinearRegression</code> model object.</li> <li>We fit the model to our data using the <code>.fit()</code> method.</li> <li>Finally, we print out the slope (coefficient) and intercept of the best-fitting line.</li> </ul> </li> </ul> <p></p> <p>Let's do some visualization</p> <pre><code># Plotting the linear regression line\nplt.scatter(X, y, color='blue', label='Data Points')\nplt.plot(X, y_predicted, color='red', label='Linear Regression Line')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Linear Regression')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p>So, in summary, \"fitting a model\" means finding the best parameters (like slope and intercept in the case of linear regression) that describe the relationship between our input data and the output we want to predict.</p> <p>If we have multiple independent variables (features) in \\(X\\), the process is still the same, but the equation becomes more complex. This is known as multiple linear regression.</p> <p>Here's how it works:</p> <p>Here are the corrected texts with LaTeX formulas suitable for Jupyter notebooks:</p> <pre><code># Creating sample data with multiple variables\n# Independent variables (features)\nX = np.array([[1, 2], [2, 4], [3, 6], [4, 8], [5, 10]])  \n\n# Dependent variable (target)\ny = np.array([2, 4, 5, 4, 5])                            \n</code></pre> <pre><code># Creating a multiple linear regression model\nmultiple_linear_regression_model = LinearRegression()\n\n# Fitting the multiple_linear_regression_model to our data\nmultiple_linear_regression_model.fit(X, y)\n</code></pre> <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. <p>\u00a0\u00a0LinearRegression?Documentation for LinearRegressioniFitted<pre>LinearRegression()</pre> <p> </p> <pre><code># Printing the coefficients (intercept and slopes) of the best-fitting line\nprint(\"Intercept (b0):\", multiple_linear_regression_model.intercept_)\nprint(\"Coefficients (b1, b2):\", multiple_linear_regression_model.coef_)\n</code></pre> <pre>\n<code>Intercept (b0): 2.200000000000001\nCoefficients (b1, b2): [0.12 0.24]\n</code>\n</pre> <ul> <li>In this code:<ul> <li><code>X</code> represents the independent variables (features), where each row is a data point and each column represents a different feature.</li> <li><code>y</code> represents the dependent variable (target).</li> <li>We create a <code>LinearRegression</code> model object.</li> <li>We fit the model to our data using the <code>.fit()</code> method.</li> <li>Finally, we print out the intercept and coefficients of the best-fitting line.</li> </ul> </li> </ul> <p>So, fitting a model with many variables involves finding the best parameters (coefficients) that describe the relationship between our input data (multiple independent variables) and the output we want to predict.</p> <p></p>"},{"location":"Machine%20Learning/42_training_models_meaning/#for-one-independent-variable-feature","title":"For one independent variable (feature)","text":""},{"location":"Machine%20Learning/42_training_models_meaning/#1-understanding-linear-regression","title":"1. Understanding Linear Regression:","text":"<ul> <li>Linear regression is a statistical method used to model the relationship between a dependent variable (often denoted as \\(y\\)) and one or more independent variables (often denoted as \\(x\\)).</li> <li>The relationship is modeled as a straight line equation: \\(y = mx + b\\), where \\(m\\) is the slope of the line and \\(b\\) is the y-intercept.</li> </ul>"},{"location":"Machine%20Learning/42_training_models_meaning/#2-fitting-a-model","title":"2. Fitting a Model:","text":"<ul> <li>When we say we're \"fitting a model\" in the context of linear regression, it means we're determining the best-fitting line (or plane in higher dimensions) that represents the relationship between the independent variable(s) and the dependent variable.</li> <li>This process involves finding the values of \\(m\\) and \\(b\\) that minimize the difference between the actual observed values of the dependent variable and the values predicted by the model.</li> <li>In simpler terms, fitting a model means finding the line that best describes the relationship between our input data and the output we want to predict.</li> </ul>"},{"location":"Machine%20Learning/42_training_models_meaning/#3-python-code-example","title":"3. Python Code Example:","text":"<ul> <li>Here's a simple example of how you might fit a linear regression model using Python, particularly with the <code>scikit-learn</code> library:</li> </ul>"},{"location":"Machine%20Learning/42_training_models_meaning/#for-multiple-independent-variables","title":"For Multiple Independent Variables","text":""},{"location":"Machine%20Learning/42_training_models_meaning/#1-understanding-multiple-linear-regression","title":"1. Understanding Multiple Linear Regression:","text":"<ul> <li>Instead of a single independent variable \\(x\\), we have multiple independent variables represented as a matrix \\(X\\).</li> <li>The relationship between the dependent variable \\(y\\) and the independent variables \\(X\\) is modeled as:     \\(\\(y = b_0 + b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n\\)\\)    where \\(b_0\\) is the intercept, \\(b_1, b_2, \\ldots, b_n\\) are the coefficients corresponding to each independent variable \\(x_1, x_2, \\ldots, x_n\\).</li> </ul> <p>Logistic regression is a type of regression analysis used for predicting the probability of a binary outcome based on one or more predictor variables. Here's how the fitting process works with logistic regression:</p> <pre><code># z = b_0 + b_1x_1 + ... + b_nx_n\nsigmoid = lambda z: 1 / (1 + np.exp(-z))\n</code></pre> <pre><code>def sigmoid(z):\n    prob = 1 / (1 + np.exp(-z))\n    return prob\n</code></pre> <pre><code>print('z = 1000000:', sigmoid(z=1000000))\nprint('z = 0.0000001:', sigmoid(z=0.0000001))\n</code></pre> <pre>\n<code>z = 1000000: 1.0\nz = 0.0000001: 0.500000025\n</code>\n</pre> <pre><code># import the logistic regression model\nfrom sklearn.linear_model import LogisticRegression\n</code></pre> <pre><code># Creating sample data\nX = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9]])  # Independent variable\ny = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])             # Binary outcome (0 or 1)\n</code></pre> <pre><code># Creating a logistic regression model\nlogistic_regression_model = LogisticRegression()\n\n# Fitting the model to our data\nlogistic_regression_model.fit(X, y)\n</code></pre> <pre>LogisticRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. <p>\u00a0\u00a0LogisticRegression?Documentation for LogisticRegressioniFitted<pre>LogisticRegression()</pre> <p> </p> <pre><code># Printing the intercept and coefficient(s) of the best-fitting logistic curve\nprint(\"Intercept (b0):\", logistic_regression_model.intercept_)\nprint(\"Coefficient (b1):\", logistic_regression_model.coef_)\n</code></pre> <pre>\n<code>Intercept (b0): [-5.29559243]\nCoefficient (b1): [[1.17808562]]\n</code>\n</pre> <ul> <li>In this code:<ul> <li><code>X</code> represents the independent variable.</li> <li><code>y</code> represents the binary outcome.</li> <li>We create a <code>LogisticRegression</code> model object.</li> <li>We fit the model to our data using the <code>.fit()</code> method.</li> <li>Finally, we print out the intercept and coefficient(s) of the best-fitting logistic curve.</li> </ul> </li> </ul> <pre><code># Predicted probabilities\nprobabilities = logistic_regression_model.predict_proba(X)\n</code></pre> <pre><code>probability_of_1 = logistic_regression_model.predict_proba(X)[:, 1]\n</code></pre> <pre><code># Plotting the logistic regression curve\nplt.scatter(X, y, color='blue', label='Data Points')\nplt.plot(probability_of_1, color='red', label='Logistic Regression Curve')\nplt.xlabel('X')\nplt.ylabel('Probability')\nplt.title('Logistic Regression')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p>So, fitting a logistic regression model involves finding the best parameters (coefficients) that describe the relationship between our input data and the probability of the binary outcome.</p> <pre><code>from dataidea import np,pd\n</code></pre> <pre><code>dataf = pd.DataFrame(\n    data={\n        'income': [1000, 2000, 3000],\n        'age': [10, 20, 30]\n    }\n)\n\ndataf\n</code></pre> income age 0 1000 10 1 2000 20 2 3000 30"},{"location":"Machine%20Learning/42_training_models_meaning/#2-fitting-a-model-with-many-variables","title":"2. Fitting a Model with Many Variables:","text":"<ul> <li>Fitting the model involves finding the values of the coefficients \\(b_0, b_1, \\ldots, b_n\\) that minimize the difference between the actual observed values of the dependent variable and the values predicted by the model.</li> <li>The process is essentially the same as in simple linear regression, but with more coefficients to estimate.</li> </ul>"},{"location":"Machine%20Learning/42_training_models_meaning/#3-python-code-example_1","title":"3. Python Code Example:","text":"<ul> <li>Here's how you might fit a multiple linear regression model using Python:</li> </ul>"},{"location":"Machine%20Learning/42_training_models_meaning/#what-about-logistic-regression","title":"What about Logistic Regression?","text":""},{"location":"Machine%20Learning/42_training_models_meaning/#1-understanding-logistic-regression","title":"1. Understanding Logistic Regression:","text":"<ul> <li>Logistic regression models the probability that a given input belongs to a particular category (binary classification problem).</li> <li>Instead of fitting a straight line or plane like in linear regression, logistic regression uses the logistic function (also known as the sigmoid function) to model the relationship between the independent variables and the probability of the binary outcome.</li> <li> <p>The logistic function is defined as:</p> <p>\\(\\(P(y=1 \\,|\\, X) = \\frac{1}{1 + e^{-(b_0 + b_1x_1 + ... + b_nx_n)}}\\)\\)    where \\(P(y=1 \\,|\\, X)\\) is the probability of the positive outcome given the input \\(X\\), \\(b_0\\) is the intercept, \\(b_1, b_2, ..., b_n\\) are the coefficients, and \\(x_1, x_2, ..., x_n\\) are the independent variables.</p> </li> </ul>"},{"location":"Machine%20Learning/42_training_models_meaning/#2-fitting-a-logistic-regression-model","title":"2. Fitting a Logistic Regression Model:","text":"<ul> <li>Fitting the logistic regression model involves finding the values of the coefficients \\(b_0, b_1, ..., b_n\\) that maximize the likelihood of observing the given data under the assumed logistic regression model.</li> <li>This is typically done using optimization techniques such as gradient descent or other optimization algorithms.</li> <li>The goal is to find the set of coefficients that best separates the two classes or minimizes the error between the predicted probabilities and the actual binary outcomes in the training data.</li> </ul>"},{"location":"Machine%20Learning/42_training_models_meaning/#3-python-code-example_2","title":"3. Python Code Example:","text":"<ul> <li>Here's how you might fit a logistic regression model using Python with the <code>scikit-learn</code> library:</li> </ul>"},{"location":"Machine%20Learning/42_training_models_meaning/#congratulations","title":"Congratulations!","text":"<p>If you reached here, you've explored the meaning of training a machine learning model of one and multiple independent variable models for Linear and Logistic Regression</p>"},{"location":"Machine%20Learning/42_training_models_meaning/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning/","title":"Introduction","text":"<p>title: Unsupervised Learning author: Juma Shafara date: \"2024-02\" date-modified: \"2024-07-25\" description: Unsupervised machine learning refers to the category of machine learning techniques where models are trained on a dataset without labels.  keywords: [Introduction to Unsupervised Learning, Clustering, K Means Clustering, what number of clusters is good enough]</p> <p></p> <p>The following topics are covered in this tutorial:</p> <ul> <li> Overview of unsupervised learning algorithms in Scikit-learn.</li> <li> K Means clustering</li> </ul> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>Let's install the required libraries.</p> <pre><code># # uncomment and run this cell to install the packages and libraries\n# !pip install dataidea\n</code></pre> <p>Here are the topics in machine learning that we're studying in this course (source): </p> <p></p> <p>Scikit-learn offers the following cheatsheet to decide which model to pick for a given problem. Can you identify the unsupervised learning algorithms?</p> <p></p> <p>Here is a full list of unsupervised learning algorithms available in Scikit-learn: https://scikit-learn.org/stable/unsupervised_learning.html</p> <p></p> <p></p> <p>Here is a visual representation of clustering:</p> <p></p> <p>Here are some real-world applications of clustering:</p> <ul> <li> Customer segmentation</li> <li> Product recommendation</li> <li> Feature engineering</li> <li> Anomaly/fraud detection</li> <li> Taxonomy creation</li> </ul> <p>We'll use the Iris flower dataset to study some of the clustering algorithms available in <code>scikit-learn</code>. It contains various measurements for 150 flowers belonging to 3 different species.</p> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nsns.set_style('darkgrid')\n%matplotlib inline\n</code></pre> <p>Let's load the popular iris and penguin datasets. These datasets are already built in seaborn</p> <pre><code># load the iris dataset\niris_df = sns.load_dataset('iris')\niris_df.head()\n</code></pre> sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa <pre><code>sns.scatterplot(data=iris_df, x='sepal_length', y='petal_length', hue='species')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Length')\nplt.xlabel('Sepal Length')\nplt.show()\n</code></pre> <p>We'll attempt to cluster observations using numeric columns in the data. </p> <pre><code>numeric_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nX = iris_df[numeric_cols]\nX.head()\n</code></pre> sepal_length sepal_width petal_length petal_width 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 <p>Here's how the K-means algorithm works:</p> <ol> <li>Pick K random objects as the initial cluster centers.</li> <li>Classify each object into the cluster whose center is closest to the point.</li> <li>For each cluster of classified objects, compute the centroid (mean).</li> <li>Now reclassify each object using the centroids as cluster centers.</li> <li>Calculate the total variance of the clusters (this is the measure of goodness).</li> <li>Repeat steps 1 to 6 a few more times and pick the cluster centers with the lowest total variance.</li> </ol> <p>Here's a video showing the above steps:</p> <p>Let's apply K-means clustering to the Iris dataset.</p> <pre><code>from sklearn.cluster import KMeans\n</code></pre> <pre><code>model = KMeans(n_clusters=3, random_state=42)\n# training the model\nmodel.fit(X)\n</code></pre> <pre>KMeans(n_clusters=3, random_state=42)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. <p>\u00a0\u00a0KMeans?Documentation for KMeansiFitted<pre>KMeans(n_clusters=3, random_state=42)</pre> <p> </p> <p>We can check the cluster centers for each cluster.</p> <pre><code>model.cluster_centers_\n</code></pre> <pre>\n<code>array([[6.85384615, 3.07692308, 5.71538462, 2.05384615],\n       [5.006     , 3.428     , 1.462     , 0.246     ],\n       [5.88360656, 2.74098361, 4.38852459, 1.43442623]])</code>\n</pre> <p>We can now classify points using the model.</p> <pre><code># making predictions on X (clustering)\npreds = model.predict(X)\n</code></pre> <pre><code># assign each row to their cluster\nX['clusters'] = preds\n# looking at some samples\nX.tail(n=5)\n</code></pre> sepal_length sepal_width petal_length petal_width clusters 145 6.7 3.0 5.2 2.3 0 146 6.3 2.5 5.0 1.9 2 147 6.5 3.0 5.2 2.0 0 148 6.2 3.4 5.4 2.3 0 149 5.9 3.0 5.1 1.8 2 <p>Let's use seaborn and pyplot to visualize the clusters</p> <pre><code>sns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=preds)\ncenters_x, centers_y = model.cluster_centers_[:,0], model.cluster_centers_[:,2]\nplt.plot(centers_x, centers_y, 'xb')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Length')\nplt.xlabel('Sepal Length')\nplt.show()\n</code></pre> <p>As you can see, K-means algorithm was able to classify (for the most part) different specifies of flowers into separate clusters. Note that we did not provide the \"species\" column as an input to <code>KMeans</code>.</p> <p>We can check the \"goodness\" of the fit by looking at <code>model.inertia_</code>, which contains the sum of squared distances of samples to their closest cluster center. Lower the inertia, better the fit.</p> <pre><code>model.inertia_\n</code></pre> <pre>\n<code>78.8556658259773</code>\n</pre> <p>Let's try creating 6 clusters.</p> <pre><code>model = KMeans(n_clusters=6, random_state=42)\n# fitting the model\nmodel.fit(X)\n# making predictions on X (clustering)\npreds = model.predict(X)\n# assign each row to their cluster\nX['clusters'] = preds\n# looking at some samples\nX.sample(n=5)\n</code></pre> sepal_length sepal_width petal_length petal_width clusters 37 4.9 3.6 1.4 0.1 5 54 6.5 2.8 4.6 1.5 0 2 4.7 3.2 1.3 0.2 1 59 5.2 2.7 3.9 1.4 2 56 6.3 3.3 4.7 1.6 0 <p>Let's visualize the clusters</p> <pre><code>sns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=preds)\ncenters_x, centers_y = model.cluster_centers_[:,0], model.cluster_centers_[:,2]\nplt.plot(centers_x, centers_y, 'xb')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n</code></pre> <pre><code># Let's calculate the new model inertia\nmodel.inertia_\n</code></pre> <pre>\n<code>50.560990643274856</code>\n</pre> <pre><code>options = range(2, 11)\ninertias = []\n\nfor n_clusters in options:\n    model = KMeans(n_clusters, random_state=42).fit(X)\n    inertias.append(model.inertia_)\n\nplt.plot(options, inertias, linestyle='-', marker='o')\nplt.title(\"No. of clusters vs. Inertia\")\nplt.xlabel('No. of clusters (K)')\nplt.ylabel('Inertia')\n</code></pre> <pre>\n<code>Text(0, 0.5, 'Inertia')</code>\n</pre> <p>The chart is creates an \"elbow\" plot, and you can pick the number of clusters beyond which the reduction in inertia decreases sharply.</p> <p>Mini Batch K Means: The K-means algorithm can be quite slow for really large dataset. Mini-batch K-means is an iterative alternative to K-means that works well for large datasets. Learn more about it here: https://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans</p> <p>&gt; EXERCISE: Perform clustering on the Mall customers dataset on Kaggle. Study the segments carefully and report your observations.</p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning/#introduction-to-unsupervised-learning","title":"Introduction to Unsupervised Learning","text":"<p>Unsupervised machine learning refers to the category of machine learning techniques where models are trained on a dataset without labels. Unsupervised learning is generally use to discover patterns in data and reduce high-dimensional data to fewer dimensions. Here's how unsupervised learning fits into the landscape of machine learning algorithms(source):</p> <p></p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning/#clustering","title":"Clustering","text":"<p>Clustering is the process of grouping objects from a dataset such that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (Wikipedia). Scikit-learn offers several clustering algorithms. You can learn more about them here: https://scikit-learn.org/stable/modules/clustering.html</p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning/#k-means-clustering","title":"K Means Clustering","text":"<p>The K-means algorithm attempts to classify objects into a pre-determined number of clusters by finding optimal central points (called centroids) for each cluster. Each object is classifed as belonging the cluster represented by the closest centroid.</p> <p></p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning/#so-what-number-of-clusters-is-good-enough","title":"So, what number of clusters is good enough?","text":"<p>In most real-world scenarios, there's no predetermined number of clusters. In such a case, you can create a plot of \"No. of clusters\" vs \"Inertia\" to pick the right number of clusters.</p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning/#summary-and-references","title":"Summary and References","text":"<p>Congratulations on finishing this lesson. The following topics were covered in this tutorial:</p> <ul> <li> Overview of unsupervised learning algorithms in Scikit-learn</li> <li> K Means Clustering</li> </ul> <p>Check out these resources to learn more:</p> <ul> <li>https://blog.dataidea.org/posts/cost-function-in-machine-learning</li> <li>https://www.coursera.org/learn/machine-learning</li> <li>https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/</li> <li>https://scikit-learn.org/stable/unsupervised_learning.html</li> <li>https://scikit-learn.org/stable/modules/clustering.html</li> </ul>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/","title":"Advanced","text":"<p>title: Unsupervised Learning Part 2 author: Juma Shafara date: \"2024-02\" date-modified: \"2024-07-25\" description: Unsupervised machine learning refers to the category of machine learning techniques where models are trained on a dataset without labels.  keywords: [Introduction to Unsupervised Learning, Clustering, DBSCAN]</p> <p></p> <p>In the previous section, we looked clustering and KMeans as model for cluster analysis. In this section, we will look at more clustering algorithms and Dimensionality reduction.</p> <ul> <li>Clustering algorithms: Hierarchical clustering etc.</li> <li>Dimensionality reduction (PCA) and manifold learning (t-SNE)</li> </ul> <p>Let's install the required libraries.</p> <pre><code># # uncomment and run this cell to install the packages and libraries\n# !pip install dataidea\n</code></pre> <p>Here is a visual representation of clustering:</p> <p></p> <p>Here are some real-world applications of clustering:</p> <ul> <li> Customer segmentation</li> <li> Product recommendation</li> <li> Feature engineering</li> <li> Anomaly/fraud detection</li> <li> Taxonomy creation</li> </ul> <p>We'll use the Iris flower dataset to study some of the clustering algorithms available in <code>scikit-learn</code>. It contains various measurements for 150 flowers belonging to 3 different species.</p> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nsns.set_style('darkgrid')\n%matplotlib inline\n</code></pre> <p>Let's load the popular iris and penguin datasets. These datasets are already built in seaborn</p> <pre><code># load the iris dataset\niris_df = sns.load_dataset('iris')\niris_df.head()\n</code></pre> sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa <pre><code># load the penguin dataset\nsns.get_dataset_names()\nping_df = sns.load_dataset('penguins')\nping_df.head()\n</code></pre> species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex 0 Adelie Torgersen 39.1 18.7 181.0 3750.0 Male 1 Adelie Torgersen 39.5 17.4 186.0 3800.0 Female 2 Adelie Torgersen 40.3 18.0 195.0 3250.0 Female 3 Adelie Torgersen NaN NaN NaN NaN NaN 4 Adelie Torgersen 36.7 19.3 193.0 3450.0 Female <pre><code>sns.scatterplot(data=ping_df, x='bill_length_mm', y='bill_depth_mm', hue='species')\nplt.title('Penguin Bill Depth against Bill Length per Species')\nplt.ylabel('Bill Depth')\nplt.xlabel('Bill Length')\nplt.show()\n</code></pre> <pre><code>sns.scatterplot(data=iris_df, x='sepal_length', y='petal_length', hue='species')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n</code></pre> <p>We'll attempt to cluster observations using numeric columns in the data. </p> <pre><code>numeric_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nX = iris_df[numeric_cols]\nX.head()\n</code></pre> sepal_length sepal_width petal_length petal_width 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 <pre><code>from sklearn.cluster import DBSCAN\n</code></pre> <pre><code>model = DBSCAN(eps=1.1, min_samples=4)\nmodel.fit(X)\n</code></pre> <pre>DBSCAN(eps=1.1, min_samples=4)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. <p>\u00a0\u00a0DBSCAN?Documentation for DBSCANiFitted<pre>DBSCAN(eps=1.1, min_samples=4)</pre> <p> </p> <p>In DBSCAN, there's no prediction step. It directly assigns labels to all the inputs.</p> <pre><code>model.labels_\n</code></pre> <pre>\n<code>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1,\n       1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1,\n       1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2])</code>\n</pre> <pre><code>sns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=model.labels_)\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n</code></pre>  EXERCISE<p>Try changing the values of `eps` and `min_samples` and observe how the number of clusters the classification changes..</p> <p>Here's how the results of DBSCAN and K Means differ:</p> <p></p>  EXERCISE<p>Implement hierarchical clustering for the Iris dataset using `scikit-learn`</p> <p>There are several other clustering algorithms in Scikit-learn. You can learn more about them and when to use them here: https://scikit-learn.org/stable/modules/clustering.html</p> <p>Let's save our work before continuing.</p> <pre><code>iris_df = sns.load_dataset('iris')\niris_df.sample(n=5)\n</code></pre> sepal_length sepal_width petal_length petal_width species 86 6.7 3.1 4.7 1.5 versicolor 27 5.2 3.5 1.5 0.2 setosa 146 6.3 2.5 5.0 1.9 virginica 85 6.0 3.4 4.5 1.6 versicolor 30 4.8 3.1 1.6 0.2 setosa <pre><code>numeric_cols\n</code></pre> <pre>\n<code>['sepal_length', 'sepal_width', 'petal_length', 'petal_width']</code>\n</pre> <pre><code>from sklearn.decomposition import PCA\n</code></pre> <pre><code>pca = PCA(n_components=2)\npca.fit(iris_df[numeric_cols])\n</code></pre> <pre>PCA(n_components=2)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. <p>\u00a0\u00a0PCA?Documentation for PCAiFitted<pre>PCA(n_components=2)</pre> <p> </p> <pre><code>transformed = pca.transform(iris_df[numeric_cols])\n</code></pre> <pre><code>sns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=iris_df['species'])\n</code></pre> <p>As you can see, the PCA algorithm has done a very good job of separating different species of flowers using just 2 measures.</p> <p>&gt; EXERCISE: Apply Principal Component Analysis to a large high-dimensional dataset and train a machine learning model using the low-dimensional results. Observe the changes in the loss and training time for different numbers of target dimensions.</p> <p>Learn more about Principal Component Analysis here: https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html</p> <pre><code>from sklearn.manifold import TSNE\n</code></pre> <pre><code>tsne = TSNE(n_components=2)\ntransformed = tsne.fit_transform(iris_df[numeric_cols])\n</code></pre> <pre><code>sns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=iris_df['species']);\n</code></pre> <p>As you can see, the flowers from the same species are clustered very closely together. The relative distance between the species is also conveyed by the gaps between the clusters.</p> <p>&gt; EXERCISE: Use t-SNE to visualize the MNIST handwritten digits dataset.</p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/#introduction-to-unsupervised-learning","title":"Introduction to Unsupervised Learning","text":"<p>Unsupervised machine learning refers to the category of machine learning techniques where models are trained on a dataset without labels. Unsupervised learning is generally use to discover patterns in data and reduce high-dimensional data to fewer dimensions. Here's how unsupervised learning fits into the landscape of machine learning algorithms(source):</p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/#clustering","title":"Clustering","text":"<p>Clustering is the process of grouping objects from a dataset such that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (Wikipedia). Scikit-learn offers several clustering algorithms. You can learn more about them here: https://scikit-learn.org/stable/modules/clustering.html</p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/#dbscan","title":"DBSCAN","text":"<p>Density-based spatial clustering of applications with noise (DBSCAN) uses the density of points in a region to form clusters. It has two main parameters: \"epsilon\" and \"min samples\" using which it classifies each point as a core point, reachable point or noise point (outlier).</p> <p></p> <p>Here's a video explaining how the DBSCAN algorithm works: https://www.youtube.com/watch?v=C3r7tGRe2eI</p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/#hierarchical-clustering","title":"Hierarchical Clustering","text":"<p>Hierarchical clustering, as the name suggests, creates a hierarchy or a tree of clusters.</p> <p></p> <p>While there are several approaches to hierarchical clustering, the most common approach works as follows:</p> <ol> <li>Mark each point in the dataset as a cluster.</li> <li>Pick the two closest cluster centers without a parent and combine them into a new cluster. </li> <li>The new cluster is the parent cluster of the two clusters, and its center is the mean of all the points in the cluster.</li> <li>Repeat steps 2 and 3 till there's just one cluster left.</li> </ol> <p>Watch this video for a visual explanation of hierarchical clustering: https://www.youtube.com/watch?v=7xHsRkOdVwo</p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/#dimensionality-reduction-and-manifold-learning","title":"Dimensionality Reduction and Manifold Learning","text":"<p>In machine learning problems, we often encounter datasets with a very large number of dimensions (features or columns). Dimensionality reduction techniques are used to reduce the number of dimensions or features within the data to a manageable or convenient number. </p> <p>Applications of dimensionality reduction:</p> <ul> <li>Reducing size of data without loss of information</li> <li>Training machine learning models efficiently</li> <li>Visualizing high-dimensional data in 2/3 dimensions</li> </ul>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<p>Principal component is a dimensionality reduction technique that uses linear projections of data to reduce their dimensions, while attempting to maximize the variance of data in the projection. Watch this video to learn how PCA works:</p> <p>Here's an example of PCA to reduce 2D data to 1D:</p> <p>Let's apply Principal Component Analysis to the Iris dataset.</p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/#t-distributed-stochastic-neighbor-embedding-t-sne","title":"t-Distributed Stochastic Neighbor Embedding (t-SNE)","text":"<p>Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high. Scikit-learn provides many algorithms for manifold learning: https://scikit-learn.org/stable/modules/manifold.html . A commonly-used manifold learning technique is t-Distributed Stochastic Neighbor Embedding or t-SNE, used to visualize high dimensional data in one, two or three dimensions. </p> <p>Here's a visual representation of t-SNE applied to visualize 2 dimensional data in 1 dimension:</p> <p></p> <p>Here's a video explaning how t-SNE works: https://www.youtube.com/watch?v=NEaUSP4YerM</p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/#summary-and-references","title":"Summary and References","text":"<p>The following topics were covered in this tutorial:</p> <ul> <li> Overview of unsupervised learning algorithms in Scikit-learn</li> <li> Clustering algorithms: DBScan, Hierarchical clustering etc.</li> <li> Dimensionality reduction (PCA) and manifold learning (t-SNE)</li> </ul> <p>Check out these resources to learn more:</p> <ul> <li>https://www.coursera.org/learn/machine-learning</li> <li>https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/</li> <li>https://scikit-learn.org/stable/unsupervised_learning.html</li> <li>https://scikit-learn.org/stable/modules/clustering.html</li> </ul>"},{"location":"Machine%20Learning/71_feature_selection/","title":"Feature Selection","text":"<p>title: Feature Selection description: Feature selection is a process where you automatically select those features in your data that contribute most to the prediction variable keywords: [feature selection] author: Juma Shafara date: \"2024-03\" date-modified: \"2024-07-25\"</p> <p></p> <p>Feature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.</p> <p>Having irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression.</p> <p>Three benefits of performing feature selection before modeling your data are:</p> <ul> <li>Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.</li> <li>Improves Accuracy: Less misleading data means modeling accuracy improves.</li> <li>Reduces Training Time: Less data means that algorithms train faster.</li> </ul> <p>You can learn more about feature selection with scikit-learn in the article Feature selection.</p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom dataidea.datasets import loadDataset\n</code></pre> <pre><code>data = loadDataset('../assets/demo_cleaned.csv', \n                    inbuilt=False, file_type='csv')\ndata.head()\n</code></pre> age gender marital_status address income income_category job_category 0 55 f 1 12 72.0 3.0 3 1 56 m 0 29 153.0 4.0 3 2 24 m 1 4 26.0 2.0 1 3 45 m 0 9 76.0 4.0 2 4 44 m 1 17 144.0 4.0 3 <pre><code>data = pd.get_dummies(data, columns=['gender'], \n                      dtype='int', drop_first=True)\ndata.head(n=5)\n</code></pre> age marital_status address income income_category job_category gender_m 0 55 1 12 72.0 3.0 3 0 1 56 0 29 153.0 4.0 3 1 2 24 1 4 26.0 2.0 1 1 3 45 0 9 76.0 4.0 2 1 4 44 1 17 144.0 4.0 3 1 <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code>from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import f_regression\n</code></pre> <p>Let's first separate our data into features ie <code>X</code> and outcome ie <code>y</code> as below.</p> <pre><code>X = data.drop('marital_status', axis=1)\ny = data.marital_status\n</code></pre> <pre><code>X_numeric = X[['age', 'income', 'address']].copy()\n</code></pre> <pre><code># create a test object from SelectKBest\ntest = SelectKBest(score_func=f_classif, k=2)\n\n# fit the test object to the data\nfit = test.fit(X_numeric, y)\n\n# get the scores and features\nscores = fit.scores_\n\n# get the selected indices\nfeatures = fit.transform(X_numeric)\nselected_indices = test.get_support(indices=True)\n\n# print the scores and features\nprint('Feature Scores: ', scores)\nprint('Selected Features Indices: ', selected_indices)\n</code></pre> <pre>\n<code>Feature Scores:  [1.34973748 1.73808724 0.02878244]\nSelected Features Indices:  [0 1]\n</code>\n</pre> <p>This shows us that the best 2 features to use to differentiate between the groups in our outcome are <code>[0, 1]</code> ie <code>age</code> and <code>income</code></p> <pre><code># pick numeric input and output\nX = data[['age', 'address']].copy()\ny = data.income\n</code></pre> <p>We will still use the <code>SelectKBest</code> class but with our <code>score_func</code> as <code>f_regression</code> instead. </p> <pre><code>test = SelectKBest(score_func=f_regression, k=1)\n\n# Fit the test to the data\nfit = test.fit(X, y)\n\n# get scores\ntest_scores = fit.scores_\n\n# summarize selected features\nfeatures = fit.transform(X)\n\n# Get the selected feature indices\nselected_indices = fit.get_support(indices=True)\n\nprint('Feature Scores: ', test_scores)\nprint('Selected Features Indices: ', selected_indices)\n</code></pre> <pre>\n<code>Feature Scores:  [25.18294605 23.43115992]\nSelected Features Indices:  [0]\n</code>\n</pre> <p>Here, we can see that <code>age</code> is selected because it returns the higher f_statistic between the two features</p> <pre><code># selecting categorical features\nX = data[['gender_m', 'income_category', 'job_category']].copy()\n\n# selecting categorical outcome\ny = data.marital_status\n</code></pre> <p>Now we shall again use <code>SelectKBest</code> but with the <code>score_func</code> as <code>chi2</code>.</p> <pre><code>from sklearn.feature_selection import chi2\n</code></pre> <pre><code>test = SelectKBest(score_func=chi2, k=2)\nfit = test.fit(X, y)\nscores = fit.scores_\nfeatures = fit.transform(X)\nselected_indices = fit.get_support(indices=True)\n\nprint('Feature Scores: ', scores)\nprint('Selected Features Indices: ', selected_indices)\n</code></pre> <pre>\n<code>Feature Scores:  [0.20921223 0.61979264 0.00555967]\nSelected Features Indices:  [0 1]\n</code>\n</pre> <p>Note: When using the Chi-Square (chi2) as the the score function for feature selection, you use the Chi-Square statistic.</p> <p>Again, we can see that the features with higher f_statistic scores have been selected</p> <ul> <li><code>f_classif</code> is most applicable where the input features are continuous and the outcome is categorical.</li> <li><code>f_regression</code> is most applicable where the input features are continuous and the outcome is continuous.</li> <li><code>chi2</code> is best for when the both the input and outcome are categorical.</li> </ul> <pre><code>from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n</code></pre> <pre><code>X = data.drop('marital_status', axis=1)\ny = data.marital_status\n</code></pre> <pre><code># feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model)\nfit = rfe.fit(X, y)\n\nprint(\"Num Features: %d\" % fit.n_features_)\nprint(\"Selected Features: %s\" % fit.support_)\nprint(\"Feature Ranking: %s\" % fit.ranking_)\n</code></pre> <pre>\n<code>Num Features: 3\nSelected Features: [False False False  True  True  True]\nFeature Ranking: [2 3 4 1 1 1]\n</code>\n</pre> <pre><code>X.head()\n</code></pre> age address income income_category job_category gender_m 0 55 12 72.0 3.0 3 0 1 56 29 153.0 4.0 3 1 2 24 4 26.0 2.0 1 1 3 45 9 76.0 4.0 2 1 4 44 17 144.0 4.0 3 1 <p>From the operation above, we can observe features that bring out the best from the <code>LogisticRegression</code> model ranked from <code>1</code> as most best and bigger numbers as less.</p> <pre><code>from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n</code></pre> <pre><code># feature extraction\nmodel = ExtraTreesClassifier()\nmodel.fit(X, y)\n\n# see the best features\nprint(model.feature_importances_)\n</code></pre> <pre>\n<code>[0.29058207 0.24978811 0.26342117 0.06763375 0.08501043 0.04356447]\n</code>\n</pre> <pre><code># feature extraction\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\n\n# see the best features\nprint(model.feature_importances_)\n</code></pre> <pre>\n<code>[0.28927782 0.2515934  0.28839236 0.06166801 0.06610313 0.04296528]\n</code>\n</pre> <p>more about random forest here</p>"},{"location":"Machine%20Learning/71_feature_selection/#what-is-feature-selection","title":"What is Feature Selection","text":""},{"location":"Machine%20Learning/71_feature_selection/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Machine%20Learning/71_feature_selection/#univariate-feature-selection-techniques","title":"Univariate Feature Selection Techniques","text":"<p>Statistical tests can be used to select those features that have the strongest relationship with the output variable.</p> <p>The scikit-learn library provides the <code>SelectKBest</code> class that can be used with a suite of different statistical tests to select a specific number of features.</p> <p>Many different statistical tests can be used with this selection method. For example the ANOVA F-value method is appropriate for numerical inputs and categorical data. This can be used via the f_classif() function. We will select the 4 best features using this method in the example below.</p>"},{"location":"Machine%20Learning/71_feature_selection/#numeric-or-continuous-features-with-categorical-outcome","title":"Numeric or Continuous Features with Categorical Outcome","text":"<p>Beginning with the numeric columns, let's find which of them best contributes to the outcome variable</p>"},{"location":"Machine%20Learning/71_feature_selection/#numeric-features-with-numeric-outcome","title":"Numeric Features with Numeric Outcome","text":"<p>Let's selecting the input features <code>X</code>, and the output (outcome), <code>y</code></p>"},{"location":"Machine%20Learning/71_feature_selection/#both-input-and-outcome-categorical","title":"Both input and outcome Categorical","text":"<p>Let's begin by selecting out only the categorical features to make our <code>X</code> set and set <code>y</code> as categorical</p>"},{"location":"Machine%20Learning/71_feature_selection/#recursive-feature-elimination","title":"Recursive Feature Elimination","text":"<p>The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.</p> <p>It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.</p> <p>You can learn more about the RFE class in the scikit-learn documentation.</p>"},{"location":"Machine%20Learning/71_feature_selection/#logistic-regression","title":"Logistic Regression","text":""},{"location":"Machine%20Learning/71_feature_selection/#feature-importance","title":"Feature Importance","text":"<p>Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features.</p> <p>In the example below we construct a ExtraTreesClassifier classifier for the Pima Indians onset of diabetes dataset. You can learn more about the ExtraTreesClassifier class in the scikit-learn API.</p>"},{"location":"Machine%20Learning/71_feature_selection/#extra-trees-classifier","title":"Extra Trees Classifier","text":""},{"location":"Machine%20Learning/71_feature_selection/#random-forest-classifier","title":"Random Forest Classifier","text":""},{"location":"Machine%20Learning/71_feature_selection/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Machine%20Learning/72_why_scaling/","title":"Why Scaling","text":"<p>title: Why re-scaling (Iris)? author: Juma Shafara description: We\u2019ll demonstrate the effect of not scaling the features on K-means clustering date: \"2024-04\" keywords: [clustering, rescaling, machine learning]</p> <p></p> <p>Let's use the Iris dataset, a popular dataset in machine learning. The Iris dataset consists of 150 samples of iris flowers, with each sample containing four features: sepal length, sepal width, petal length, and petal width. We'll demonstrate the effect of not scaling the features on K-means clustering.</p> <p>First, let's import the necessary libraries and load the Iris dataset:</p> <pre><code>import matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n</code></pre> <pre><code># Load the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n</code></pre> <pre><code># picking the first row\nX[0]\n</code></pre> <pre>\n<code>array([5.1, 3.5, 1.4, 0.2])</code>\n</pre> <p>Next, let's perform K-means clustering on the original dataset without scaling the features:</p> <pre><code># Apply K-means clustering without scaling\nkmeans_unscaled = KMeans(n_clusters=2, random_state=42)\nkmeans_unscaled.fit(X)\n\n# Get the cluster centers and labels\ncentroids_unscaled = kmeans_unscaled.cluster_centers_\nlabels_unscaled = kmeans_unscaled.labels_\n</code></pre> <pre><code>from sklearn.metrics import silhouette_score\n</code></pre> <pre><code>silhouette_avg = silhouette_score(X, kmeans_unscaled.labels_)\nprint('Silhouette Average (Unscaled): ', silhouette_avg)\n</code></pre> <pre>\n<code>Silhouette Average (Unscaled):  0.6810461692117462\n</code>\n</pre> <p>The interpretation of the silhouette score is relatively straightforward:</p> <ul> <li> <p>Close to +1: A silhouette score near +1 indicates that the sample is far away from the neighboring clusters. This is a good indication of a well-clustered data point.</p> </li> <li> <p>Close to 0: A silhouette score near 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters. It could imply that the sample could belong to either one of the clusters.</p> </li> <li> <p>Close to -1: A silhouette score near -1 indicates that the samples might have been assigned to the wrong clusters. This could happen if the clusters overlap significantly or if the wrong number of clusters was chosen.</p> </li> </ul> <p>Read more about the Silhouette Score from here(sklearn)</p> <p>Now, let's visualize the clusters without scaling:</p> <pre><code># Visualize clusters without scaling\nplt.figure(figsize=(10, 6))\n\nplt.scatter(X[:, 0], X[:, 1], c=labels_unscaled, cmap='viridis', s=50)\nplt.scatter(centroids_unscaled[:, 0], centroids_unscaled[:, 1], marker='x', s=200, c='black')\n\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Sepal Width (cm)')\nplt.title('K-means Clustering Without Scaling')\n\nplt.show()\n</code></pre> <p>You'll notice that the clusters may not seem well-separated or meaningful. This is because the features of the Iris dataset have different scales, with sepal length ranging from approximately 4 to 8 cm, while sepal width ranges from approximately 2 to 4.5 cm.</p> <p>Now, let's repeat the process after scaling the features using StandardScaler:</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\n# Scale the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply K-means clustering on scaled features\nkmeans_scaled = KMeans(n_clusters=2, random_state=42)\nkmeans_scaled.fit(X_scaled)\n\n# Get the cluster centers and labels\ncentroids_scaled = kmeans_scaled.cluster_centers_\nlabels_scaled = kmeans_scaled.labels_\n</code></pre> <pre><code>silhouette_avg = silhouette_score(X, kmeans_scaled.labels_)\nprint('Silhouette Average (Scaled): ', silhouette_avg)\n</code></pre> <pre>\n<code>Silhouette Average (Scaled):  0.6867350732769777\n</code>\n</pre> <p>Visualize the clusters after scaling:</p> <pre><code># Visualize clusters with scaling\nplt.figure(figsize=(10, 6))\n\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_scaled, cmap='viridis', s=50)\nplt.scatter(centroids_scaled[:, 0], centroids_scaled[:, 1], marker='x', s=200, c='black')\n\nplt.xlabel('Sepal Length (scaled)')\nplt.ylabel('Sepal Width (scaled)')\nplt.title('K-means Clustering With Scaling')\n\nplt.show()\n</code></pre> <p>You should see clearer and more meaningful clusters after scaling the features, demonstrating the importance of feature scaling for K-means clustering, especially when dealing with datasets with features of different scales.</p>"},{"location":"Machine%20Learning/72_why_scaling/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Machine%20Learning/72_why_scaling_fpl/","title":"Feature Scaling (FPL)","text":"<p>title: Why re-scale data (fpl)?  author: Juma Shafara keywords: [kmeans clustering, re-scaling data, standard scaler] description: In this notebook, we\u2019ll use Kmeans clustering to demonstrate the importance of scaling data date: \"2024-04\"</p> <p></p> <p>In this notebook, we'll use Kmeans clustering to demonstrate the importance of scaling data</p> <p>K-means clustering is an unsupervised machine learning algorithm used for partitioning a dataset into K distinct, non-overlapping clusters. The goal of K-means is to minimize the sum of squared distances between data points and their respective cluster centroids.</p> <p>Here's how the K-means algorithm works:</p> <ol> <li>Pick K random objects as the initial cluster centers.</li> <li>Classify each object into the cluster whose center is closest to the point.</li> <li>For each cluster of classified objects, compute the centroid (mean).</li> <li>Now reclassify each object using the centroids as cluster centers.</li> <li>Calculate the total variance of the clusters (this is the measure of goodness).</li> <li>Repeat steps 1 to 6 a few more times and pick the cluster centers with the lowest total variance.</li> </ol> <p>Here's a video showing the above steps: https://www.youtube.com/watch?v=4b5d3muPQmA</p> <p>First, let's import the necessary libraries and load the Iris dataset:</p> <pre><code>from dataidea.packages import pd, plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom dataidea.datasets import loadDataset\n</code></pre> <pre><code>fpl_data = loadDataset('fpl')\n</code></pre> <p>We can be able to load it like this because it inbuilt in the dataidea package</p> <p>Now let's pick out a few numeric columns that we might consider moving forward</p> <pre><code>fpl_sample = fpl_data[['Goals_Scored', 'Assists','Total_Points', \n                       'Minutes', 'Saves', 'Goals_Conceded', \n                       'Creativity', 'Influence'\n                      ]]\n</code></pre> <pre><code>fpl_sample.head()\n</code></pre> Goals_Scored Assists Total_Points Minutes Saves Goals_Conceded Creativity Influence 0 18 14 244 3101 0 36 1414.9 1292.6 1 23 14 242 3083 0 39 659.1 1318.2 2 22 6 231 3077 0 41 825.7 1056.0 3 17 11 228 3119 0 36 1049.9 1052.2 4 17 11 194 3052 0 50 371.0 867.2 <p>Next, let's perform K-means clustering on the original dataset without scaling the features:</p> <pre><code># Apply K-means clustering without scaling\nkmeans_unscaled = KMeans(n_clusters=4, random_state=42)\nkmeans_unscaled.fit(fpl_sample)\n\n# Get the cluster centers and labels\ncentroids_unscaled = kmeans_unscaled.cluster_centers_\nlabels_unscaled = kmeans_unscaled.labels_\n</code></pre> <p>Let's see the performance</p> <pre><code># Visualize clusters without scaling\nplt.figure(figsize=(10, 6))\n\nplt.scatter(fpl_sample.Assists, fpl_sample.Goals_Scored, c=labels_unscaled, cmap='viridis',)\n\nplt.show()\n</code></pre> <p>You'll notice that the clusters may not seem well-separated or meaningful. This is because the features of the Iris dataset have different scales, </p> <p>Now, let's repeat the process after scaling the features using StandardScaler:</p> <pre><code># Scale the features\nscaler = StandardScaler()\nfpl_sample_scaled = scaler.fit_transform(fpl_sample)\n\n# Transform scaled features back to DataFrame\nfpl_sample_scaled_dataframe = pd.DataFrame(fpl_sample_scaled, columns=fpl_sample.columns)\n\n# Apply K-means clustering on scaled features\nkmeans_scaled = KMeans(n_clusters=4, random_state=42)\nkmeans_scaled.fit(fpl_sample_scaled)\n\n# Get the cluster centers and labels\ncentroids_scaled = kmeans_scaled.cluster_centers_\nlabels_scaled = kmeans_scaled.labels_\n</code></pre> <pre><code># Visualize clusters without scaling\nplt.figure(figsize=(10, 6))\n\nplt.scatter(fpl_sample_scaled_dataframe.Assists, \n            fpl_sample_scaled_dataframe.Goals_Scored, \n            c=labels_scaled, cmap='viridis')\n\nplt.show()\n</code></pre> <p>You should see clearer and more meaningful clusters after scaling the features, demonstrating the importance of feature scaling for K-means clustering, especially when dealing with datasets with features of different scales.</p> <p>If the data doesn't follow a standard scale, meaning the features have different scales or variances, it can lead to some issues when applying K-means clustering:</p> <ol> <li> <p>Unequal feature influence: Features with larger scales or variances can dominate the clustering process. Since K-means relies on Euclidean distance, features with larger scales will contribute more to the distance calculation, potentially biasing the clustering results towards those features.</p> </li> <li> <p>Incorrect cluster shapes: K-means assumes that clusters are isotropic (spherical) and have similar variances along all dimensions. If the data has features with different scales, clusters may be stretched along certain dimensions, leading to suboptimal cluster assignments.</p> </li> <li> <p>Convergence speed: Features with larger scales can cause centroids to move more quickly towards areas with denser data, potentially affecting the convergence speed of the algorithm.</p> </li> </ol> <p>By scaling the data before clustering, you ensure that each feature contributes equally to the distance calculations, helping to mitigate the issues associated with different feature scales. This can lead to more accurate and reliable clustering results.</p> <p></p> <p> To be among the first to hear about future updates, simply enter your email below, follow us on   (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Machine%20Learning/72_why_scaling_fpl/#clustering-without-scaling","title":"Clustering without Scaling","text":""},{"location":"Machine%20Learning/72_why_scaling_fpl/#clustering-after-scaling","title":"Clustering after Scaling","text":""},{"location":"Machine%20Learning/72_why_scaling_fpl/#take-away","title":"Take away","text":""},{"location":"Machine%20Learning/73_normalization_and_standardization/","title":"Normalization & Standardization","text":"<p>title: Normalization vs Standardization keywords: [normalization, standardization, rescaling] author: Juma Shafara date: \"2024-03\" description: This Jupyter Notebook provides an overview of the importance of data normalization and standardization in preparing data for analysis and modeling</p> <p></p> <pre><code>import numpy as np\nimport dataidea as di\n</code></pre> <ol> <li>Normalization: Normalization typically refers to scaling numerical features to a common scale, often between 0 and 1. This is usually done by subtracting the minimum value and then dividing by the range (maximum - minimum). Normalization is useful when the distribution of the data does not follow a Gaussian distribution (Normal Distribution).</li> </ol> <pre><code># Data Normalization without libraries:\ndef minMaxScaling(data):\n    min_val = min(data)\n    max_val = max(data)\n\n    scaled_data = []\n    for value in data:\n        scaled = (value - min_val) / (max_val - min_val)\n        scaled_data.append(scaled)\n    return scaled_data\n</code></pre> <pre><code># Example data\ndata = np.array([10, 20, 30, 40, 50])\nnormalized_data = minMaxScaling(data)\nprint(\"Normalized data (Min-Max Scaling):\", normalized_data)\n</code></pre> <pre>\n<code>Normalized data (Min-Max Scaling): [0.0, 0.25, 0.5, 0.75, 1.0]\n</code>\n</pre> <pre><code>from sklearn.preprocessing import MinMaxScaler\n\n# Sample data\ndata = np.array([[1, 2], [3, 4], [5, 6]])\n\n# Create the scaler\nscaler = MinMaxScaler()\n\n# Fit the scaler to the data and transform the data\nnormalized_data = scaler.fit_transform(data)\n\nprint(\"Original data:\")\nprint(data)\nprint(\"\\nNormalized data:\")\nprint(normalized_data)\n</code></pre> <pre>\n<code>Original data:\n[[1 2]\n [3 4]\n [5 6]]\n\nNormalized data:\n[[0.  0. ]\n [0.5 0.5]\n [1.  1. ]]\n</code>\n</pre> <p>Let's now try on a real world dataset!</p> <pre><code>boston_data = di.loadDataset('boston')\nboston_data.head()\n</code></pre> CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0 0.469 6.421 78.9 4.9671 2 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0 0.469 7.185 61.1 4.9671 2 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0 0.458 6.998 45.8 6.0622 3 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0 0.458 7.147 54.2 6.0622 3 222.0 18.7 396.90 5.33 36.2 <pre><code>boston_scaler = MinMaxScaler()\nnormalized_data = boston_scaler.fit_transform(boston_data[['CRIM', 'AGE', 'TAX']])\nnp.set_printoptions(suppress=True)\nnormalized_data\n</code></pre> <pre>\n<code>array([[0.        , 0.64160659, 0.20801527],\n       [0.00023592, 0.78269825, 0.10496183],\n       [0.0002357 , 0.59938208, 0.10496183],\n       ...,\n       [0.00061189, 0.90731205, 0.16412214],\n       [0.00116073, 0.88980433, 0.16412214],\n       [0.00046184, 0.80226571, 0.16412214]])</code>\n</pre> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <ol> <li>Standardization: Standardization, often implemented with a method like z-score standardization, transforms the data to have a mean of 0 and a standard deviation of 1. This means that the data will have a Gaussian distribution (if the original data had a Gaussian distribution). </li> </ol> <pre><code>def zScoreNormalization(data):\n    mean = sum(data) / len(data)\n    variance = sum((x - mean) ** 2 for x in data) / len(data)\n    std_dev = variance ** 0.5\n    standardized_data = [(x - mean) / std_dev for x in data]\n    return standardized_data\n</code></pre> <pre><code># Example data\ndata = [10, 20, 30, 40, 50]\nstandardized_data = zScoreNormalization(data)\nprint(\"Standardized data (Z-Score Normalization):\", standardized_data)\n</code></pre> <pre>\n<code>Standardized data (Z-Score Normalization): [-1.414213562373095, -0.7071067811865475, 0.0, 0.7071067811865475, 1.414213562373095]\n</code>\n</pre> <p>In Python, we can also typically use the <code>StandardScaler</code> from the <code>sklearn.preprocessing</code> module to standardize data.</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\n# Sample data\ndata = np.array([[1, 2, 3], [3, 4, 5], [5, 6, 7]])\n\n# Create the scaler\nscaler = StandardScaler()\n\n# Fit the scaler to the data and transform the data\nstandardized_data = scaler.fit_transform(data)\n\nprint(\"Original data:\")\nprint(data)\nprint(\"\\nStandardized data:\")\nprint(standardized_data)\n</code></pre> <pre>\n<code>Original data:\n[[1 2 3]\n [3 4 5]\n [5 6 7]]\n\nStandardized data:\n[[-1.22474487 -1.22474487 -1.22474487]\n [ 0.          0.          0.        ]\n [ 1.22474487  1.22474487  1.22474487]]\n</code>\n</pre> <pre><code>boston_scaler = StandardScaler()\nstandardized_data = boston_scaler.fit_transform(boston_data[['CRIM', 'AGE', 'TAX']])\nnp.set_printoptions(suppress=True)\nstandardized_data\n</code></pre> <pre>\n<code>array([[-0.41978194, -0.12001342, -0.66660821],\n       [-0.41733926,  0.36716642, -0.98732948],\n       [-0.41734159, -0.26581176, -0.98732948],\n       ...,\n       [-0.41344658,  0.79744934, -0.80321172],\n       [-0.40776407,  0.73699637, -0.80321172],\n       [-0.41500016,  0.43473151, -0.80321172]])</code>\n</pre> <ol> <li>Normalization:</li> <li>Use normalization when the scale of features is meaningful and should be preserved.</li> <li>Normalize data when you're working with algorithms that require input features to be on a similar scale, such as algorithms using distance metrics like k-nearest neighbors or clustering algorithms like K-means.</li> <li>If the distribution of your data is not Gaussian and you want to scale the features to a fixed range, normalization might be a better choice.</li> </ol> <ol> <li>Standardization:</li> <li>Use standardization when the distribution of your data is Gaussian or when you're unsure about the distribution.</li> <li>Standardization is less affected by outliers compared to normalization, making it more suitable when your data contains outliers.</li> <li>If you're working with algorithms that assume your data is normally distributed, such as linear regression, logistic regression, standardization is typically preferred.</li> </ol> <p>In some cases, you might experiment with both approaches and see which one yields better results for your specific dataset and analysis. Additionally, it's always a good practice to understand your data and the underlying assumptions of the algorithms you're using to make informed decisions about data preprocessing techniques.</p>"},{"location":"Machine%20Learning/73_normalization_and_standardization/#introduction","title":"Introduction:","text":"<p>In data analysis and machine learning, preprocessing steps such as data normalization and standardization are crucial for improving the performance and interpretability of models.</p> <p>This Jupyter Notebook provides an overview of the importance of data normalization and standardization in preparing data for analysis and modeling.</p>"},{"location":"Machine%20Learning/73_normalization_and_standardization/#normalization","title":"Normalization","text":""},{"location":"Machine%20Learning/73_normalization_and_standardization/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Machine%20Learning/73_normalization_and_standardization/#standardization","title":"Standardization","text":""},{"location":"Machine%20Learning/73_normalization_and_standardization/#importance","title":"Importance:","text":"<ol> <li>Data Normalization:</li> <li>Uniform Scaling: Ensures all features are scaled to a similar range, preventing dominance by features with larger scales.</li> <li>Improved Convergence: Facilitates faster convergence in optimization algorithms by making the loss surface more symmetric.</li> <li> <p>Interpretability: Easier interpretation as values are on a consistent scale, aiding in comparison and understanding of feature importance.</p> </li> <li> <p>Data Standardization:</p> </li> <li>Mean Centering: Transforms data to have a mean of 0 and a standard deviation of 1, simplifying interpretation of coefficients in linear models.</li> <li>Handling Different Scales: Useful when features have different scales or units, making them directly comparable.</li> <li>Reducing Sensitivity to Outliers: Less affected by outliers compared to normalization, leading to more robust models.</li> <li>Maintaining Information: Preserves relative relationships between data points without altering the distribution shape.</li> </ol>"},{"location":"Machine%20Learning/73_normalization_and_standardization/#which-one","title":"Which one?","text":"<p>The choice between normalization and standardization depends on your data and the requirements of your analysis. Here are some guidelines to help you decide:</p>"},{"location":"Machine%20Learning/73_normalization_and_standardization/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Machine%20Learning/75_bonus/","title":"75 bonus","text":"<p>title: ANOVA for Feature Selection keywords: [     \"ANOVA for feature selection\",     \"Feature selection techniques\",     \"Data Science tutorial\",     \"Machine learning feature selection\",     \"ANOVA in data science\",     \"Univariate feature selection\",     \"Fantasy Premier League dataset analysis\",     \"SelectKBest example\",     \"Statistical tests for feature selection\",     \"F-statistic in ANOVA\",     \"Python feature selection\",     \"Scikit-learn SelectKBest\",     \"Analysis of Variance\",     \"Machine learning with ANOVA\",     \"Data science programming\" ] description: In this notebook, we demonstrate how ANOVA (Analysis of Variance) can be used to identify better features for machine learning models author: Juma Shafara date: \"2023-03\"</p> <p></p> <p>In this notebook, we demonstrate how ANOVA (Analysis of Variance) can be used to identify better features for machine learning models. We'll use the Fantasy Premier League (FPL) dataset to show how ANOVA helps in selecting features that best differentiate categories.</p> <pre><code># Uncomment the line below if you need to install the dataidea package\n# !pip install -U dataidea\n</code></pre> <p>First, we'll import the necessary packages: <code>scipy</code> for performing ANOVA, <code>dataidea</code> for loading the FPL dataset, and <code>SelectKBest</code> from <code>scikit-learn</code> for univariate feature selection based on statistical tests.</p> <pre><code>import scipy as sp\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport dataidea as di\n</code></pre> <p>Let's load the FPL dataset and preview the top 5 rows.</p> <pre><code># Load FPL dataset\nfpl = di.loadDataset('fpl') \n\n# Preview the top 5 rows\nfpl.head(n=5)\n</code></pre> First_Name Second_Name Club Goals_Scored Assists Total_Points Minutes Saves Goals_Conceded Creativity Influence Threat Bonus BPS ICT_Index Clean_Sheets Red_Cards Yellow_Cards Position 0 Bruno Fernandes MUN 18 14 244 3101 0 36 1414.9 1292.6 1253 36 870 396.2 13 0 6 MID 1 Harry Kane TOT 23 14 242 3083 0 39 659.1 1318.2 1585 40 880 355.9 12 0 1 FWD 2 Mohamed Salah LIV 22 6 231 3077 0 41 825.7 1056.0 1980 21 657 385.8 11 0 0 MID 3 Heung-Min Son TOT 17 11 228 3119 0 36 1049.9 1052.2 1046 26 777 315.2 13 0 0 MID 4 Patrick Bamford LEE 17 11 194 3052 0 50 371.0 867.2 1512 26 631 274.6 10 0 3 FWD <p>ANOVA helps us determine if there's a significant difference between the means of different groups. We use it to select features that best show the difference between categories. Features with higher F-statistics are preferred.</p> <pre><code># Create groups of goals scored for each player position\nforwards_goals = fpl[fpl.Position == 'FWD']['Goals_Scored']\nmidfielders_goals = fpl[fpl.Position == 'MID']['Goals_Scored']\ndefenders_goals = fpl[fpl.Position == 'DEF']['Goals_Scored']\ngoalkeepers_goals = fpl[fpl.Position == 'GK']['Goals_Scored']\n\n# Perform the ANOVA test for the groups\nf_statistic, p_value = sp.stats.f_oneway(forwards_goals, midfielders_goals, defenders_goals, goalkeepers_goals)\nprint(\"F-statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\n</code></pre> <pre>\n<code>F-statistic: 33.281034594400445\np-value: 3.9257634156019246e-20\n</code>\n</pre> <p>We observe an F-statistic of <code>33.281</code> and a p-value of <code>3.926e-20</code>, indicating a significant difference at multiple confidence levels.</p> <pre><code># Create groups of assists for each player position\nforwards_assists = fpl[fpl.Position == 'FWD']['Assists']\nmidfielders_assists = fpl[fpl.Position == 'MID']['Assists']\ndefenders_assists = fpl[fpl.Position == 'DEF']['Assists']\ngoalkeepers_assists = fpl[fpl.Position == 'GK']['Assists']\n\n# Perform the ANOVA test for the groups\nf_statistic, p_value = sp.stats.f_oneway(forwards_assists, midfielders_assists, defenders_assists, goalkeepers_assists)\nprint(\"F-statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\n</code></pre> <pre>\n<code>F-statistic: 19.263717036430815\np-value: 5.124889288362087e-12\n</code>\n</pre> <p>We observe an F-statistic of <code>19.264</code> and a p-value of <code>5.125e-12</code>, again indicating significance.</p> <pre><code># Use scikit-learn's SelectKBest (with f_classif)\ntest = SelectKBest(score_func=f_classif, k=1)\n\n# Fit the model to the data\nfit = test.fit(fpl[['Goals_Scored', 'Assists']], fpl.Position)\n\n# Get the F-statistics\nscores = fit.scores_\n\n# Select the best feature\nfeatures = fit.transform(fpl[['Goals_Scored', 'Assists']])\n\n# Get the indices of the selected features (optional)\nselected_indices = test.get_support(indices=True)\n\n# Print indices and scores\nprint('Feature Scores: ', scores)\nprint('Selected Features Indices: ', selected_indices)\n</code></pre> <pre>\n<code>Feature Scores:  [33.28103459 19.26371704]\nSelected Features Indices:  [0]\n</code>\n</pre> <p>The <code>0th</code> feature (Goals Scored) is selected as the best feature based on the F-statistics.</p>"},{"location":"Machine%20Learning/75_bonus/#anova-for-goals-scored","title":"ANOVA for Goals Scored","text":"<p>We will create groups of goals scored by each player position (forwards, midfielders, defenders, and goalkeepers) and run an ANOVA test.</p>"},{"location":"Machine%20Learning/75_bonus/#anova-for-assists","title":"ANOVA for Assists","text":"<p>Next, we'll create groups for assists and run an ANOVA test.</p>"},{"location":"Machine%20Learning/75_bonus/#comparing-results","title":"Comparing Results","text":"<p>Both features show significant F-statistics, but goals scored has a higher value, indicating it is a better feature for differentiating player positions.</p>"},{"location":"Machine%20Learning/75_bonus/#using-selectkbest-for-feature-selection","title":"Using SelectKBest for Feature Selection","text":"<p>We can also use <code>SelectKBest</code> from <code>scikit-learn</code> to automate this process.</p>"},{"location":"Machine%20Learning/75_bonus/#summary","title":"Summary","text":"<p>In this notebook, we demonstrated how to use ANOVA for feature selection in the Fantasy Premier League dataset. By comparing the F-statistics of different features, we identified that 'Goals Scored' is a more significant feature than 'Assists' for differentiating player positions. Using <code>SelectKBest</code> from <code>scikit-learn</code>, we confirmed that 'Goals Scored' is the best feature among the two. This method can be applied to other datasets and features to enhance the performance of machine learning models.</p>"},{"location":"Machine%20Learning/75_bonus/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Machine%20Learning/76_handling_missing_data/","title":"Handling Missing Data","text":"<p>title: Handling Missing Data keywords: [handling missing data, handling missing data in python, python data analysis, dataidea, machine learning, data preprocessing, sklearn data preprocessing] description: n this notebook, we look into the top four missing data imputation methods author: Juma Shafara date: \"2024-03\"</p> <p></p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># install the libraries for this demonstration\n# ! pip install -U dataidea\n</code></pre> <pre><code>import pandas as pd\nimport dataidea as di\n</code></pre> <p><code>loadDataset</code> allows us to load datasets inbuilt in the dataidea library</p> <pre><code>weather = di.loadDataset('weather') \nweather\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 NaN 7.0 NaN 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN NaN 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <pre><code>weather.isna().sum()\n</code></pre> <pre>\n<code>day            0\ntemperature    4\nwindspead      4\nevent          2\ndtype: int64</code>\n</pre> <p>Let's demonstrate how to use the top three missing data imputation methods\u2014SimpleImputer, KNNImputer, and IterativeImputer\u2014using the simple weather dataset.</p> <pre><code># select age from the data\ntemp_wind = weather[['temperature', 'windspead']].copy()\n</code></pre> <pre><code>temp_wind_imputed = temp_wind.copy()\n</code></pre> <p></p> <pre><code>from sklearn.impute import SimpleImputer\n\nsimple_imputer = SimpleImputer(strategy='mean')\ntemp_wind_simple_imputed = simple_imputer.fit_transform(temp_wind)\n\ntemp_wind_simple_imputed_df = pd.DataFrame(temp_wind_simple_imputed, columns=temp_wind.columns)\n</code></pre> <p>Let's have a look at the outcome</p> <pre><code>temp_wind_simple_imputed_df\n</code></pre> temperature windspead 0 32.0 6.0 1 33.2 9.0 2 28.0 8.4 3 33.2 7.0 4 32.0 8.4 5 33.2 8.4 6 33.2 8.4 7 34.0 8.0 8 40.0 12.0 <p></p> <pre><code>from sklearn.impute import KNNImputer\n\nknn_imputer = KNNImputer(n_neighbors=2)\ntemp_wind_knn_imputed = knn_imputer.fit_transform(temp_wind)\n\ntemp_wind_knn_imputed_df = pd.DataFrame(temp_wind_knn_imputed, columns=temp_wind.columns)\n</code></pre> <p>If we take a look at the outcome</p> <pre><code>weather\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 NaN 7.0 NaN 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN NaN 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <p></p> <pre><code>from sklearn.impute import KNNImputer\n\nknn_imputer = KNNImputer(n_neighbors=2)\nwindspead_imputed = knn_imputer.fit_transform(weather[['windspead']].reset_index())\n\nwindspead_imputed\n</code></pre> <pre>\n<code>array([[ 0. ,  6. ],\n       [ 1. ,  9. ],\n       [ 2. ,  8. ],\n       [ 3. ,  7. ],\n       [ 4. ,  8. ],\n       [ 5. ,  7.5],\n       [ 6. , 10. ],\n       [ 7. ,  8. ],\n       [ 8. , 12. ]])</code>\n</pre> <pre><code># we can fill it back in the weather data\nweather['windspead'] = windspead_imputed[:, 1]\n\n# now looking at the data\nweather\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 8.0 Snow 3 06/01/2017 NaN 7.0 NaN 4 07/01/2017 32.0 8.0 Rain 5 08/01/2017 NaN 7.5 Sunny 6 09/01/2017 NaN 10.0 NaN 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <p></p> <pre><code>from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\niterative_imputer = IterativeImputer()\ntemp_wind_iterative_imputed = iterative_imputer.fit_transform(temp_wind)\n\ntemp_wind_iterative_imputed_df = pd.DataFrame(temp_wind_iterative_imputed, columns=temp_wind.columns)\n\ntemp_wind_iterative_imputed_df\n</code></pre> temperature windspead 0 32.000000 6.0 1 33.967053 9.0 2 28.000000 8.0 3 31.410210 7.0 4 32.000000 8.0 5 32.049421 7.5 6 35.245474 10.0 7 34.000000 8.0 8 40.000000 12.0 <p>You can also choose an estimator of your choice, let's try a <code>Linear Regression</code> model</p> <pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# set estimator to an instance of a model\niterative_imputer = IterativeImputer(estimator=LinearRegression())\ntemp_wind_iterative_imputed = iterative_imputer.fit_transform(temp_wind)\n\ntemp_wind_iterative_imputed_df = pd.DataFrame(temp_wind_iterative_imputed, columns=temp_wind.columns)\n\ntemp_wind_iterative_imputed_df\n</code></pre> temperature windspead 0 32.000000 6.0 1 34.125000 9.0 2 28.000000 8.0 3 31.041667 7.0 4 32.000000 8.0 5 31.812500 7.5 6 35.666667 10.0 7 34.000000 8.0 8 40.000000 12.0 <p></p> <pre><code># import datawig\n\n# # Impute missing values\n# df_imputed = datawig.SimpleImputer.complete(weather)\n</code></pre> <p>These top imputation methods offer different trade-offs in terms of computational complexity, handling of missing data patterns, and ease of use. The choice between them depends on the specific characteristics of the dataset and the requirements of the analysis.</p> <p> Don't miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it's easy and safe. </p>"},{"location":"Machine%20Learning/76_handling_missing_data/#introduction","title":"Introduction:","text":"<p>Missing data is a common hurdle in data analysis, impacting the reliability of insights drawn from datasets. Python offers a range of solutions to address this issue, some of which we discussed in the earlier weeks. In this notebook, we look into the top four missing data imputation methods:</p> <ul> <li>SimpleImputer</li> <li>KNNImputer</li> <li>IterativeImputer </li> <li>Datawig</li> </ul> <p>We'll explore these essential techniques, using sklearn and the weather dataset.</p>"},{"location":"Machine%20Learning/76_handling_missing_data/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Machine%20Learning/76_handling_missing_data/#simpleimputer-from-scikit-learn","title":"SimpleImputer from scikit-learn:","text":"<ul> <li>Usage: SimpleImputer is a straightforward method for imputing missing values by replacing them with a constant, mean, median, or most frequent value along each column.</li> <li>Pros:<ul> <li>Easy to use and understand.</li> <li>Can handle both numerical and categorical data.</li> <li>Offers flexibility with different imputation strategies.</li> </ul> </li> <li>Cons:<ul> <li>It doesn't consider relationships between features.</li> <li>May not be the best choice for datasets with complex patterns of missingness.</li> </ul> </li> <li>Example:</li> </ul>"},{"location":"Machine%20Learning/76_handling_missing_data/#exercise","title":"Exercise:","text":"<ol> <li>Try out the SimpleImputer with different imputation strategies like mode, constant </li> <li>Choose and try some imputation techniques on categorical data</li> </ol>"},{"location":"Machine%20Learning/76_handling_missing_data/#knnimputer-from-scikit-learn","title":"KNNImputer from scikit-learn:","text":"<ul> <li>Usage: <ul> <li>KNNImputer imputes missing values using k-nearest neighbors, replacing them with the mean value of the nearest neighbors.</li> <li>You can read more about the KNNImputer from the sklearn official docs site</li> </ul> </li> <li>Pros:<ul> <li>Considers relationships between features, making it suitable for datasets with complex patterns of missingness.</li> <li>Can handle both numerical and categorical data.</li> </ul> </li> <li>Cons:<ul> <li>Computationally expensive for large datasets.</li> <li>Requires careful selection of the number of neighbors (k).</li> </ul> </li> </ul> Note!<p>By default, the KNNImputer uses 'nan' values as missing data and the 'nan_euclidean' metric to calculate the distances between values.</p> <ul> <li>Example:</li> </ul>"},{"location":"Machine%20Learning/76_handling_missing_data/#filling-a-single-column-independently-using-the-knnimputer","title":"Filling a single column independently using the <code>KNNImputer</code>","text":"<p>To use the KNNImputer for a single independ column, you can use the index as the other column instead, this will result into equal euclidean distances resulting into the use of the physical neighbors in the data table.</p>"},{"location":"Machine%20Learning/76_handling_missing_data/#exercise_1","title":"Exercise","text":"<ul> <li>Try out the KNNImputer with different numbers of neighbors and compare the results</li> <li>Findo out how to use KNNImputer to fill categorical data</li> </ul>"},{"location":"Machine%20Learning/76_handling_missing_data/#iterativeimputer-from-scikit-learn","title":"IterativeImputer from scikit-learn:","text":"<ul> <li>Usage: IterativeImputer models each feature with missing values as a function of other features and uses that estimate for imputation. It iteratively estimates the missing values.</li> <li>Pros:<ul> <li>Takes into account relationships between features, making it suitable for datasets with complex missing patterns.</li> <li>More robust than SimpleImputer for handling missing data.</li> </ul> </li> <li>Cons:<ul> <li>Can be computationally intensive and slower than SimpleImputer.</li> <li>Requires careful tuning of model parameters.</li> </ul> </li> <li>Example:</li> </ul>"},{"location":"Machine%20Learning/76_handling_missing_data/#datawig","title":"Datawig:","text":"<p>Datawig is a library specifically designed for imputing missing values in tabular data using deep learning models.</p>"},{"location":"Machine%20Learning/76_handling_missing_data/#homework","title":"Homework","text":"<ul> <li>Try out these techniques for categorical data</li> </ul>"},{"location":"Machine%20Learning/76_handling_missing_data/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Machine%20Learning/80_classification_metrics/","title":"Classification Metrics","text":"<p>title: SciKit-Learn Classification Metrics keywords: [machine learning, machine learning classification, machine learning classification metrics, decision trees, python, precision, recall, f1 score, weighted, accuracy, linear regression] description: Here are some commonly used classification metrics along with examples of how to implement them using scikit-learn author: Juma Shafara date: \"2024-03\" date-modified: \"2024-09-18\"</p> <p></p> <p>In scikit-learn, classification metrics are essential tools to evaluate the performance of a classification model. </p> <p>They provide insights into how well the model is performing and where it may need improvements. </p> <p>Here are some commonly used classification metrics along with examples of how to implement them using scikit-learn:</p> <pre><code>import pandas as pd\n</code></pre> <pre><code># True labels\ny_true = [0, 1, 1, 0, 1]\n# Predicted labels\ny_pred = [1, 1, 0, 0, 1]\n</code></pre> <p></p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code>from sklearn.metrics import accuracy_score\n\n# Calculate accuracy\naccuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\nprint(\"Accuracy:\", accuracy)\n</code></pre> <pre>\n<code>Accuracy: 0.6\n</code>\n</pre> <p></p> <pre><code>from sklearn.metrics import precision_score\n\n# Calculate precision\nprecision = precision_score(y_true, y_pred)\nprint(\"Precision:\", precision)\n</code></pre> <pre>\n<code>Precision: 0.6666666666666666\n</code>\n</pre> <p></p> <pre><code>from sklearn.metrics import recall_score\n\n# Calculate recall\nrecall = recall_score(y_true, y_pred)\nprint(\"Recall:\", recall)\n</code></pre> <pre>\n<code>Recall: 0.6666666666666666\n</code>\n</pre> <pre><code>from sklearn.metrics import f1_score\n\n# Calculate F1 Score\nf1 = f1_score(y_true, y_pred)\nprint(\"F1 Score:\", f1)\n</code></pre> <pre>\n<code>F1 Score: 0.6666666666666666\n</code>\n</pre> <p>In classification tasks, metrics like precision, recall, and F1-score are commonly used to evaluate the performance of a model. When dealing with multi-class classification, you often need a way to aggregate these metrics across all classes. Three common methods for doing this are micro-average, macro-average, and weighted average.</p> <p></p> <pre><code># Calculate micro-average\nmicro_precision = precision_score(y_true, y_pred, average='micro')\nmicro_recall = recall_score(y_true, y_pred, average='micro')\nmicro_f1 = f1_score(y_true, y_pred, average='micro')\n\nprint('Micro Precision:', micro_precision)\nprint('Micro Recall:', micro_recall)\nprint('Micro F1:', micro_f1)\n</code></pre> <pre>\n<code>Micro Precision: 0.6\nMicro Recall: 0.6\nMicro F1: 0.6\n</code>\n</pre> <p></p> <pre><code># Calculate macro-average\nmacro_precision = precision_score(y_true, y_pred, average='macro')\nmacro_recall = recall_score(y_true, y_pred, average='macro')\nmacro_f1 = f1_score(y_true, y_pred, average='macro')\n\nprint('Macro Precision:', macro_precision)\nprint('Macro Recall:', macro_recall)\nprint('Macro F1:', macro_f1)\n</code></pre> <pre>\n<code>Macro Precision: 0.5833333333333333\nMacro Recall: 0.5833333333333333\nMacro F1: 0.5833333333333333\n</code>\n</pre> <pre><code># Calculate weighted-average\nweighted_precision = precision_score(y_true, y_pred, average='weighted')\nweighted_recall = recall_score(y_true, y_pred, average='weighted')\nweighted_f1 = f1_score(y_true, y_pred, average='weighted')\n\nprint('Weighted Precision:', weighted_precision)\nprint('Weighted Recall:', weighted_recall)\nprint('Weighted F1:', weighted_f1)\n</code></pre> <pre>\n<code>Weighted Precision: 0.6\nWeighted Recall: 0.6\nWeighted F1: 0.6\n</code>\n</pre> <p>Weighted_precision:</p> <p><code>(Precision_A * N_A + Precision_B * N_B, ... , Precision_n * N_n) / (N_A + N_B + ... + N_n)</code></p> <ul> <li>Micro-average is useful when overall performance across all classes is important</li> <li>Macro-average is helpful when you want to evaluate the model's performance on smaller classes equally.</li> <li>Weighted average is suitable when you want to account for class imbalance.</li> </ul> <pre><code>from sklearn.metrics import classification_report\n\n# Generate classification report\nclass_report_dict = classification_report(y_true, y_pred, output_dict=True)\nclass_report_df = pd.DataFrame(class_report_dict).transpose()\n\nprint(\"Classification Report:\\n\")\nclass_report_df\n</code></pre> <pre>\n<code>Classification Report:\n\n</code>\n</pre> precision recall f1-score support 0 0.500000 0.500000 0.500000 2.0 1 0.666667 0.666667 0.666667 3.0 accuracy 0.600000 0.600000 0.600000 0.6 macro avg 0.583333 0.583333 0.583333 5.0 weighted avg 0.600000 0.600000 0.600000 5.0 <pre><code>(0.666667 + 0.5)/2\n</code></pre> <pre>\n<code>0.5833335</code>\n</pre> <pre><code>from sklearn.metrics import confusion_matrix\nimport pandas as pd\n\n# Calculate confusion matrix\nconf_matrix = confusion_matrix(y_true, y_pred)\n\nconf_matrix = pd.DataFrame(conf_matrix, index=[1, 0], columns=[1, 0])\n# print(\"Confusion Matrix:\\n\", conf_matrix)\n\nconf_matrix\n</code></pre> 1 0 1 1 1 0 1 2 <p>These are just a few of the many classification metrics available in scikit-learn. Depending on your specific problem and requirements, you may want to explore other metrics as well.</p> <p>Understanding these metrics and how they are computed can provide valuable insights into the performance of a classification model and help in making informed decisions about its improvement.</p>"},{"location":"Machine%20Learning/80_classification_metrics/#accuracy","title":"Accuracy:","text":"<ul> <li>Accuracy measures the ratio of correctly predicted instances to the total instances.</li> <li>Formula:</li> </ul> <p><code>Accuracy = Number of Correct Predictions / Total Number of Predictions</code></p>"},{"location":"Machine%20Learning/80_classification_metrics/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Machine%20Learning/80_classification_metrics/#precision","title":"Precision:","text":"<ul> <li>Precision measures the ability of the classifier not to label as positive a sample that is negative.</li> <li>Precision is simply the models ability to not make a mistake</li> <li>Formula:</li> </ul> <p><code>Precision = True Positives / (True Positives + False Positives)</code></p>"},{"location":"Machine%20Learning/80_classification_metrics/#recall-also-known-as-sensitivity-or-true-positive-rate","title":"Recall (also known as Sensitivity or True Positive Rate):","text":"<ul> <li>Recall measures the ability of the classifier to find all the positive samples.</li> <li>Formula:</li> </ul> <p><code>Recall = True Positives / (True Positives + False Negatives)</code></p>"},{"location":"Machine%20Learning/80_classification_metrics/#f1-score","title":"F1 Score:","text":"<ul> <li>F1 Score is the harmonic mean of precision and recall. </li> <li>It provides a balance between precision and recall.</li> <li>Formula:</li> </ul> <p><code>F1 Score = (2 x Precision x Recall) / (Precision + Recall)</code></p>"},{"location":"Machine%20Learning/80_classification_metrics/#micro-average","title":"Micro-average:","text":"<ul> <li>Calculate metrics globally by counting the total true positives, false negatives, and false positives.</li> <li>This method gives equal weight to each individual prediction, regardless of class imbalance.</li> </ul>"},{"location":"Machine%20Learning/80_classification_metrics/#macro-average","title":"Macro-average:","text":"<ul> <li>Calculate metrics for each class individually and then average them.</li> <li>This method treats all classes equally, giving each class the same weight.</li> <li>To obtain macro-averaged precision, recall, and F1-score:<ul> <li>Calculate precision, recall, and F1-score for each class.</li> <li>Average the precision, recall, and F1-score across all classes.</li> </ul> </li> </ul>"},{"location":"Machine%20Learning/80_classification_metrics/#weighted-average","title":"Weighted average:","text":"<ul> <li>Calculate metrics for each class individually and then average them, weighted by the number of true instances for each class.</li> <li>This method considers class imbalance by giving more weight to classes with more instances.</li> <li>To obtain weighted-averaged precision, recall, and F1-score:<ul> <li>Calculate precision, recall, and F1-score for each class.</li> <li>Weighted average is calculated as the <code>sum of (metric * class_weight) / total_number_of_samples</code>, where class_weight is the ratio of the number of true instances in the given class to the total number of true instances.</li> </ul> </li> </ul>"},{"location":"Machine%20Learning/80_classification_metrics/#the-classification-report","title":"The Classification Report","text":"<p>The classification report in scikit-learn provides a comprehensive summary of different classification metrics for each class in the dataset. It includes precision, recall, F1-score, and support (the number of true instances for each label). Here's how you can generate a classification report:</p>"},{"location":"Machine%20Learning/80_classification_metrics/#confusion-matrix","title":"Confusion Matrix:","text":"<ul> <li>A confusion matrix is a table that is often used to describe the performance of a classification model.</li> <li>It presents a summary of the model's predictions on the classification problem, showing correct predictions as well as types of errors made.</li> </ul>"},{"location":"Machine%20Learning/80_classification_metrics/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Machine%20Learning/81_regression_metrics/","title":"Regression Metrics","text":"<p>title: Regression Metrics keywords: [Regression Metrics, Mean Absolute Error, Mean Square Error, Root Mean Squared Error, R-Squared (Coefficient of Determination)] description: Learn several metrics to evaluate the performance of regression models author: Juma Shafara date: \"2024-03\"</p> <p></p> <p>In regression tasks, the goal is to predict continuous numerical values. Scikit-learn provides several metrics to evaluate the performance of regression models. In this notebook, we will look at the following</p> <ul> <li>Mean Absolute Error</li> <li>Mean Square Error</li> <li>Root Mean Squared Error</li> <li>R-Squared (Coefficient of Determination)</li> </ul> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># True labels\ny_true = [2.5, 3.7, 5.1, 4.2, 6.8]\n# Predicted labels\ny_pred = [2.3, 3.5, 4.9, 4.0, 6.5]\n</code></pre> <pre><code>from sklearn.metrics import mean_absolute_error\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_true, y_pred)\nprint(\"Mean Absolute Error (MAE):\", mae)\n</code></pre> <pre>\n<code>Mean Absolute Error (MAE): 0.21999999999999992\n</code>\n</pre> <pre><code>from sklearn.metrics import mean_squared_error\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_true, y_pred)\nprint(\"Mean Squared Error (MSE):\", mse)\n</code></pre> <pre>\n<code>Mean Squared Error (MSE): 0.04999999999999997\n</code>\n</pre> <pre><code>from sklearn.metrics import root_mean_squared_error\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse = root_mean_squared_error(y_true, y_pred,)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\n</code></pre> <pre>\n<code>Root Mean Squared Error (RMSE): 0.2236067977499789\n</code>\n</pre> <pre><code>from sklearn.metrics import r2_score\n\n# Calculate R-squared (Coefficient of Determination)\nr2 = r2_score(y_true, y_pred)\nprint(\"R-squared (R2 Score):\", r2)\n</code></pre> <pre>\n<code>R-squared (R2 Score): 0.975896644812958\n</code>\n</pre> <p>Understanding these metrics can help you assess the performance of your regression model and make necessary adjustments to improve its accuracy.</p>"},{"location":"Machine%20Learning/81_regression_metrics/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Machine%20Learning/81_regression_metrics/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE):","text":"<ul> <li>MAE measures the average absolute errors between predicted values and actual values.</li> <li>Imagine you're trying to hit a target with darts. The MAE is like calculating the average distance between where your darts hit and the bullseye. You just sum up how far each dart landed from the center (without caring if it was too short or too far) and then find the average. The smaller the MAE, the closer your predictions are to the actual values.</li> <li>Formula: \\(\\(\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{true}} - y_{\\text{pred}}|\\)\\)</li> </ul>"},{"location":"Machine%20Learning/81_regression_metrics/#mean-squared-error-mse","title":"Mean Squared Error (MSE):","text":"<ul> <li>MSE measures the average of the squares of the errors between predicted values and actual values.</li> <li>This is similar to MAE, but instead of just adding up the distances, you square them before averaging. Squaring makes bigger differences more noticeable (by making them even bigger), so MSE penalizes larger errors more than smaller ones.</li> <li>Formula: \\(\\(\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{true}} - y_{\\text{pred}})^2\\)\\)</li> </ul>"},{"location":"Machine%20Learning/81_regression_metrics/#root-mean-squared-error-rmse","title":"Root Mean Squared Error (RMSE):","text":"<ul> <li>RMSE is the square root of the MSE, providing a more interpretable scale since it's in the same units as the target variable.</li> <li>It's just like MSE, but we take the square root of the result. This brings the error back to the same scale as the original target variable, which makes it easier to interpret. RMSE gives you an idea of how spread out your errors are in the same units as your data.</li> <li>Formula: \\(\\(\\text{RMSE} = \\sqrt{\\text{MSE}}\\)\\)</li> </ul>"},{"location":"Machine%20Learning/81_regression_metrics/#r-squared-coefficient-of-determination","title":"R-squared (Coefficient of Determination)**:","text":"<ul> <li>R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables.</li> <li>This tells you how well your model's predictions match the actual data compared to a simple average. If R-squared is 1, it means your model perfectly predicts the target variable. If it's 0, it means your model is no better than just predicting the mean of the target variable. So, the closer R-squared is to 1, the better your model fits the data.</li> <li> <p>Formula: \\(\\(R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_{\\text{true}} - y_{\\text{pred}})^2}{\\sum_{i=1}^{n} (y_{\\text{true}} - \\bar{y}_{\\text{true}})^2}\\)\\)</p> </li> <li> <p>where $$ \\bar{y}_{\\text{true}}$$ is the mean of the observed data.</p> </li> </ul>"},{"location":"Machine%20Learning/81_regression_metrics/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Machine%20Learning/82_sklearn_pipeline/","title":"Pipelines","text":"<p>title: Pipeline keywords: [pipeline, machine learning, supervised machine learning, data preprocessing, preprocessing, feature extraction, feature selection, model fitting] description: In this notebook we are gonna be looking at the following steps in a pipeline preprocessing, feature extraction, feature selection, model fitting author: Juma Shafara date: \"2024-03\"</p> <p></p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code>## install the version of dataidea used for this notebook\n# !pip install --upgrade dataidea\n</code></pre> <pre><code># Let's import some packages\n\nimport numpy as np\nimport pandas as pd\nimport dataidea as di\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsRegressor\n</code></pre> <pre><code># loading the data set\n\ndata = di.loadDataset('boston')\n</code></pre> <p>The Boston Housing Dataset</p> <p>The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of  Boston MA. The following describes the dataset columns:</p> <ul> <li>CRIM - per capita crime rate by town</li> <li>ZN - proportion of residential land zoned for lots over 25,000 sq.ft.</li> <li>INDUS - proportion of non-retail business acres per town.</li> <li>CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)</li> <li>NOX - nitric oxides concentration (parts per 10 million)</li> <li>RM - average number of rooms per dwelling</li> <li>AGE - proportion of owner-occupied units built prior to 1940</li> <li>DIS - weighted distances to five Boston employment centres</li> <li>RAD - index of accessibility to radial highways</li> <li>TAX - full-value property-tax rate per $10,000</li> <li>PTRATIO - pupil-teacher ratio by town</li> <li>B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</li> <li>LSTAT - % lower status of the population</li> <li>MEDV - Median value of owner-occupied homes in $1000's</li> </ul> <pre><code># looking at the top part\n\ndata.head()\n</code></pre> CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0 0.469 6.421 78.9 4.9671 2 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0 0.469 7.185 61.1 4.9671 2 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0 0.458 6.998 45.8 6.0622 3 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0 0.458 7.147 54.2 6.0622 3 222.0 18.7 396.90 5.33 36.2 <pre><code># Selecting our X set and y\n\nX = data.drop('MEDV', axis=1)\ny = data.MEDV\n</code></pre> <p>Now we can train the <code>KNeighborsRegressor</code> model, this model naturally makes predictions by averaging the values of the 5 neighbors to the point that you want to predict</p> <pre><code># lets traing the KNeighborsRegressor\n\nknn_model = KNeighborsRegressor() # instanciate the model class\nknn_model.fit(X, y) # train the model on X, y\nscore = knn_model.score(X, y) # obtain the model score on X, y\npredicted_y = knn_model.predict(X) # make predictions on X\n\nprint('score:', score)\n</code></pre> <pre>\n<code>score: 0.716098217736928\n</code>\n</pre> <p>Now lets go ahead and try to visualize the performance of the model. The scatter plot is of true labels against predicted labels. Do you think the model is doing well?</p> <pre><code># looking at the performance \n\nplt.scatter(y, predicted_y)\nplt.title('Model Performance')\nplt.xlabel('Predicted y')\nplt.ylabel('True y')\nplt.show()\n</code></pre> <pre><code>from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression # score function for ANOVA with continuous outcome\n</code></pre> <pre><code># lets do some feature selection using ANOVA\n\ndata_num = data.drop(['CHAS','RAD'], axis=1) # dropping categorical\nX = data_num.drop(\"MEDV\", axis=1) \ny = data_num.MEDV\n\n# using SelectKBest\ntest_reg = SelectKBest(score_func=f_regression, k=6) \nfit_boston = test_reg.fit(X, y)\nindexes = fit_boston.get_support(indices=True)\n\nprint(fit_boston.scores_)\nprint(indexes)\n</code></pre> <pre>\n<code>[ 89.48611476  75.2576423  153.95488314 112.59148028 471.84673988\n  83.47745922  33.57957033 141.76135658 175.10554288  63.05422911\n 601.61787111]\n[ 2  3  4  7  8 10]\n</code>\n</pre> <p>From above, we can see from above that the best features for now are those in indexes <code>[ 2  3  4  7  8 10]</code> in the <code>num_data</code> dataset. Lets find them in the <code>data</code> and add on our categorical ones to set up our new X set</p> <pre><code>data_num.sample()\n</code></pre> CRIM ZN INDUS NOX RM AGE DIS TAX PTRATIO B LSTAT MEDV 211 0.37578 0.0 10.59 0.489 5.404 88.6 3.665 277.0 18.6 395.24 23.98 19.3 <pre><code># redifining the X set \n\nnew_X = data[['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT', 'CHAS','RAD']]\n</code></pre> <pre><code>knn_model = KNeighborsRegressor()\nknn_model.fit(new_X, y)\nnew_score = knn_model.score(new_X, y)\nnew_predicted_y = knn_model.predict(new_X)\n\nprint('Feature selected score:', new_score)\n</code></pre> <pre>\n<code>Feature selected score: 0.8324963639640872\n</code>\n</pre> <p>The model seems to score better with a significant increment in accuracy from <code>0.71</code> to <code>0.83</code>. As like last time, let us try to visualize the difference in performance</p> <pre><code>plt.scatter(y, new_predicted_y)\nplt.title('Model Performance')\nplt.xlabel('New Predicted y')\nplt.ylabel('True y')\nplt.show()\n</code></pre> <p>I do not know about you, but as for me, I notice a meaningful improvement in the predictions made from the model considering this scatter plot</p> <pre><code># importing the StandardScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# instanciating the StandardScaler\nscaler = StandardScaler() \n</code></pre> <p>This initializes a <code>StandardScaler</code> which standardizes features by removing the mean and scaling to unit variance. It's applied to numeric columns to ensure they are on a similar scale.</p> <pre><code>standardized_data_num = scaler.fit_transform(\n    data[['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']]\n    ) # rescaline numeric features\nstandardized_data_num_df = pd.DataFrame(\n    standardized_data_num, \n    columns=['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT'] \n    ) # converting the standardized to dataframe\n</code></pre> <pre><code>standardized_data_num_df.head()\n</code></pre> INDUS NOX RM TAX PTRATIO LSTAT 0 -1.287909 -0.144217 0.413672 -0.666608 -1.459000 -1.075562 1 -0.593381 -0.740262 0.194274 -0.987329 -0.303094 -0.492439 2 -0.593381 -0.740262 1.282714 -0.987329 -0.303094 -1.208727 3 -1.306878 -0.835284 1.016303 -1.106115 0.113032 -1.361517 4 -1.306878 -0.835284 1.228577 -1.106115 0.113032 -1.026501 <ol> <li>Categorical Transformer: </li> </ol> <pre><code># importing the OneHotEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n# instanciating the OneHotEncoder\none_hot_encoder = OneHotEncoder()\n</code></pre> <p>This initializes a <code>OneHotEncoder</code> which converts categorical variables into a format that can be provided to ML algorithms to do a better job in prediction.</p> <pre><code>encoded_data_cat = one_hot_encoder.fit_transform(data[['CHAS', 'RAD']])\nencoded_data_cat_array = encoded_data_cat.toarray()\n# Get feature names\nfeature_names = one_hot_encoder.get_feature_names_out(['CHAS', 'RAD'])\n\nencoded_data_cat_df = pd.DataFrame(\n    data=encoded_data_cat_array,\n    columns=feature_names\n)\n</code></pre> <pre><code>encoded_data_cat_df.head()\n</code></pre> CHAS_0 CHAS_1 RAD_1 RAD_2 RAD_3 RAD_4 RAD_5 RAD_6 RAD_7 RAD_8 RAD_24 0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 4 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 <p>Let us add that to the new X and form a standardized new X set</p> <pre><code>transformed_new_X = pd.concat(\n    [standardized_data_num_df, encoded_data_cat_df], \n    axis=1\n    )\n</code></pre> <pre><code>transformed_new_X.head()\n</code></pre> INDUS NOX RM TAX PTRATIO LSTAT CHAS_0 CHAS_1 RAD_1 RAD_2 RAD_3 RAD_4 RAD_5 RAD_6 RAD_7 RAD_8 RAD_24 0 -1.287909 -0.144217 0.413672 -0.666608 -1.459000 -1.075562 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 -0.593381 -0.740262 0.194274 -0.987329 -0.303094 -0.492439 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 -0.593381 -0.740262 1.282714 -0.987329 -0.303094 -1.208727 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3 -1.306878 -0.835284 1.016303 -1.106115 0.113032 -1.361517 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 4 -1.306878 -0.835284 1.228577 -1.106115 0.113032 -1.026501 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 <pre><code>knn_model = KNeighborsRegressor()\nknn_model.fit(transformed_new_X, y)\nnew_transformed_score = knn_model.score(transformed_new_X, y)\nnew_predicted_y = knn_model.predict(transformed_new_X)\n\nprint('Transformed score:', new_transformed_score)\n</code></pre> <pre>\n<code>Transformed score: 0.8734524530397529\n</code>\n</pre> <p>This new models appears to do better than the earlier ones with an improvement in score from <code>0.83</code> to <code>0.87</code>. Do you think this is now a good model?</p> <p>It turns out the above efforts to improve the performance of the model add extra steps to pass before you can have a good model. But what about if we can put together the transformers into on object we do most of that stuff.</p> <p>The sklearn <code>Pipeline</code> allows you to sequentially apply a list of transformers to preprocess the data and, if desired, conclude the sequence with a final predictor for predictive modeling.</p> <p>Intermediate steps of the pipeline must be \u2018transforms\u2019, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.</p> <p>Let us build a model that puts together transformation and modelling steps into one <code>pipeline</code> object</p> <pre><code># lets import the Pipeline from sklearn\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n</code></pre> <pre><code>numeric_cols = ['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']\ncategorical_cols = ['CHAS', 'RAD']\n</code></pre> <pre><code># Preprocessing steps\nnumeric_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder()\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('numerical', numeric_transformer, numeric_cols),\n        ('categorical', categorical_transformer, categorical_cols)\n    ])\n\n# Pipeline\npipe = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', KNeighborsRegressor())\n])\n\n# display pipe\npipe\n</code></pre> <pre>Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('numerical', StandardScaler(),\n                                                  ['INDUS', 'NOX', 'RM', 'TAX',\n                                                   'PTRATIO', 'LSTAT']),\n                                                 ('categorical',\n                                                  OneHotEncoder(),\n                                                  ['CHAS', 'RAD'])])),\n                ('model', KNeighborsRegressor())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. <p>\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('numerical', StandardScaler(),\n                                                  ['INDUS', 'NOX', 'RM', 'TAX',\n                                                   'PTRATIO', 'LSTAT']),\n                                                 ('categorical',\n                                                  OneHotEncoder(),\n                                                  ['CHAS', 'RAD'])])),\n                ('model', KNeighborsRegressor())])</pre> <p>\u00a0preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformer<pre>ColumnTransformer(transformers=[('numerical', StandardScaler(),\n                                 ['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO',\n                                  'LSTAT']),\n                                ('categorical', OneHotEncoder(),\n                                 ['CHAS', 'RAD'])])</pre> <p></p> numerical<pre>['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']</pre> \u00a0StandardScaler?Documentation for StandardScaler<pre>StandardScaler()</pre> categorical<pre>['CHAS', 'RAD']</pre> \u00a0OneHotEncoder?Documentation for OneHotEncoder<pre>OneHotEncoder()</pre> \u00a0KNeighborsRegressor?Documentation for KNeighborsRegressor<pre>KNeighborsRegressor()</pre> <p> </p> <p>The code above sets up a data preprocessing and modeling pipeline using the <code>scikit-learn</code> library. Let's break down each part:</p> <p>Now we can instead fit the Pipeline and use it for making predictions</p> <pre><code># Fit the pipeline\npipe.fit(new_X, y)\n\n# Score the pipeline\npipe_score = pipe.score(new_X, y)\n\n# Predict using the pipeline\npipe_predicted_y = pipe.predict(new_X)\n\nprint('Pipe Score:', pipe_score)\n</code></pre> <pre>\n<code>Pipe Score: 0.8734524530397529\n</code>\n</pre> <pre><code>plt.scatter(y, pipe_predicted_y)\nplt.title('Pipe Performance')\nplt.xlabel('Pipe Predicted y')\nplt.ylabel('True y')\nplt.show()\n</code></pre> <p>We can observe that the model still gets the same good score, but now all the transformation steps, both on numeric and categorical variables are in a single pipeline object together with the model.</p>"},{"location":"Machine%20Learning/82_sklearn_pipeline/#pipeline","title":"Pipeline","text":"<p>A pipeline is a series of data processing steps that are chained together sequentially. Each step in the pipeline typically performs some transformation on the data. In this notebook we are gonna be looking at the following steps in a pipeline: </p> <ul> <li>Preprocessing</li> <li>Feature extraction</li> <li>Feature selection</li> <li>Model fitting</li> </ul>"},{"location":"Machine%20Learning/82_sklearn_pipeline/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Machine%20Learning/82_sklearn_pipeline/#lets-redefine-a-model","title":"Let's redefine a model","text":"<p>In week 4, we introduced ourselves to Machine Learning Concepts, in week 5 we learned some statistical tests and we applied them in week 7 to find the best feature and transform them to efficient forms. In this section, we will build on top of those concepts to redefine what a Machine Learning model is and hence come up with a more efficient way of developing good Machine Learning models</p> <p>First, let's install the <code>dataidea</code> package, which will help us with loading packages and datasets with much more ease</p>"},{"location":"Machine%20Learning/82_sklearn_pipeline/#training-our-first-model","title":"Training our first model","text":"<p>In week 4, we learned that to train a model (for supervised machine learning), we needed to have a set of X variables (also called independent, predictor etc), and then, we needed a y variable (also called dependent, outcome, predicted etc).</p>"},{"location":"Machine%20Learning/82_sklearn_pipeline/#some-feature-selection","title":"Some feature selection.","text":"<p>Feature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.</p> <p>In week 7 we learned that having irrelevant features in your data can decrease the accuracy of many models. In the code below, we try to find out the best features that best contribute to the outcome variable</p>"},{"location":"Machine%20Learning/82_sklearn_pipeline/#training-our-second-model","title":"Training our second model","text":"<p>Now that we have selected out the features, X that we thing best contribute to the outcome, let's retrain our machine learning model and see if we are gonna get better results</p>"},{"location":"Machine%20Learning/82_sklearn_pipeline/#transforming-the-data","title":"Transforming the data","text":"<p>In week 7, we learned some advantages of scaling our data like:</p> <ul> <li>preventing dominance by features with larger scales</li> <li>faster convergence in optimization algorithms</li> <li> <p>reduce the impact of outliers</p> </li> <li> <p>Numeric Transformer: </p> </li> </ul>"},{"location":"Machine%20Learning/82_sklearn_pipeline/#training-our-third-model","title":"Training our third model","text":"<p>Now that we have the right features selected and standardized, let us train a new model and see if it is gonna beat the first models</p>"},{"location":"Machine%20Learning/82_sklearn_pipeline/#the-pipeline","title":"The Pipeline","text":""},{"location":"Machine%20Learning/82_sklearn_pipeline/#combine-preprocessing-steps","title":"Combine Preprocessing Steps","text":"<ol> <li>ColumnTransformer:    <pre><code>preprocessor = ColumnTransformer(\n    transformers=[\n        ('numerical', numeric_transformer, numeric_cols),\n        ('categorical', categorical_transformer, categorical_cols)\n    ])\n</code></pre></li> <li>The <code>ColumnTransformer</code> is used to apply different preprocessing steps to different columns of the data. It combines the <code>numeric_transformer</code> for numeric columns and the <code>categorical_transformer</code> for categorical columns.</li> <li><code>numeric_cols</code> and <code>categorical_cols</code> are lists containing the names of numeric and categorical columns respectively.</li> </ol>"},{"location":"Machine%20Learning/82_sklearn_pipeline/#pipeline_1","title":"Pipeline","text":"<ol> <li>Pipeline Setup:    <pre><code>pipe = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', KNeighborsRegressor())\n])\n</code></pre></li> <li>A <code>Pipeline</code> is created which sequentially applies a list of transforms and a final estimator. </li> <li>The <code>preprocessor</code> step applies the <code>ColumnTransformer</code> defined earlier.</li> <li>The <code>model</code> step applies a <code>KNeighborsRegressor</code>, which is a regression model that predicts the target variable based on the k-nearest neighbors in the feature space.</li> </ol>"},{"location":"Machine%20Learning/82_sklearn_pipeline/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Machine%20Learning/83_GridSearchCV/","title":"Hyperparameter Tuning","text":"<p>title: GridSearchCV description: GridSearchCV is a method in the scikit-learn library, which is a popular machine learning library in Python keywords: [What is GridSearchCV, What is a Pipeline, Machine Learning, Machine Learning Pipeline, ColumnTransformer, StandardScaler, OneHotEncoder, sklearn, KNeighborsRegressor, Hyperparameters, Hyperparameter tuning] author: Juma Shafara date: \"2024-03\"</p> <p></p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># Let's import some packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport dataidea as di\nfrom sklearn.neighbors import KNeighborsRegressor\n</code></pre> <pre><code># lets import the Pipeline from sklearn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n</code></pre> <pre><code># loading the data set\ndata = di.loadDataset('boston')\n</code></pre> <pre><code># looking at the top part\ndata.head()\n</code></pre> CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0 0.469 6.421 78.9 4.9671 2 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0 0.469 7.185 61.1 4.9671 2 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0 0.458 6.998 45.8 6.0622 3 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0 0.458 7.147 54.2 6.0622 3 222.0 18.7 396.90 5.33 36.2 Reveal more about the Boston dataset  The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of [ Boston MA](http://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). The following describes the dataset columns:  * CRIM - per capita crime rate by town * ZN - proportion of residential land zoned for lots over 25,000 sq.ft. * INDUS - proportion of non-retail business acres per town. * CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise) * NOX - nitric oxides concentration (parts per 10 million) * RM - average number of rooms per dwelling * AGE - proportion of owner-occupied units built prior to 1940 * DIS - weighted distances to five Boston employment centres * RAD - index of accessibility to radial highways * TAX - full-value property-tax rate per \\$10,000 * PTRATIO - pupil-teacher ratio by town * B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town * LSTAT - % lower status of the population * MEDV - Median value of owner-occupied homes in \\$1000's  <pre><code># Selecting our X set and y\nX = data.drop('MEDV', axis=1)\ny = data.MEDV\n</code></pre> <pre><code># numeric columns\nnumeric_cols = [\n    'INDUS', 'NOX', 'RM', \n    'TAX', 'PTRATIO', 'LSTAT'\n    ]\n\n# categorical columns\ncategorical_cols = ['CHAS', 'RAD']\n</code></pre> <pre><code># Preprocessing steps\nnumeric_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\n# Combine preprocessing steps\ncolumn_transformer = ColumnTransformer(\n    transformers=[\n        ('numeric', numeric_transformer, numeric_cols),\n        ('categorical', categorical_transformer, categorical_cols)\n    ])\n</code></pre> <pre><code># Pipeline\npipe = Pipeline([\n    ('column_transformer', column_transformer),\n    ('model', KNeighborsRegressor(n_neighbors=10))\n])\n\npipe\n</code></pre> <pre>Pipeline(steps=[('column_transformer',\n                 ColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                                  ['INDUS', 'NOX', 'RM', 'TAX',\n                                                   'PTRATIO', 'LSTAT']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['CHAS', 'RAD'])])),\n                ('model', KNeighborsRegressor(n_neighbors=10))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. <p>\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('column_transformer',\n                 ColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                                  ['INDUS', 'NOX', 'RM', 'TAX',\n                                                   'PTRATIO', 'LSTAT']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['CHAS', 'RAD'])])),\n                ('model', KNeighborsRegressor(n_neighbors=10))])</pre> <p>\u00a0column_transformer: ColumnTransformer?Documentation for column_transformer: ColumnTransformer<pre>ColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                 ['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO',\n                                  'LSTAT']),\n                                ('categorical',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['CHAS', 'RAD'])])</pre> <p></p> numeric<pre>['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']</pre> \u00a0StandardScaler?Documentation for StandardScaler<pre>StandardScaler()</pre> categorical<pre>['CHAS', 'RAD']</pre> \u00a0OneHotEncoder?Documentation for OneHotEncoder<pre>OneHotEncoder(handle_unknown='ignore')</pre> \u00a0KNeighborsRegressor?Documentation for KNeighborsRegressor<pre>KNeighborsRegressor(n_neighbors=10)</pre> <p> </p> <pre><code># Fit the pipeline\npipe.fit(X, y)\n\n# Score the pipeline\npipe_score = pipe.score(X, y)\n\n# Predict using the pipeline\npipe_predicted_y = pipe.predict(X)\n\nprint('Pipe Score:', pipe_score)\n</code></pre> <pre>\n<code>Pipe Score: 0.818140222027107\n</code>\n</pre> <pre><code>from sklearn.model_selection import GridSearchCV\n</code></pre> <pre><code>model = GridSearchCV(\n    estimator=pipe,\n    param_grid={\n        'model__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    },\n    cv=3\n    )\n</code></pre> <pre><code>model.fit(X, y)\n</code></pre> <pre>GridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('column_transformer',\n                                        ColumnTransformer(transformers=[('numeric',\n                                                                         StandardScaler(),\n                                                                         ['INDUS',\n                                                                          'NOX',\n                                                                          'RM',\n                                                                          'TAX',\n                                                                          'PTRATIO',\n                                                                          'LSTAT']),\n                                                                        ('categorical',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['CHAS',\n                                                                          'RAD'])])),\n                                       ('model',\n                                        KNeighborsRegressor(n_neighbors=10))]),\n             param_grid={'model__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. <p>\u00a0\u00a0GridSearchCV?Documentation for GridSearchCViFitted<pre>GridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('column_transformer',\n                                        ColumnTransformer(transformers=[('numeric',\n                                                                         StandardScaler(),\n                                                                         ['INDUS',\n                                                                          'NOX',\n                                                                          'RM',\n                                                                          'TAX',\n                                                                          'PTRATIO',\n                                                                          'LSTAT']),\n                                                                        ('categorical',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['CHAS',\n                                                                          'RAD'])])),\n                                       ('model',\n                                        KNeighborsRegressor(n_neighbors=10))]),\n             param_grid={'model__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})</pre> <p>estimator: Pipeline<pre>Pipeline(steps=[('column_transformer',\n                 ColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                                  ['INDUS', 'NOX', 'RM', 'TAX',\n                                                   'PTRATIO', 'LSTAT']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['CHAS', 'RAD'])])),\n                ('model', KNeighborsRegressor(n_neighbors=10))])</pre> <p></p> \u00a0column_transformer: ColumnTransformer?Documentation for column_transformer: ColumnTransformer<pre>ColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                 ['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO',\n                                  'LSTAT']),\n                                ('categorical',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['CHAS', 'RAD'])])</pre> numeric<pre>['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']</pre> \u00a0StandardScaler?Documentation for StandardScaler<pre>StandardScaler()</pre> categorical<pre>['CHAS', 'RAD']</pre> \u00a0OneHotEncoder?Documentation for OneHotEncoder<pre>OneHotEncoder(handle_unknown='ignore')</pre> \u00a0KNeighborsRegressor?Documentation for KNeighborsRegressor<pre>KNeighborsRegressor(n_neighbors=10)</pre> <p> </p> <pre><code>cv_results = pd.DataFrame(model.cv_results_)\ncv_results\n</code></pre> mean_fit_time std_fit_time mean_score_time std_score_time param_model__n_neighbors params split0_test_score split1_test_score split2_test_score mean_test_score std_test_score rank_test_score 0 0.006364 0.003203 0.003916 0.000833 1 {'model__n_neighbors': 1} 0.347172 0.561780 0.295295 0.401415 0.115356 10 1 0.004014 0.000250 0.003659 0.001165 2 {'model__n_neighbors': 2} 0.404829 0.612498 0.276690 0.431339 0.138369 9 2 0.003741 0.000159 0.003376 0.000710 3 {'model__n_neighbors': 3} 0.466325 0.590333 0.243375 0.433345 0.143552 8 3 0.004399 0.000464 0.002981 0.000075 4 {'model__n_neighbors': 4} 0.569672 0.619854 0.246539 0.478688 0.165428 4 4 0.003881 0.000336 0.002855 0.000071 5 {'model__n_neighbors': 5} 0.613900 0.600994 0.230320 0.481738 0.177857 2 5 0.004046 0.000582 0.003318 0.000555 6 {'model__n_neighbors': 6} 0.620587 0.607083 0.225238 0.484302 0.183269 1 6 0.003628 0.000127 0.002781 0.000018 7 {'model__n_neighbors': 7} 0.639693 0.583685 0.218612 0.480663 0.186704 3 7 0.003585 0.000059 0.002839 0.000093 8 {'model__n_neighbors': 8} 0.636143 0.567841 0.209472 0.471152 0.187125 5 8 0.003649 0.000175 0.002755 0.000031 9 {'model__n_neighbors': 9} 0.649335 0.542624 0.197917 0.463292 0.192639 6 9 0.003591 0.000071 0.002790 0.000060 10 {'model__n_neighbors': 10} 0.653370 0.535112 0.191986 0.460156 0.195674 7 <pre><code>model.score(X, y)\n</code></pre> <pre>\n<code>0.8661624926868122</code>\n</pre> Reveal the interpretation of the CV results  These are the results of a grid search cross-validation performed on our pipeline (`pipe`). Let's break down each column:  - `mean_fit_time`: The average time taken to fit the estimator on the training data across all folds. - `std_fit_time`: The standard deviation of the fitting time across all folds. - `mean_score_time`: The average time taken to score the estimator on the test data across all folds. - `std_score_time`: The standard deviation of the scoring time across all folds. - `param_model__n_neighbors`: The value of the `n_neighbors` parameter of the KNeighborsRegressor model in our pipeline for this particular grid search iteration. - `params`: A dictionary containing the parameters used in this grid search iteration. - `split0_test_score`, `split1_test_score`, `split2_test_score`: The test scores obtained for each fold of the cross-validation. Each fold corresponds to one entry here. - `mean_test_score`: The average test score across all folds. - `std_test_score`: The standard deviation of the test scores across all folds. - `rank_test_score`: The rank of this model configuration based on the mean test score. Lower values indicate better performance.  These results allow you to compare different parameter configurations and select the one that performs best based on the mean test score and other relevant metrics.  <p>From the results above, it appears that the best number of neighbors to is 6.</p> <p>From now on, I would like you to consider a GridSearchCV whenever you want to build a machine learning model.</p>"},{"location":"Machine%20Learning/83_GridSearchCV/#what-is-gridsearchcv","title":"What is GridSearchCV","text":"<p><code>GridSearchCV</code> is a method in the scikit-learn library, which is a popular machine learning library in Python. It's used for hyperparameter optimization, which involves searching for the best set of hyperparameters for a machine learning model. In this notebook, we'll learn:</p> <ul> <li>how to setup a proper GridSearchCV and</li> <li>how to use it for hyperparameter optimization.</li> </ul>"},{"location":"Machine%20Learning/83_GridSearchCV/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Machine%20Learning/83_GridSearchCV/#lets-import-some-packages","title":"Let's import some packages","text":"<p>We begin by importing necessary packages and modules. The <code>KNeighborsRegressor</code> model is imported from the <code>sklearn.neighbors</code> module.  KNN regression is a non-parametric method that, in an intuitive manner, approximates the association between independent variables and the continuous outcome by averaging the observations in the same neighbourhood.  Read more about the KNN Regressor from this link </p>"},{"location":"Machine%20Learning/83_GridSearchCV/#lets-import-necessary-components-from-sklearn","title":"Let's import necessary components from sklearn","text":"<p>We import essential components from <code>sklearn</code>, including <code>Pipeline</code>, which we'll use to create a <code>pipe</code> as from the previous section, <code>ColumnTransformer</code>, <code>StandardScaler</code>, and <code>OneHotEncoder</code> which we'll use to transform the numeric and categorical columns respectively to be good for modelling.</p>"},{"location":"Machine%20Learning/83_GridSearchCV/#loading-the-dataset","title":"Loading the dataset","text":"<p>We load the dataset named boston using the <code>loadDataset</code> function, which is inbuilt in the dataidea package. The loaded dataset is stored in the variable <code>data</code>. </p>"},{"location":"Machine%20Learning/83_GridSearchCV/#selecting-features-x-and-target-variable-y","title":"Selecting features (X) and target variable (y)","text":"<p>We separate the features (X) from the target variable (y). Features are stored in <code>X</code>, excluding the target variable 'MEDV', which is stored in <code>y</code>.</p>"},{"location":"Machine%20Learning/83_GridSearchCV/#defining-numeric-and-categorical-columns","title":"Defining numeric and categorical columns","text":"<p>We define lists of column names representing numeric and categorical features in the dataset. We identified these columns as the best features from the previous section of this week. Click here to learn about feature selection</p>"},{"location":"Machine%20Learning/83_GridSearchCV/#preprocessing-steps","title":"Preprocessing steps","text":"<p>We define transformers for preprocessing numeric and categorical features. <code>StandardScaler</code> is used for standardizing numeric features, while <code>OneHotEncoder</code> is used for one-hot encoding categorical features. These transformers are applied to respective feature types using <code>ColumnTransformer</code> as we learned in the previous section.</p>"},{"location":"Machine%20Learning/83_GridSearchCV/#defining-the-pipeline","title":"Defining the pipeline","text":"<p>We construct a machine learning pipeline using <code>Pipeline</code>. The pipeline consists of preprocessing steps (defined in <code>column_transformer</code>) and a <code>KNeighborsRegressor</code> model with 10 neighbors. Learn about Machine Learning Pipelining here</p>"},{"location":"Machine%20Learning/83_GridSearchCV/#fitting-the-pipeline","title":"Fitting the pipeline","text":"<p>As we learned, the Pipeline has the <code>fit</code>, <code>score</code> and <code>predict</code> methods which we use to fit on the dataset (<code>X</code>, <code>y</code>) and evaluate the model's performance using the <code>score()</code> method, finally making predictions.</p>"},{"location":"Machine%20Learning/83_GridSearchCV/#hyperparameter-tuning-using-gridsearchcv","title":"Hyperparameter tuning using GridSearchCV","text":"<p>We perform hyperparameter tuning using <code>GridSearchCV</code>. The pipeline (<code>pipe</code>) serves as the base estimator, and we define a grid of hyperparameters to search through.</p> <p>For this demonstration, we will focus on the number of neighbors for the KNN model.</p>"},{"location":"Machine%20Learning/83_GridSearchCV/#fitting-the-model-for-hyperparameter-tuning","title":"Fitting the model for hyperparameter tuning","text":"<p>We fit the <code>GridSearchCV</code> model on the dataset to find the optimal hyperparameters. This involves preprocessing the data and training the model multiple times using cross-validation.</p>"},{"location":"Machine%20Learning/83_GridSearchCV/#extracting-and-displaying-cross-validation-results","title":"Extracting and displaying cross-validation results","text":"<p>We extract the results of cross-validation performed during hyperparameter tuning and present them in a tabular format using a DataFrame.</p>"},{"location":"Machine%20Learning/83_GridSearchCV/#congratulations","title":"Congratulations!","text":"<p>If you reached here, you have learned the following:</p> <ul> <li>Selecting Features</li> <li>Preprocessing data</li> <li>Creating a Machine Learning Pipeline</li> <li>Creating a GridSearchCV</li> <li>Using the GridSearchCV to find the best Hyperparameters for our Machine Learning model.</li> </ul>"},{"location":"Machine%20Learning/83_GridSearchCV/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Machine%20Learning/Exercises/classification_metrics_quiz/","title":"Classification Metrics Quiz","text":"<p>title: Classification Metrics Quiz keywords: [machine learning, machine learning classification, machine learning classification metrics, decision trees, python, precision, recall, f1 score, weighted, accuracy, linear regression] description: Test your knowledge in the machine learning classification metrics with an objective-type quiz author: Juma Shafara date: \"2024-06\"</p> <p></p>"},{"location":"Machine%20Learning/Exercises/classification_metrics_quiz/#1-what-does-the-accuracy-metric-measure-in-a-classification-model","title":"1. What does the accuracy metric measure in a classification model?","text":"<ul> <li>A. The ratio of correctly predicted instances to the total instances.</li> <li>B. The proportion of positive cases that were correctly identified.</li> <li>C. The proportion of actual positives that were correctly identified.</li> <li>D. The proportion of predicted positive cases that were correct.</li> </ul> Reveal answer <p>A. The ratio of correctly predicted instances to the total instances.</p>"},{"location":"Machine%20Learning/Exercises/classification_metrics_quiz/#2-which-metric-is-most-suitable-for-evaluating-the-performance-of-a-classification-model-when-dealing-with-imbalanced-datasets","title":"2. Which metric is most suitable for evaluating the performance of a classification model when dealing with imbalanced datasets?","text":"<ul> <li>A. Accuracy</li> <li>B. Precision</li> <li>C. Recall</li> <li>D. F1-Score</li> </ul> Reveal answer <p>D. F1-Score</p>"},{"location":"Machine%20Learning/Exercises/classification_metrics_quiz/#3what-does-precision-measure-in-the-context-of-classification","title":"3.What does precision measure in the context of classification?","text":"<ul> <li>A. The ratio of true positives to the sum of true positives and false positives.</li> <li>B. The ratio of true positives to the sum of true positives and false negatives.</li> <li>C. The ratio of correctly predicted instances to the total instances.</li> <li>D. The ratio of false negatives to the sum of false negatives and true positives.</li> </ul> Reveal answer <p>D. F1-Score </p>"},{"location":"Machine%20Learning/Exercises/classification_metrics_quiz/#4-what-does-recall-or-sensitivity-measure","title":"4. What does recall (or sensitivity) measure?","text":"<ul> <li>A. The ratio of true positives to the sum of true positives and false positives.</li> <li>B. The ratio of true positives to the sum of true positives and false negatives.</li> <li>C. The ratio of true negatives to the sum of true negatives and false positives.</li> <li>D. The ratio of false negatives to the sum of false negatives and true positives.</li> </ul> Reveal answer <p>B. The ratio of true positives to the sum of true positives and false positives. </p>"},{"location":"Machine%20Learning/Exercises/classification_metrics_quiz/#5-the-f1-score-is-the-harmonic-mean-of-which-two-metrics","title":"5. The F1-score is the harmonic mean of which two metrics?","text":"<ul> <li>A. Accuracy and Precision</li> <li>B. Precision and Recall</li> <li>C. Recall and Specificity</li> <li>D. Specificity and Precision</li> </ul> Reveal answer <p>B. Precision and Recall</p>"},{"location":"Machine%20Learning/Exercises/classification_metrics_quiz/#6-in-a-confusion-matrix-what-does-the-bottom-right-cell-represent","title":"6. In a confusion matrix, what does the bottom-right cell represent?**","text":"<ul> <li>A. True Positives (TP)</li> <li>B. True Negatives (TN)</li> <li>C. False Positives (FP)</li> <li>D. False Negatives (FN)</li> </ul> Reveal answer <p>B. True Negatives (TN)</p>"},{"location":"Machine%20Learning/Exercises/classification_metrics_quiz/#7-which-metric-would-you-use-to-measure-the-proportion-of-negative-cases-that-were-correctly-identified","title":"7. Which metric would you use to measure the proportion of negative cases that were correctly identified?","text":"<ul> <li>A. Sensitivity</li> <li>B. Specificity</li> <li>C. Precision</li> <li>D. Accuracy</li> </ul> Reveal answer <p>B. Specificity</p>"},{"location":"Machine%20Learning/Exercises/classification_metrics_quiz/#8-what-is-the-main-drawback-of-using-accuracy-as-the-sole-metric-for-evaluating-a-classification-model","title":"8. What is the main drawback of using accuracy as the sole metric for evaluating a classification model?","text":"<ul> <li>A. It does not consider the balance between positive and negative classes.</li> <li>B. It requires a balanced dataset.</li> <li>C. It only considers the correctly predicted positive cases.</li> <li>D. It only considers the correctly predicted negative cases.</li> </ul> Reveal answer <p>A. It does not consider the balance between positive and negative classes.</p>"},{"location":"Machine%20Learning/Exercises/classification_metrics_quiz/#end","title":"End","text":""},{"location":"Machine%20Learning/Exercises/classification_metrics_quiz/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Machine%20Learning/Exercises/handling_missing_data_quiz/","title":"Missing Data Quiz","text":"<p>title: Handling Missing Data Quiz keywords: [Handling Missing Data Quiz, Handling Missing Data] author: Juma Shafara date: \"2024-03\"</p> <p></p>"},{"location":"Machine%20Learning/Exercises/handling_missing_data_quiz/#questions","title":"Questions:","text":""},{"location":"Machine%20Learning/Exercises/handling_missing_data_quiz/#1-which-of-the-following-is-a-common-method-for-handling-missing-data","title":"1. Which of the following is a common method for handling missing data?","text":"<ul> <li>A) Deleting rows with missing values</li> <li>B) Using the mean to fill missing values</li> <li>C) Using a machine learning algorithm to predict missing values</li> <li>D) All of the above</li> </ul>"},{"location":"Machine%20Learning/Exercises/handling_missing_data_quiz/#2-what-is-the-term-used-for-removing-rows-or-columns-that-contain-missing-data","title":"2. What is the term used for removing rows or columns that contain missing data?","text":"<ul> <li>A) Imputation</li> <li>B) Deletion</li> <li>C) Interpolation</li> <li>D) Normalization</li> </ul>"},{"location":"Machine%20Learning/Exercises/handling_missing_data_quiz/#3-which-imputation-method-replaces-missing-values-with-the-mean-median-or-mode","title":"3. Which imputation method replaces missing values with the mean, median, or mode?","text":"<ul> <li>A) Random sampling imputation</li> <li>B) Regression imputation</li> <li>C) Central tendency imputation</li> <li>D) K-nearest neighbors imputation</li> </ul>"},{"location":"Machine%20Learning/Exercises/handling_missing_data_quiz/#4-what-is-the-potential-drawback-of-deleting-rows-with-missing-data","title":"4. What is the potential drawback of deleting rows with missing data?","text":"<ul> <li>A) It is computationally expensive.</li> <li>B) It can lead to biased results.</li> <li>C) It always improves model accuracy.</li> <li>D) It requires complex algorithms.</li> </ul>"},{"location":"Machine%20Learning/Exercises/handling_missing_data_quiz/#5-which-technique-involves-predicting-missing-values-based-on-other-available-data","title":"5. Which technique involves predicting missing values based on other available data?","text":"<ul> <li>A) Listwise deletion</li> <li>B) Pairwise deletion</li> <li>C) Multiple imputation</li> <li>D) Hot deck imputation</li> </ul>"},{"location":"Machine%20Learning/Exercises/handling_missing_data_quiz/#6-which-of-the-following-is-not-a-method-for-handling-missing-data-in-time-series-analysis","title":"6. Which of the following is NOT a method for handling missing data in time series analysis?","text":"<ul> <li>A) Forward fill</li> <li>B) Backward fill</li> <li>C) Interpolation</li> <li>D) Cross-validation</li> </ul>"},{"location":"Machine%20Learning/Exercises/handling_missing_data_quiz/#7-in-the-context-of-handling-missing-data-what-does-mcar-stand-for","title":"7. In the context of handling missing data, what does 'MCAR' stand for?","text":"<ul> <li>A) Missing Completely at Random</li> <li>B) Missing Conditional on Available Rows</li> <li>C) Missing Characteristic Attribute Reduction</li> <li>D) Missing Completely Available Records</li> </ul>"},{"location":"Machine%20Learning/Exercises/handling_missing_data_quiz/#8-which-method-is-most-suitable-for-handling-missing-data-when-the-data-is-mar-missing-at-random","title":"8. Which method is most suitable for handling missing data when the data is 'MAR' (Missing At Random)?","text":"<ul> <li>A) Listwise deletion</li> <li>B) Multiple imputation</li> <li>C) Mean imputation</li> <li>D) Mode imputation</li> </ul>"},{"location":"Machine%20Learning/Exercises/handling_missing_data_quiz/#9-which-python-library-is-widely-used-for-data-manipulation-and-handling-missing-data","title":"9. Which Python library is widely used for data manipulation and handling missing data?","text":"<ul> <li>A) NumPy</li> <li>B) Pandas</li> <li>C) SciPy</li> <li>D) Matplotlib</li> </ul>"},{"location":"Machine%20Learning/Exercises/handling_missing_data_quiz/#10-which-of-the-following-is-a-disadvantage-of-using-mean-imputation","title":"10. Which of the following is a disadvantage of using mean imputation?","text":"<ul> <li>A) It is computationally intensive.</li> <li>B) It can distort the variance of the data.</li> <li>C) It requires labeled data.</li> <li>D) It is only applicable to categorical data.</li> </ul>"},{"location":"Machine%20Learning/Exercises/handling_missing_data_quiz/#answers","title":"Answers:","text":"<ol> <li>D) All of the above</li> <li>B) Deletion</li> <li>C) Central tendency imputation</li> <li>B) It can lead to biased results.</li> <li>C) Multiple imputation</li> <li>D) Cross-validation</li> <li>A) Missing Completely at Random</li> <li>B) Multiple imputation</li> <li>B) Pandas</li> <li>B) It can distort the variance of the data.</li> </ol>"},{"location":"Machine%20Learning/Exercises/handling_missing_data_quiz/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Machine%20Learning/Exercises/pipelining_quiz/","title":"Pipeline Quiz","text":"<p>title: Machine Learning Pipelining Quiz keywords: [machine learning, machine learning classification, machine learning classification metrics, decision trees, python, precision, recall, f1 score, weighted, accuracy, linear regression] description: Here are some multiple choice and true/false questions on machine learning pipelining author: Juma Shafara date: \"2024-06\"</p> <p></p> <p>Here are some multiple choice and true/false questions on machine learning pipelining:</p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Machine%20Learning/Exercises/pipelining_quiz/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Machine%20Learning/Exercises/pipelining_quiz/#multiple-choice-questions","title":"Multiple Choice Questions","text":"<ol> <li>What is the primary purpose of a machine learning pipeline?</li> <li>A. To visualize data</li> <li>B. To automate the workflow of data processing and model training</li> <li>C. To analyze data manually</li> <li>D. To store data securely</li> </ol> Reveal answer <p>B. To automate the workflow of data processing and model training</p> <ol> <li>Which of the following steps is typically the first in a machine learning pipeline?</li> <li>A. Model evaluation</li> <li>B. Data preprocessing</li> <li>C. Model deployment</li> <li>D. Hyperparameter tuning</li> </ol> Reveal answer <p>B. Data preprocessing</p> <ol> <li>In a scikit-learn pipeline, what does the <code>StandardScaler</code> do?</li> <li>A. Select features</li> <li>B. Scale features to a standard normal distribution</li> <li>C. Reduce the dimensionality of data</li> <li>D. Train the model</li> </ol> Reveal answer <p>B. Scale features to a standard normal distribution</p> <ol> <li>Which of the following is an advantage of using pipelines?</li> <li>A. They make code less readable</li> <li>B. They ensure reproducibility</li> <li>C. They slow down model training</li> <li>D. They increase the risk of data leakage</li> </ol> Reveal answer <p>B. They ensure reproducibility</p> <ol> <li>Which step in a machine learning pipeline is responsible for improving the model by adjusting its parameters?</li> <li>A. Data preprocessing</li> <li>B. Model training</li> <li>C. Hyperparameter tuning</li> <li>D. Model evaluation</li> </ol> Reveal answer <p>C. Hyperparameter tuning</p>"},{"location":"Machine%20Learning/Exercises/pipelining_quiz/#true-or-false-questions","title":"True or False Questions","text":"<ol> <li>Pipelines in scikit-learn can only include pre-built transformers and estimators.</li> </ol> Reveal answer <p>False</p> <ol> <li>Using a pipeline ensures that the same data transformations are applied during both training and testing phases.</li> </ol> Reveal answer <p>True</p> <ol> <li>You can use GridSearchCV with a pipeline to perform hyperparameter tuning on multiple steps simultaneously.</li> </ol> Reveal answer <p>True</p> <ol> <li>The steps in a machine learning pipeline must be specified in a particular order.</li> </ol> Reveal answer <p>True</p> <ol> <li>A machine learning pipeline can be saved to disk using joblib or pickle in Python.</li> </ol> Reveal answer <p>True</p> <ol> <li>Transformers in a pipeline are fit using the training data and then applied to the test data.</li> </ol> Reveal answer <p>True</p> <ol> <li>Model evaluation is typically done before model training in a pipeline.</li> </ol> Reveal answer <p>False</p> <ol> <li>A pipeline helps in avoiding data leakage by ensuring proper separation of training and testing data transformations.</li> </ol> Reveal answer <p>True</p> <ol> <li>Pipelines cannot be used for text data processing.</li> </ol> Reveal answer <p>False</p> <ol> <li>Feature extraction can be included as a step in a machine learning pipeline.</li> </ol> Reveal answer <p>True</p>"},{"location":"Machine%20Learning/Exercises/pipelining_quiz/#end","title":"End","text":""},{"location":"Machine%20Learning/Exercises/pipelining_quiz/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Maths%20%26%20Statistics/01_introduction/","title":"Introduction to Mathematics & Statistics for Data Science","text":""},{"location":"Maths%20%26%20Statistics/01_introduction/#welcome-to-mathematics-statistics-for-data-science","title":"Welcome to Mathematics &amp; Statistics for Data Science","text":"<p>This section provides a comprehensive introduction to the mathematical and statistical foundations essential for data science. Whether you're analyzing data, building machine learning models, or making data-driven decisions, a solid understanding of these concepts is crucial.</p>"},{"location":"Maths%20%26%20Statistics/01_introduction/#learning-objectives","title":"Learning Objectives","text":"<p>By completing this section, you will be able to:</p> <ul> <li>Understand and apply descriptive statistics to summarize and explore data</li> <li>Grasp fundamental probability concepts and their applications</li> <li>Perform inferential statistics to draw conclusions from samples</li> <li>Build and evaluate statistical models</li> <li>Apply linear algebra concepts in data science contexts</li> <li>Use calculus concepts for optimization in machine learning</li> </ul>"},{"location":"Maths%20%26%20Statistics/01_introduction/#course-structure","title":"Course Structure","text":"<p>This course is organized into the following modules, designed to build upon each other:</p> <ol> <li> <p>Descriptive Statistics - Learn to summarize and describe data using measures of central tendency, variability, and distribution shape.</p> </li> <li> <p>Probability Foundations - Understand fundamental probability theory, conditional probability, and Bayes' theorem.</p> </li> <li> <p>Inferential Statistics - Learn to make inferences about populations from samples, including hypothesis testing, confidence intervals, and statistical tests.</p> </li> <li> <p>Statistical Models - Build and evaluate statistical models including linear and logistic regression.</p> </li> <li> <p>Advanced Linear Algebra - Explore eigenvalues, eigenvectors, and their applications in dimensionality reduction.</p> </li> </ol>"},{"location":"Maths%20%26%20Statistics/01_introduction/#prerequisites","title":"Prerequisites","text":"<p>Before starting this section, you should be familiar with:</p> <ul> <li>Basic Python programming (variables, data types, functions)</li> <li>NumPy and Pandas basics</li> <li>Basic data visualization with Matplotlib</li> </ul>"},{"location":"Maths%20%26%20Statistics/01_introduction/#essential-mathematical-concepts","title":"Essential Mathematical Concepts","text":""},{"location":"Maths%20%26%20Statistics/01_introduction/#linear-algebra-basics","title":"Linear Algebra Basics","text":"<p>Linear algebra is fundamental to data science. Here are the key concepts we'll cover:</p> <p>Vectors and Matrices</p> <p>Vectors and matrices are the building blocks of data representation in data science.</p> <pre><code>import numpy as np\n\n# Creating a vector\nvector = np.array([3, 4])\nprint(\"Vector:\", vector)\n\n# Creating a matrix\nmatrix = np.array([[1, 2], [3, 4]])\nprint(\"Matrix:\\n\", matrix)\n</code></pre> <p>Matrix Operations</p> <p>Understanding matrix operations is essential for data manipulation and machine learning algorithms.</p> <pre><code>A = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\n# Element-wise addition\nC = A + B\nprint(\"A + B:\\n\", C)\n\n# Matrix multiplication (dot product)\nF = np.dot(A, B)\nprint(\"Dot Product A @ B:\\n\", F)\n\n# Transpose\nG = A.T\nprint(\"Transpose of A:\\n\", G)\n</code></pre> <p>Applications of Linear Algebra in Data Science: - Data representation and transformation - Principal Component Analysis (PCA) - Machine learning algorithms (neural networks, support vector machines) - Image processing and computer vision</p>"},{"location":"Maths%20%26%20Statistics/01_introduction/#calculus-basics","title":"Calculus Basics","text":"<p>Derivatives</p> <p>Derivatives measure the rate of change of a function, which is crucial for optimization in machine learning.</p> <pre><code>from sympy import symbols, diff\n\n# Define symbol x for differentiation\nx = symbols('x')\nf = x**2 + 3*x + 2\n# Get derivative\nf_derivative = diff(f, x)\nprint(\"Derivative of f(x) = x^2 + 3x + 2 is:\", f_derivative)\n</code></pre> <p>Gradients</p> <p>The gradient represents the direction and rate of steepest increase of a function, essential for optimization algorithms like gradient descent.</p> <p>Applications of Calculus in Data Science: - Optimization algorithms (gradient descent) - Neural network training (backpropagation) - Finding optimal model parameters - Cost function minimization</p>"},{"location":"Maths%20%26%20Statistics/01_introduction/#probability-distributions-overview","title":"Probability Distributions Overview","text":"<p>Probability distributions describe how values are distributed. Understanding common distributions is essential for statistical analysis.</p> <p>Common Distributions:</p> <ol> <li>Uniform Distribution - All values within a range are equally likely</li> <li> <p>Examples: Rolling a fair die, flipping a fair coin</p> </li> <li> <p>Normal Distribution - Symmetric, bell-shaped distribution</p> </li> <li> <p>Examples: Heights of people, IQ scores, measurement errors</p> </li> <li> <p>Binomial Distribution - Models the number of successes in n trials</p> </li> <li>Examples: Number of heads in coin flips, number of defective items in a batch</li> </ol> <p>We'll explore these distributions in detail in the Probability Foundations module.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, binom\n\n# Normal distribution example\nmu, sigma = 60, 10\nages = np.random.normal(mu, sigma, 1000)\n\nplt.hist(ages, bins=30, density=True, alpha=0.6, color='g', label='Histogram')\nx = np.linspace(ages.min(), ages.max(), 100)\ny = norm.pdf(x, mu, sigma)\nplt.plot(x, y, 'r-', label='Normal Distribution')\nplt.title('Normal Distribution Example')\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"Maths%20%26%20Statistics/01_introduction/#expectation-and-variance","title":"Expectation and Variance","text":"<p>Expectation (Mean): The expected value of a random variable, representing the long-run average.</p> <p>Variance: Measures the spread or dispersion of data around the mean.</p> <pre><code># Example with a discrete random variable\nvalues = np.array([1, 2, 3, 4, 5])\nprobs = np.array([0.1, 0.2, 0.3, 0.2, 0.2])  # Probabilities sum to 1\nexpectation = np.sum(values * probs)\nvariance = np.sum((values**2) * probs) - expectation**2\nprint(f\"Expectation (E[X]): {expectation}\")\nprint(f\"Variance (Var[X]): {variance}\")\n</code></pre> <p>Applications: - Portfolio management and risk assessment - Machine learning model performance evaluation - Hypothesis testing and statistical inference</p>"},{"location":"Maths%20%26%20Statistics/01_introduction/#how-to-use-this-section","title":"How to Use This Section","text":"<ol> <li> <p>Follow the sequence: Work through the modules in order, as each builds on previous concepts.</p> </li> <li> <p>Practice actively: Run all code examples and try modifying them to deepen your understanding.</p> </li> <li> <p>Connect concepts: Pay attention to how different mathematical concepts connect to data science applications.</p> </li> <li> <p>Apply to real data: Use the techniques you learn on real datasets to reinforce your understanding.</p> </li> </ol>"},{"location":"Maths%20%26%20Statistics/01_introduction/#next-steps","title":"Next StepsWhat's on your mind? Put it in the comments!","text":"<p>Ready to begin? Start with Descriptive Statistics to learn how to summarize and explore your data.</p>"},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/","title":"Descriptive Statistics","text":""},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/#introduction","title":"Introduction","text":"<p>Descriptive statistics is a branch of statistics that deals with the presentation and summary of data in a meaningful and informative way. This module provides the foundation for understanding your data before moving to more advanced statistical analysis.</p>"},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ul> <li>Calculate and interpret measures of central tendency (mean, median, mode)</li> <li>Understand and apply measures of variability (variance, standard deviation, range, IQR)</li> <li>Describe distribution shape using skewness and kurtosis</li> <li>Measure associations between variables using correlation and chi-square tests</li> <li>Use Python to compute and visualize descriptive statistics</li> </ul>"},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/#prerequisites","title":"Prerequisites","text":"<p>Before starting this module, you should be familiar with: - Basic Python programming - NumPy and Pandas basics - Basic data visualization with Matplotlib - Introduction to Mathematics &amp; Statistics</p>"},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/#next-steps","title":"Next Steps","text":"<p>After completing this module, you'll be ready to learn about Probability Foundations and Inferential Statistics.</p> <pre><code>#| hide\n#| default_exp statistics\n</code></pre> <pre><code>## Uncomment and run this cell to install the packages\n# !pip install --upgrade dataidea\n</code></pre>"},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/#descriptive-statistics-and-summary-metrics","title":"Descriptive Statistics and Summary Metrics","text":"<p>In this notebook, we will learn to obtain important values that describe our data including:</p> <ul> <li> Measures of central tendency</li> <li> Measures of variability</li> <li> Measures of distribution shape</li> <li> Measures of association</li> </ul> <pre><code>import pandas as pd\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\n</code></pre> <p>This notebook has been modified to use the Nobel Price Laureates Dataset which you can download from opendatasoft</p> <pre><code># load the dataset (modify the path to point to your copy of the dataset)\ndata = pd.read_csv('../assets/nobel_prize_year.csv')\ndata = data[data.Gender != 'org'] # removing organizations\ndata.sample(n=5)\n</code></pre> Year Gender Category birth_year age 505 2014 male Physics 1929 85 318 1952 male Literature 1885 67 883 1933 male Literature 1870 63 481 1995 male Peace 1908 87 769 2005 male Peace 1942 63  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/#what-is-descriptive-statistics","title":"What is Descriptive Statistics","text":"<p>Descriptive statistics is a branch of statistics that deals with the presentation and summary of data in a meaningful and informative way. Its primary goal is to describe and summarize the main features of a dataset. </p> <p>Commonly used measures in descriptive statistics include:</p> <ol> <li> <p>Measures of central tendency: These describe the center or average of a dataset and include metrics like mean, median, and mode.</p> </li> <li> <p>Measures of variability: These indicate the spread or dispersion of the data and include metrics like range, variance, and standard deviation.</p> </li> <li> <p>Measures of distribution shape: These describe the distribution of data points and include metrics like skewness and kurtosis.</p> </li> <li> <p>Measures of association: These quantify the relationship between variables and include correlation coefficients.</p> </li> </ol> <p>Descriptive statistics provide simple summaries about the sample and the observations that have been made.</p>"},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/#1-measures-of-central-tendency-ie-mean-median-mode","title":"1. Measures of central tendency ie Mean, Median, Mode:","text":"<p>The Center of the Data:</p> <p>The center of the data is where most of the values are concentrated.</p> <p> Mean: It is the average value of a dataset calculated by summing all values(numerical) and dividing by the total count.</p> <pre><code>mean_value = np.mean(data.age)\nprint(\"Mean:\", mean_value)\n</code></pre> <pre><code>Mean: 60.21383647798742\n</code></pre> <p> Median: It is the middle value of a dataset when arranged in ascending order. If there is an even number of observations, the median is the average of the two middle values.</p> <pre><code>median_value = np.median(data.age)\nprint(\"Median:\", median_value)\n</code></pre> <pre><code>Median: 60.0\n</code></pre> <p> Mode: It is the value that appears most frequently in a dataset.</p> <pre><code>mode_value = sp.stats.mode(data.age)[0]\nprint(\"Mode:\", mode_value)\n</code></pre> <pre><code>Mode: 56\n</code></pre> <p>Homework:</p> <p> Other ways to find mode (ie using pandas and numpy)</p>"},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/#2-measures-of-variability","title":"2. Measures of variability","text":"<p>The Variation of the Data:</p> <p>The variation of the data is how spread out the data are around the center.</p> <p>a) Variance and Standard Deviation:</p> <p> Variance: It measures the spread of the data points around the mean.</p> <pre><code># how to implement the variance and standard deviation using numpy\nvariance_value = np.var(data.age)\nprint(\"Variance:\", variance_value)\n</code></pre> <pre><code>Variance: 159.28551085795658\n</code></pre> <p> Standard Deviation: It is the square root of the variance, providing a measure of the average distance between each data point and the mean.</p> <pre><code>std_deviation_value = np.std(data.age)\nprint(\"Standard Deviation:\", std_deviation_value)\n</code></pre> <pre><code>Standard Deviation: 12.620836377116873\n</code></pre>  Summary<p>In summary, variance provides a measure of dispersion in squared units, while standard deviation provides a measure of dispersion in the original units of the data</p>  Note!<p>Smaller variances and standard deviation values mean that the data has values similar to each other and closer to the mean and the vice versa is true</p> <pre><code>plt.hist(x=data.age, bins=20, edgecolor='black')\n# add standard deviation lines\nplt.axvline(mean_value, color='red', linestyle='--', label='Mean')\nplt.axvline(mean_value+std_deviation_value, color='orange', linestyle='--', label='1st std Dev')\nplt.axvline(mean_value-std_deviation_value, color='orange', linestyle='--')\nplt.title('Age of Nobel Prize Winners')\nplt.ylabel('Frequency')\nplt.xlabel('Age')\n# Adjust the position of the legend\nplt.legend(loc='upper left')\n\nplt.show()\n</code></pre> <p></p> <p>b) Range and Interquartile Range (IQR):</p> <p> Range: It is the difference between the maximum and minimum values in a dataset. It is simplest measure of variation</p> <pre><code># One way to obtain range\nmin_age = min(data.age)\nmax_age = max(data.age)\nage_range = max_age - min_age\nprint('Range:', age_range)\n</code></pre> <pre><code>Range: 80\n</code></pre> <pre><code># Calculating the range using numpy\nrange_value = np.ptp(data.age)\nprint(\"Range:\", range_value)\n</code></pre> <pre><code>Range: 80\n</code></pre> <p> Interquartile Range (IQR): It is the range between the first quartile (25th percentile) and the third quartile (75th percentile) of the dataset.</p> <p>Quartiles:</p> <p>Calculating Quartiles</p> <p>The quartiles (Q0,Q1,Q2,Q3,Q4) are the values that separate each quarter.</p> <p>Between Q0 and Q1 are the 25% lowest values in the data. Between Q1 and Q2 are the next 25%. And so on.</p> <ul> <li> Q0 is the smallest value in the data.</li> <li> Q1 is the value separating the first quarter from the second quarter of the data.</li> <li> Q2 is the middle value (median), separating the bottom from the top half.</li> <li> Q3 is the value separating the third quarter from the fourth quarter</li> <li> Q4 is the largest value in the data.</li> </ul> <pre><code># Calculate the quartile\nquartiles = np.quantile(a=data.age, q=[0, 0.25, 0.5, 0.75, 1])\n\nprint('Quartiles:', quartiles)\n</code></pre> <pre><code>Quartiles: [17. 51. 60. 69. 97.]\n</code></pre> <p>Percentiles:</p> <p>Percentiles are values that separate the data into 100 equal parts.</p> <p>For example, The 95th percentile separates the lowest 95% of the values from the top 5%</p> <ul> <li> The 25th percentile (P25%) is the same as the first quartile (Q1).</li> <li> The 50th percentile (P50%) is the same as the second quartile (Q2) and the median.</li> <li> The 75th percentile (P75%) is the same as the third quartile (Q3)</li> </ul> <p>Calculating Percentiles with Python</p> <p>To get all the percentile values, we can use <code>np.percentile()</code> method and pass in the data, and the list of the percentiles as showed below.</p> <pre><code># Getting many percentiles\npercentiles = np.percentile(data.age, [25, 50, 75])\nprint(f'Percentiles: {percentiles}')\n</code></pre> <pre><code>Percentiles: [51. 60. 69.]\n</code></pre> <p>To get a single percentile value, we can again use the <code>np.percentile()</code> method and pass in the data, and a the specicific percentile you're interested in eg:</p> <pre><code># Getting one percentile at a time\nfirst_quartile = np.percentile(a=data.age, q=25) # 25th percentile\nmiddle_percentile = np.percentile(data.age, 50)\nthird_quartile = np.percentile(data.age, 75) # 75th percentile\n\nprint('Q1: ', first_quartile)\nprint('Q2: ', middle_percentile)  \nprint('Q3: ', third_quartile)\n</code></pre> <pre><code>Q1:  51.0\nQ2:  60.0\nQ3:  69.0\n</code></pre> Note!<p>Note also that we can be able to use the `np.quantile()` method  to calculate the percentiles which makes logical sense as all the values mark a fraction(percentage) of the data</p> <pre><code>percentiles = np.quantile(a=data.age, q=[0.25, 0.50, 0.75])\nprint('Percentiles:', percentiles)\n</code></pre> <pre><code>Percentiles: [51. 60. 69.]\n</code></pre> <p>Now we can be able to obtain the interquartile range as the difference between the third and first quartiles as predefined.</p> <pre><code># obtain the interquartile\niqr_value = third_quartile - first_quartile\nprint('Interquartile range: ', iqr_value)\n</code></pre> <pre><code>Interquartile range:  18.0\n</code></pre> <p>Note: Quartiles and percentiles are both types of quantiles</p>  Summary<p>While the range gives an overview of the entire spread of the data from lowest to highest, the interquartile range focuses s`pecifically on the spread of the middle portion of the data, making it more robust against outliers.</p>"},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/#3-measures-of-distribution-shape-ie-skewness-and-kurtosis","title":"3. Measures of distribution shape ie Skewness and Kurtosis:","text":"<p>The shape of the Data:</p> <p>The shape of the data refers to how the data are bounded on either side of the center.</p> <p> Skewness: It measures the asymmetry of the distribution.</p> <pre><code># let's get skew from scipy\nskewness_value = sp.stats.skew(data.age)\nprint(\"Skewness:\", skewness_value)\n</code></pre> <pre><code>Skewness: -0.028324578326524283\n</code></pre> <p>How to interpret Skewness:</p> <p> Positive skewness (&gt; 0) indicates that the tail on the right side of the distribution is longer than the left side (right skewed). </p> <p> Negative skewness (&lt; 0) indicates that the tail on the left side of the distribution is longer than the right side (left skewed). </p> <pre><code># Plot the histogram\n# Set density=True for normalized histogram\nplt.hist(x=data.age, bins=20, density=True, edgecolor='black')  \n\n# Create a normal distribution curve\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = sp.stats.norm.pdf(x, mean_value, std_deviation_value)\nplt.plot(x, p, 'k', linewidth=2)  \n# 'k' indicates black color, you can change it to any color\n\n# Labels and legend\nplt.xlabel('Age')\nplt.ylabel('Probability Density')\nplt.title('Histogram with Normal Distribution Curve')\nplt.legend(['Normal Distribution', 'Histogram'])\n\nplt.show()\n</code></pre> <p></p> <p> Kurtosis: It measures the peakedness or flatness of the distribution.</p> <pre><code># let's get kurtosis from scipy\nkurtosis_value = sp.stats.kurtosis(data.age)\nprint(\"Kurtosis:\", kurtosis_value)\n</code></pre> <pre><code>Kurtosis: -0.3811155702676823\n</code></pre> <p>How to interpret Kurtosis:</p> <p> A kurtosis of 3 indicates the normal distribution (mesokurtic), also known as Gaussian distribution.</p> <p> Positive kurtosis (&gt; 3) indicates a distribution with heavier tails and a sharper peak than the normal distribution. This is called leptokurtic.</p> <p> Negative kurtosis (&lt; 3) indicates a distribution with lighter tails and a flatter peak than the normal distribution. This is called platykurtic.</p>  Note!<p>In simple terms, skewness tells you if your data is leaning more to one side or the other, while kurtosis tells you if your data has heavy or light tails and how sharply it peaks.</p>"},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/#4-measures-of-association","title":"4. Measures of association","text":"<p>a). Correlation</p> <p> Correlation measures the relationship between two numerical variables.</p> <p>Correlation Matrix</p> <p> A correlation matrix is simply a table showing the correlation coefficients between variables</p> <p>Correlation Matrix in Python</p> <p>We can use the <code>corrcoef()</code> function in Python to create a correlation matrix.</p> <pre><code># Generate example data\nx = np.array([1, 1, 3, 5, 15])\ny = np.array([2, 4, 6, 8, 10])\n\ncorrelation_matrix = np.corrcoef(x, y)\n\ncorrelation_matrix_df = pd.DataFrame(\n    correlation_matrix, \n    columns=['x', 'y'], \n    index=['x', 'y']\n    )\ncorrelation_matrix_df\n</code></pre> x y x 1.000000 0.867722 y 0.867722 1.000000 <p>Correlation Coefficient:</p> <p> The correlation coefficient measures the strength and direction of the linear relationship between two continuous variables.</p> <p> t ranges from -1 to 1, where:</p> <ul> <li> 1 indicates a perfect positive linear relationship, eg complementary good bread and blueband, battery and torch, fuel and car</li> <li> -1 indicates a perfect negative linear relationship, eg substitute goods like tea and coffee</li> <li> 0 indicates no linear relationship, eg phones and socks, house and mouse</li> </ul> <pre><code># Calculate correlation coefficient\ncorrelation = np.corrcoef(x, y)[0, 1]\nprint(\"Correlation Coefficient:\", correlation)\n</code></pre> <pre><code>Correlation Coefficient: 0.8677218312746245\n</code></pre> <p>Correlation vs Causality:</p> <p>Correlation measures the numerical relationship between two varaibles</p> <p>A high correlation coefficient (close to 1), does not mean that we can for sure conclude an actual relationship between two variables.</p> <p>A classic example:</p> <ul> <li> During the summer, the sale of ice cream at a beach increases</li> <li> Simultaneously, drowning accidents also increase as well</li> </ul> <p>Does this mean that increase of ice cream sale is a direct cause of increased drowning accidents?</p> <p>Measures of Association for Categorical Variables</p> <p>b) Contingency Tables and Chi-square Test for Independence:</p> <p> Contingency tables are used to summarize the relationship between two categorical variables by counting the frequency of observations for each combination of categories.</p> <p> Chi-square test for independence determines whether there is a statistically significant association between the two categorical variables.</p> <pre><code>demo_data = data[['Gender', 'Category']]\n\n# We drop all the missing values just for demonstration purposes\ndemo_data = demo_data.dropna()\n</code></pre> <p>Obtain the cross tabulation of Gender and Category. The cross tabulation is also known as the contingency table</p> <pre><code># cross tab\ngender_category_tab = pd.crosstab(\n    demo_data.Gender, \n    demo_data.Category\n    )\n\n# Let's have a look at the outcome\ngender_category_tab\n</code></pre> Category Chemistry Economics Literature Medicine Peace Physics Gender female 8 2 17 13 18 5 male 181 87 102 212 90 219"},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/#test-of-independence","title":"Test of Independence:","text":"<p>This test is used to determine whether there is a significant association between two categorical variables.</p> <p>Formula:    \\(\\(\u03c7\u00b2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\)\\)    where:    - \\(O_{ij}\\) = Observed frequency for each cell in the contingency table    - \\(E_{ij}\\) = Expected frequency for each cell under the assumption of independence</p> <pre><code>chi2_stat, p_value, dof, expected = sp.stats.chi2_contingency(gender_category_tab)\n\nprint('Chi-square Statistic:', chi2_stat)\nprint('p-value:', p_value)\nprint('Degrees of freedom (dof):', dof)\n# print('Expected:', expected)\n</code></pre> <pre><code>Chi-square Statistic: 40.7686907732235\np-value: 1.044840181761602e-07\nDegrees of freedom (dof): 5\n</code></pre> <p>Interpretation of Chi2 Test Results:</p> <ul> <li> The Chi-square statistic measures the difference between the observed frequencies in the contingency table and the frequencies that would be expected if the variables were independent.</li> <li> The p-value is the probability of obtaining a Chi-square statistic as extreme as, or more extreme than, the one observed in the sample, assuming that the null hypothesis is true (i.e., assuming that there is no association between the variables).</li> <li> A low p-value indicates strong evidence against the null hypothesis, suggesting that there is a significant association between the variables.</li> <li> A high p-value indicates weak evidence against the null hypothesis, suggesting that there is no significant association between the variables.</li> </ul> <p>c. Measures of Association for Categorical Variables:</p> <ul> <li> Measures like Cramer's V or phi coefficient quantify the strength of association between two categorical variables.</li> <li> These measures are based on chi-square statistics and the dimensions of the contingency table.</li> </ul>"},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/#the-formula-for-cramers-v-is","title":"The formula for Cramer's V is:","text":"\\[V = \\sqrt{\\frac{\u03c7\u00b2}{n(k - 1)}}\\] <p>Where: - \\(\u03c7\u00b2\\) is the chi-square statistic from the chi-square test of independence. - \\(n\\) is the total number of observations in the contingency table. - \\(k\\) is the minimum of the number of rows and the number of columns in the contingency table.</p> <p>Cramer's V is a normalized measure of association, making it easier to interpret compared to the raw chi-square statistic. A larger value of Cramer's V indicates a stronger association between the variables.</p> <pre><code>#| hide\n#| export\n\nimport numpy as np\n\ndef cramersV(contingency_table):\n\n    chi2_statistic = sp.stats.chi2_contingency(contingency_table)[0]\n    total_observations = contingency_table.sum().sum()\n    phi2 = chi2_statistic / total_observations\n    rows, columns = contingency_table.shape\n\n    return np.sqrt(phi2/ min(rows-1, columns-1))\n</code></pre> <pre><code>from dataidea.statistics import cramersV\n</code></pre> <pre><code>cramersV(contingency_table=gender_category_tab)\n</code></pre> <pre><code>0.20672318859163366\n</code></pre> <pre><code>#| hide\n#| export\n\nimport pandas as pd\nimport scipy as sp\n\ndef cramersVCorrected(contingency_table):\n\n    chi2_statistic = sp.stats.chi2_contingency(contingency_table)[0]\n    total_observations = contingency_table.sum().sum()\n    phi2 = chi2_statistic / total_observations\n    rows, columns = contingency_table.shape\n    phi2_corrected = max(0, phi2 - ((columns-1)*(rows-1))/(total_observations-1))\n    rows_corrected = rows - ((rows-1)**2)/(total_observations-1)\n    columns_corrected = columns - ((columns-1)**2)/(total_observations-1)\n\n    return np.sqrt(phi2_corrected / min((columns_corrected-1), (rows_corrected-1)))\n</code></pre> <pre><code>from dataidea.statistics import cramersVCorrected\n</code></pre> <pre><code>cramersVCorrected(gender_category_tab)\n</code></pre> <pre><code>0.19371955249110775\n</code></pre> <p>Cramer's V is measure of association between two categorical variables. It ranges from 0 to 1 where: </p> <ul> <li> 0 indicates no association between the variables</li> <li> 1 indicates a perfect association between the variables</li> </ul> <p>Here's an interpretation of the Cramer's V:</p> <ul> <li> Small effect: Around 0.1</li> <li> Medium effect: Around 0.3</li> <li> Large effect: Around 0.5 or greater</li> </ul>"},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/#frequency-tables","title":"Frequency Tables","text":"<p>Frequency means the number of times a value appears in the data. A table can quickly show us how many times each value appears. If the data has many different values, it is easier to use intervals of values to present them in a table.</p> <p>Here's the age of the 934 Nobel Prize winners up until the year 2020. IN the table, each row is an age interval of 10 years</p> Age Interval Frequency 10-19 1 20-29 2 30-39 48 40-49 158 50-59 236 60-69 262 70-79 174 80-89 50 90-99 3 <p>Note: The intervals for the values are also called bin</p>"},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Measures of central tendency (mean, median, mode) describe where data is centered.</li> <li>Measures of variability (variance, standard deviation, range, IQR) describe how spread out data is.</li> <li>Skewness indicates asymmetry; kurtosis indicates tail heaviness.</li> <li>Correlation measures linear relationships between continuous variables.</li> <li>Chi-square tests assess associations between categorical variables.</li> <li>Always visualize your data alongside numerical summaries for better understanding.</li> </ol>"},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/#applications-in-data-science","title":"Applications in Data Science","text":"<ul> <li>Exploratory Data Analysis (EDA): First step in any data science project</li> <li>Data quality assessment: Identify outliers, missing patterns, data quality issues</li> <li>Feature engineering: Understand relationships to create better features</li> <li>Model assumptions: Check if data meets assumptions for statistical models</li> <li>Communication: Summarize findings for stakeholders</li> </ul>"},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/#next-steps_1","title":"Next Steps","text":"<p>Now that you understand descriptive statistics, you're ready to learn about: - Probability Foundations - Understanding uncertainty and probability - Inferential Statistics - Making inferences about populations</p>"},{"location":"Maths%20%26%20Statistics/02_descriptive_statistics/#further-reading","title":"Further Reading","text":"<p>Chapter 3 of An Introduction to Statistical Methods and Data Analysis 7th Edition_New</p> <pre><code>#| hide\nimport nbdev; nbdev.nbdev_export()\n</code></pre> What's on your mind? Put it in the comments!"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/","title":"Probability Foundations","text":""},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#introduction","title":"Introduction","text":"<p>Probability is the foundation of statistics and data science. It provides the mathematical framework for quantifying uncertainty and making predictions based on data. This module covers essential probability concepts that you'll use throughout your data science journey.</p>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ul> <li>Understand and work with sample spaces and events</li> <li>Calculate probabilities using basic probability rules</li> <li>Apply conditional probability to solve real-world problems</li> <li>Use Bayes' theorem for probabilistic reasoning</li> <li>Identify and work with independent events</li> <li>Apply probability concepts using Python</li> </ul>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#prerequisites","title":"Prerequisites","text":"<p>Before starting this module, you should be familiar with: - Basic Python programming - Set theory basics (union, intersection) - Descriptive Statistics concepts</p>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#1-sample-spaces-and-events","title":"1. Sample Spaces and Events","text":""},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#sample-space","title":"Sample Space","text":"<p>A sample space (denoted by S or \u03a9) is the set of all possible outcomes of a random experiment.</p> <p>Example: When flipping a coin, the sample space is S = {Heads, Tails}</p> <pre><code># Sample space for a coin flip\nsample_space_coin = {'Heads', 'Tails'}\nprint(\"Sample space for coin flip:\", sample_space_coin)\n\n# Sample space for rolling a die\nsample_space_die = {1, 2, 3, 4, 5, 6}\nprint(\"Sample space for die roll:\", sample_space_die)\n</code></pre>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#events","title":"Events","text":"<p>An event is a subset of the sample space. It represents a collection of outcomes we're interested in.</p> <p>Example: When rolling a die, the event \"rolling an even number\" is E = {2, 4, 6}</p> <pre><code># Event: rolling an even number\nsample_space = {1, 2, 3, 4, 5, 6}\neven_numbers = {2, 4, 6}\nprint(\"Event: Even numbers =\", even_numbers)\n\n# Event: rolling a number greater than 4\ngreater_than_4 = {5, 6}\nprint(\"Event: Numbers &gt; 4 =\", greater_than_4)\n</code></pre>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#2-basic-probability-rules","title":"2. Basic Probability Rules","text":""},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#probability-of-an-event","title":"Probability of an Event","text":"<p>The probability of an event E, denoted P(E), is a number between 0 and 1 that represents the likelihood of that event occurring.</p> <p>Key Properties: - 0 \u2264 P(E) \u2264 1 - P(S) = 1 (probability of sample space is always 1) - P(\u2205) = 0 (probability of impossible event is 0)</p>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#classical-probability","title":"Classical Probability","text":"<p>For equally likely outcomes, the probability of an event E is:</p> \\[P(E) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total number of possible outcomes}}\\] <pre><code># Probability of rolling an even number on a fair die\nsample_space = {1, 2, 3, 4, 5, 6}\neven_numbers = {2, 4, 6}\n\n# Classical probability\nprob_even = len(even_numbers) / len(sample_space)\nprint(f\"P(Even number) = {prob_even}\")\n</code></pre>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#complement-rule","title":"Complement Rule","text":"<p>The probability of the complement of event E (not E) is:</p> \\[P(E^c) = 1 - P(E)\\] <pre><code># Probability of NOT rolling an even number\nprob_not_even = 1 - prob_even\nprint(f\"P(Not even) = {prob_not_even}\")\n</code></pre>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#addition-rule","title":"Addition Rule","text":"<p>For two events A and B:</p> \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\] <p>If A and B are mutually exclusive (cannot occur together):</p> \\[P(A \\cup B) = P(A) + P(B)\\] <pre><code># Example: Probability of rolling an even number OR a number greater than 4\nsample_space = {1, 2, 3, 4, 5, 6}\neven = {2, 4, 6}\ngreater_than_4 = {5, 6}\n\n# Intersection (numbers that are both even AND &gt; 4)\nintersection = even &amp; greater_than_4  # {6}\nprint(f\"Intersection: {intersection}\")\n\n# Union (numbers that are even OR &gt; 4)\nunion = even | greater_than_4  # {2, 4, 5, 6}\nprint(f\"Union: {union}\")\n\n# Using addition rule\nprob_even = len(even) / len(sample_space)\nprob_gt4 = len(greater_than_4) / len(sample_space)\nprob_intersection = len(intersection) / len(sample_space)\n\nprob_union = prob_even + prob_gt4 - prob_intersection\nprint(f\"P(Even OR &gt; 4) = {prob_union}\")\n\n# Verify by direct calculation\nprob_union_direct = len(union) / len(sample_space)\nprint(f\"Direct calculation: {prob_union_direct}\")\n</code></pre>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#3-conditional-probability","title":"3. Conditional Probability","text":"<p>Conditional probability is the probability of an event occurring given that another event has already occurred.</p> <p>The conditional probability of A given B is:</p> \\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\] <p>where P(B) &gt; 0.</p> <pre><code>import numpy as np\n\n# Example: Probability of drawing a heart given that we drew a red card\n# In a standard deck: 52 cards, 26 red (13 hearts + 13 diamonds), 13 hearts\n\n# P(Heart | Red) = P(Heart \u2229 Red) / P(Red)\n# Since all hearts are red: P(Heart \u2229 Red) = P(Heart) = 13/52\n# P(Red) = 26/52\n\nprob_heart_given_red = (13/52) / (26/52)\nprint(f\"P(Heart | Red) = {prob_heart_given_red}\")\n\n# Example with data\n# Simulating card draws\nnp.random.seed(42)\ndeck = ['H'] * 13 + ['D'] * 13 + ['C'] * 13 + ['S'] * 13  # Hearts, Diamonds, Clubs, Spades\nred_cards = ['H'] * 13 + ['D'] * 13\n\n# Simulate drawing cards\nn_simulations = 10000\nred_draws = 0\nheart_given_red = 0\n\nfor _ in range(n_simulations):\n    card = np.random.choice(deck)\n    if card in red_cards:\n        red_draws += 1\n        if card == 'H':\n            heart_given_red += 1\n\n# Empirical conditional probability\nif red_draws &gt; 0:\n    emp_prob = heart_given_red / red_draws\n    print(f\"Empirical P(Heart | Red) \u2248 {emp_prob:.3f}\")\n</code></pre>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#real-world-example-medical-testing","title":"Real-World Example: Medical Testing","text":"<pre><code># Example: Disease testing\n# Suppose 1% of population has a disease\n# Test is 99% accurate (99% true positive, 99% true negative)\n\n# P(Disease) = 0.01\n# P(Test Positive | Disease) = 0.99\n# P(Test Negative | No Disease) = 0.99\n\nprob_disease = 0.01\nprob_positive_given_disease = 0.99\nprob_negative_given_no_disease = 0.99\n\n# P(Test Positive | No Disease) = 1 - P(Test Negative | No Disease)\nprob_positive_given_no_disease = 1 - prob_negative_given_no_disease\n\n# P(Test Positive) using law of total probability\nprob_positive = (prob_positive_given_disease * prob_disease + \n                prob_positive_given_no_disease * (1 - prob_disease))\n\nprint(f\"P(Test Positive) = {prob_positive:.4f}\")\n</code></pre>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#4-bayes-theorem","title":"4. Bayes' Theorem","text":"<p>Bayes' theorem allows us to update probabilities based on new information. It's fundamental to many machine learning algorithms and statistical inference.</p> \\[P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\\] <p>Where: - P(A|B) is the posterior probability - P(B|A) is the likelihood - P(A) is the prior probability - P(B) is the marginal probability</p> <pre><code># Continuing the medical testing example\n# What is P(Disease | Test Positive)?\n\n# Using Bayes' theorem\n# P(Disease | Test Positive) = P(Test Positive | Disease) * P(Disease) / P(Test Positive)\n\nprob_disease_given_positive = (prob_positive_given_disease * prob_disease) / prob_positive\n\nprint(f\"P(Disease | Test Positive) = {prob_disease_given_positive:.4f}\")\nprint(f\"\\nEven with a 99% accurate test, if you test positive,\")\nprint(f\"there's only a {prob_disease_given_positive*100:.1f}% chance you actually have the disease!\")\nprint(f\"This is because the disease is rare (1% prevalence).\")\n</code></pre>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#law-of-total-probability","title":"Law of Total Probability","text":"<p>Before using Bayes' theorem, we often need to calculate P(B) using the law of total probability:</p> \\[P(B) = P(B|A_1) \\cdot P(A_1) + P(B|A_2) \\cdot P(A_2) + ... + P(B|A_n) \\cdot P(A_n)\\] <p>where A\u2081, A\u2082, ..., A\u2099 form a partition of the sample space.</p> <pre><code># Example: Probability of drawing different colored balls from urns\n# Urn 1: 3 red, 2 blue\n# Urn 2: 1 red, 4 blue\n# We randomly select an urn (50% chance each), then draw a ball\n\n# P(Red) = P(Red | Urn1) * P(Urn1) + P(Red | Urn2) * P(Urn2)\nprob_urn1 = 0.5\nprob_urn2 = 0.5\n\nprob_red_given_urn1 = 3/5  # 3 red out of 5 total\nprob_red_given_urn2 = 1/5  # 1 red out of 5 total\n\nprob_red = prob_red_given_urn1 * prob_urn1 + prob_red_given_urn2 * prob_urn2\nprint(f\"P(Red) = {prob_red}\")\n\n# Now, if we draw a red ball, what's the probability it came from Urn 1?\n# P(Urn1 | Red) = P(Red | Urn1) * P(Urn1) / P(Red)\nprob_urn1_given_red = (prob_red_given_urn1 * prob_urn1) / prob_red\nprint(f\"P(Urn1 | Red) = {prob_urn1_given_red:.3f}\")\n</code></pre>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#5-independence","title":"5. Independence","text":"<p>Two events A and B are independent if the occurrence of one does not affect the probability of the other:</p> \\[P(A \\cap B) = P(A) \\cdot P(B)\\] <p>Equivalently:</p> \\[P(A|B) = P(A)\\] <pre><code># Example: Independent events\n# Flipping a coin and rolling a die are independent\n\nprob_heads = 0.5\nprob_six = 1/6\n\n# P(Heads AND Six) = P(Heads) * P(Six) (if independent)\nprob_both = prob_heads * prob_six\nprint(f\"P(Heads AND Six) = {prob_both}\")\n\n# Example: Dependent events\n# Drawing two cards from a deck without replacement\n# P(Second card is Ace | First card is Ace) \u2260 P(Second card is Ace)\n\n# First draw\nprob_first_ace = 4/52  # 4 aces in 52 cards\n\n# If first card is an ace, 3 aces remain in 51 cards\nprob_second_ace_given_first = 3/51\n\n# If first card is not an ace, 4 aces remain in 51 cards\nprob_second_ace_given_not_first = 4/51\n\nprint(f\"\\nDependent events:\")\nprint(f\"P(Second Ace | First Ace) = {prob_second_ace_given_first:.4f}\")\nprint(f\"P(Second Ace | First Not Ace) = {prob_second_ace_given_not_first:.4f}\")\nprint(f\"These are different, so the events are dependent!\")\n</code></pre>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#6-common-probability-distributions","title":"6. Common Probability Distributions","text":""},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#discrete-distributions","title":"Discrete Distributions","text":"<p>Uniform Distribution (Discrete) - All outcomes are equally likely - Example: Rolling a fair die</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n# Uniform distribution for a fair die\noutcomes = [1, 2, 3, 4, 5, 6]\nprobabilities = [1/6] * 6\n\nplt.bar(outcomes, probabilities, color='skyblue', edgecolor='black')\nplt.title('Uniform Distribution: Fair Die')\nplt.xlabel('Outcome')\nplt.ylabel('Probability')\nplt.ylim(0, 0.2)\nplt.show()\n</code></pre> <p>Binomial Distribution - Models the number of successes in n independent trials - Parameters: n (number of trials), p (probability of success)</p> <pre><code>from scipy.stats import binom\n\n# Binomial: Number of heads in 10 coin flips\nn, p = 10, 0.5\nx = np.arange(0, n + 1)\ny = binom.pmf(x, n, p)\n\nplt.bar(x, y, color='lightgreen', edgecolor='black', alpha=0.7)\nplt.title('Binomial Distribution: 10 Coin Flips')\nplt.xlabel('Number of Heads')\nplt.ylabel('Probability')\nplt.show()\n</code></pre>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#continuous-distributions","title":"Continuous Distributions","text":"<p>Normal Distribution - Bell-shaped, symmetric distribution - Parameters: \u03bc (mean), \u03c3 (standard deviation)</p> <pre><code>from scipy.stats import norm\n\n# Normal distribution\nmu, sigma = 0, 1\nx = np.linspace(-4, 4, 100)\ny = norm.pdf(x, mu, sigma)\n\nplt.plot(x, y, 'b-', linewidth=2)\nplt.title('Normal Distribution (\u03bc=0, \u03c3=1)')\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Sample spaces contain all possible outcomes; events are subsets of interest.</p> </li> <li> <p>Conditional probability P(A|B) represents the probability of A given B has occurred.</p> </li> <li> <p>Bayes' theorem allows us to update probabilities with new information, which is fundamental to many machine learning algorithms.</p> </li> <li> <p>Independent events don't affect each other's probabilities: P(A \u2229 B) = P(A) \u00b7 P(B).</p> </li> <li> <p>Probability distributions describe how probabilities are distributed across outcomes.</p> </li> </ol>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#applications-in-data-science","title":"Applications in Data Science","text":"<ul> <li>Naive Bayes classifiers use Bayes' theorem for classification</li> <li>Bayesian inference updates beliefs with evidence</li> <li>A/B testing uses probability to make decisions</li> <li>Risk assessment in finance and insurance</li> <li>Recommendation systems use probabilistic models</li> </ul>"},{"location":"Maths%20%26%20Statistics/03_probability_foundations/#next-steps","title":"Next StepsWhat's on your mind? Put it in the comments!","text":"<p>Now that you understand probability foundations, you're ready to learn about Inferential Statistics, where we'll use probability to make inferences about populations from samples.</p>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/","title":"Inferential Statistics","text":""},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#introduction","title":"Introduction","text":"<p>Inferential statistics allows us to make conclusions about populations based on sample data. Unlike descriptive statistics, which summarize data we have, inferential statistics help us make predictions and test hypotheses about larger populations.</p>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ul> <li>Understand different sampling methods and their applications</li> <li>Explain the Central Limit Theorem and its importance</li> <li>Calculate and interpret confidence intervals</li> <li>Perform hypothesis testing using various statistical tests</li> <li>Understand Type I and Type II errors</li> <li>Calculate and interpret statistical power and effect size</li> <li>Apply inferential statistics using Python</li> </ul>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#prerequisites","title":"Prerequisites","text":"<p>Before starting this module, you should be familiar with: - Descriptive Statistics - Probability Foundations - Basic Python programming</p>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#1-sampling-methods","title":"1. Sampling Methods","text":"<p>Sampling is the process of selecting a subset of individuals from a population to represent the entire population. The quality of our statistical inferences depends heavily on how we sample.</p>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#simple-random-sampling","title":"Simple Random Sampling","text":"<p>Every individual in the population has an equal chance of being selected.</p> <pre><code>import numpy as np\nimport pandas as pd\n\n# Example: Simple random sampling\npopulation = np.arange(1, 1001)  # Population of 1000 individuals\nsample_size = 100\n\n# Simple random sample\nrandom_sample = np.random.choice(population, size=sample_size, replace=False)\nprint(f\"Random sample (first 10): {random_sample[:10]}\")\n</code></pre>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#stratified-sampling","title":"Stratified Sampling","text":"<p>Divide the population into strata (groups) and sample from each stratum proportionally.</p> <pre><code># Example: Stratified sampling by age groups\n# Population: 1000 people, 40% young (18-30), 35% middle (31-50), 25% old (51+)\n\nyoung = np.random.normal(25, 3, 400)  # 400 young people\nmiddle = np.random.normal(40, 5, 350)  # 350 middle-aged\nold = np.random.normal(60, 7, 250)    # 250 older people\n\n# Stratified sample: 10% from each group\nsample_young = np.random.choice(young, size=40, replace=False)\nsample_middle = np.random.choice(middle, size=35, replace=False)\nsample_old = np.random.choice(old, size=25, replace=False)\n\nstratified_sample = np.concatenate([sample_young, sample_middle, sample_old])\nprint(f\"Stratified sample size: {len(stratified_sample)}\")\nprint(f\"Mean age: {np.mean(stratified_sample):.2f}\")\n</code></pre>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#systematic-sampling","title":"Systematic Sampling","text":"<p>Select every kth individual from the population.</p> <pre><code># Systematic sampling: every 10th person\nk = 10\nsystematic_sample = population[::k]\nprint(f\"Systematic sample (first 10): {systematic_sample[:10]}\")\n</code></pre>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#cluster-sampling","title":"Cluster Sampling","text":"<p>Divide population into clusters and randomly select entire clusters.</p> <pre><code># Example: Cluster sampling by neighborhoods\nneighborhoods = {\n    'A': np.random.normal(50, 10, 200),\n    'B': np.random.normal(55, 12, 150),\n    'C': np.random.normal(48, 8, 180),\n    'D': np.random.normal(52, 11, 170),\n    'E': np.random.normal(49, 9, 300)\n}\n\n# Randomly select 2 clusters\nselected_clusters = np.random.choice(list(neighborhoods.keys()), size=2, replace=False)\ncluster_sample = np.concatenate([neighborhoods[cluster] for cluster in selected_clusters])\nprint(f\"Selected clusters: {selected_clusters}\")\nprint(f\"Cluster sample size: {len(cluster_sample)}\")\n</code></pre>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#2-central-limit-theorem-clt","title":"2. Central Limit Theorem (CLT)","text":"<p>The Central Limit Theorem is one of the most important theorems in statistics. It states:</p> <p>As the sample size increases, the distribution of sample means approaches a normal distribution, regardless of the shape of the population distribution.</p> <p>Key Points: - The mean of sample means equals the population mean: \\(\\mu_{\\bar{x}} = \\mu\\) - The standard deviation of sample means (standard error) is: \\(\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\\) - Works well for sample sizes n \u2265 30</p> <pre><code>import matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Demonstrate CLT with a non-normal population\nnp.random.seed(42)\npopulation = np.random.exponential(scale=2, size=10000)  # Exponential (not normal)\n\n# Plot population distribution\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.hist(population, bins=50, density=True, alpha=0.7, color='blue')\nplt.title('Population Distribution (Exponential)')\nplt.xlabel('Value')\nplt.ylabel('Density')\n\n# Take many samples and calculate means\nsample_means = []\nfor _ in range(1000):\n    sample = np.random.choice(population, size=30, replace=False)\n    sample_means.append(np.mean(sample))\n\n# Plot distribution of sample means\nplt.subplot(1, 3, 2)\nplt.hist(sample_means, bins=50, density=True, alpha=0.7, color='green')\nplt.title('Distribution of Sample Means (n=30)')\nplt.xlabel('Sample Mean')\nplt.ylabel('Density')\n\n# Overlay normal distribution\nmu_sample = np.mean(sample_means)\nsigma_sample = np.std(sample_means)\nx = np.linspace(mu_sample - 4*sigma_sample, mu_sample + 4*sigma_sample, 100)\ny = stats.norm.pdf(x, mu_sample, sigma_sample)\nplt.plot(x, y, 'r-', linewidth=2, label='Normal Distribution')\nplt.legend()\n\n# Compare with larger sample size\nsample_means_large = []\nfor _ in range(1000):\n    sample = np.random.choice(population, size=100, replace=False)\n    sample_means_large.append(np.mean(sample))\n\nplt.subplot(1, 3, 3)\nplt.hist(sample_means_large, bins=50, density=True, alpha=0.7, color='orange')\nplt.title('Distribution of Sample Means (n=100)')\nplt.xlabel('Sample Mean')\nplt.ylabel('Density')\n\nmu_large = np.mean(sample_means_large)\nsigma_large = np.std(sample_means_large)\nx = np.linspace(mu_large - 4*sigma_large, mu_large + 4*sigma_large, 100)\ny = stats.norm.pdf(x, mu_large, sigma_large)\nplt.plot(x, y, 'r-', linewidth=2, label='Normal Distribution')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Population mean: {np.mean(population):.3f}\")\nprint(f\"Mean of sample means (n=30): {mu_sample:.3f}\")\nprint(f\"Mean of sample means (n=100): {mu_large:.3f}\")\nprint(f\"\\nStandard error (n=30): {sigma_sample:.3f}\")\nprint(f\"Standard error (n=100): {sigma_large:.3f}\")\nprint(f\"Theoretical SE (n=30): {np.std(population)/np.sqrt(30):.3f}\")\nprint(f\"Theoretical SE (n=100): {np.std(population)/np.sqrt(100):.3f}\")\n</code></pre> <p>Why CLT Matters: - Allows us to use normal distribution for inference even when population isn't normal - Enables calculation of confidence intervals and p-values - Foundation for many statistical tests</p>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#3-confidence-intervals","title":"3. Confidence Intervals","text":"<p>A confidence interval is a range of values that likely contains the true population parameter. A 95% confidence interval means that if we repeated the sampling process many times, 95% of the intervals would contain the true parameter.</p>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#confidence-interval-for-mean-known","title":"Confidence Interval for Mean (Known \u03c3)","text":"<p>When population standard deviation is known:</p> \\[\\bar{x} \\pm z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\]"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#confidence-interval-for-mean-unknown","title":"Confidence Interval for Mean (Unknown \u03c3)","text":"<p>When population standard deviation is unknown (more common):</p> \\[\\bar{x} \\pm t_{\\alpha/2, df} \\cdot \\frac{s}{\\sqrt{n}}\\] <p>where df = n - 1 (degrees of freedom)</p> <pre><code># Example: Confidence interval for mean\nnp.random.seed(42)\nsample = np.random.normal(50, 10, 30)  # Sample from population with \u03bc=50, \u03c3=10\n\nsample_mean = np.mean(sample)\nsample_std = np.std(sample, ddof=1)  # Sample standard deviation\nn = len(sample)\nconfidence_level = 0.95\nalpha = 1 - confidence_level\n\n# Using t-distribution (unknown population std)\nt_critical = stats.t.ppf(1 - alpha/2, df=n-1)\nmargin_error = t_critical * (sample_std / np.sqrt(n))\n\nci_lower = sample_mean - margin_error\nci_upper = sample_mean + margin_error\n\nprint(f\"Sample mean: {sample_mean:.3f}\")\nprint(f\"Sample std: {sample_std:.3f}\")\nprint(f\"95% Confidence Interval: [{ci_lower:.3f}, {ci_upper:.3f}]\")\nprint(f\"Margin of error: {margin_error:.3f}\")\n\n# Using scipy for convenience\nci = stats.t.interval(confidence_level, df=n-1, loc=sample_mean, scale=stats.sem(sample))\nprint(f\"\\nUsing scipy: [{ci[0]:.3f}, {ci[1]:.3f}]\")\n</code></pre>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#confidence-interval-for-proportion","title":"Confidence Interval for Proportion","text":"<p>For proportions:</p> \\[\\hat{p} \\pm z_{\\alpha/2} \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\] <pre><code># Example: Confidence interval for proportion\n# Survey: 450 out of 1000 people support a policy\nn = 1000\nx = 450\np_hat = x / n\n\nz_critical = stats.norm.ppf(0.975)  # For 95% CI\nse = np.sqrt(p_hat * (1 - p_hat) / n)\nmargin_error = z_critical * se\n\nci_lower = p_hat - margin_error\nci_upper = p_hat + margin_error\n\nprint(f\"Sample proportion: {p_hat:.3f}\")\nprint(f\"95% Confidence Interval: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n</code></pre> <p>Interpreting Confidence Intervals: - We are 95% confident that the true population parameter lies within the interval - The interval does NOT mean there's a 95% chance the parameter is in the interval (it either is or isn't) - Wider intervals indicate more uncertainty</p>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#4-hypothesis-testing","title":"4. Hypothesis Testing","text":"<p>Hypothesis Testing is a statistical method used to make decisions or inferences about a population parameter based on sample data.</p>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#1-what-is-hypothesis-testing","title":"1. What is Hypothesis Testing?","text":"<p>Hypothesis Testing is a statistical method used to make decisions or inferences about a population parameter based on sample data. It helps determine if there is enough evidence to reject a null hypothesis in favor of an alternative hypothesis.</p>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#2-types-of-hypotheses","title":"2. Types of Hypotheses","text":"<ol> <li>Null Hypothesis (H\u2080): This is the default assumption that there is no effect or no difference. For example, \"There is no difference in mean height between men and women.\"</li> <li>Alternative Hypothesis (H\u2081): This opposes the null hypothesis, stating that there is an effect or difference. For example, \"There is a difference in mean height between men and women.\"</li> </ol>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#3-steps-in-hypothesis-testing","title":"3. Steps in Hypothesis Testing","text":"<ol> <li>Define Hypotheses: Set the null and alternative hypotheses.</li> <li>Select Significance Level (\u03b1): The probability threshold (commonly 0.05) below which the null hypothesis is rejected.</li> <li>Choose the Test Statistic: Based on the type of data and sample size, e.g., t-test, chi-square test.</li> <li>Compute p-value: This is the probability of observing the data if the null hypothesis is true.</li> <li>Make a Decision:</li> <li>If p-value &lt; \u03b1, reject the null hypothesis.</li> <li>If p-value \u2265 \u03b1, fail to reject the null hypothesis.</li> </ol>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#4-common-hypothesis-tests","title":"4. Common Hypothesis Tests","text":"<ol> <li>One-sample t-test: Tests whether the mean of a sample is significantly different from a known or hypothesized value.</li> <li>Two-sample t-test: Compares the means of two independent samples.</li> <li>Chi-square test: Tests for association between categorical variables.</li> </ol>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#5-example-hypothesis-testing-in-python","title":"5. Example: Hypothesis Testing in Python","text":""},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#a-import-libraries","title":"(a) Import Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#b-one-sample-t-test","title":"(b) One-sample t-test","text":"<p>Scenario: You want to test whether the average height of people in a town is 170 cm. You take a sample of 30 people and record their heights.</p> <pre><code># Sample data (heights of 30 people)\nnp.random.seed(42)  # For reproducibility\nsample_heights = np.random.normal(168, 5, 30)  # mean=168, std=5\n\n# Perform one-sample t-test\nt_stat, p_value = stats.ttest_1samp(sample_heights, 170)\n\nprint(f\"t-statistic: {t_stat:.4f}, p-value: {p_value:.4f}\")\n\n# Significance level\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"Reject the null hypothesis: The average height is not 170 cm.\")\nelse:\n    print(\"Fail to reject the null hypothesis: The average height is 170 cm.\")\n</code></pre> <pre><code>t-statistic: -3.5793, p-value: 0.0012\nReject the null hypothesis: The average height is not 170 cm.\n</code></pre>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#c-two-sample-t-test","title":"(c) Two-sample t-test","text":"<p>Scenario: You want to compare the average heights of men and women in the same town. You have the height data for 20 men and 20 women.</p> <pre><code># Heights of men and women (sample data)\nnp.random.seed(42)\nmen_heights = np.random.normal(175, 6, 20)\nwomen_heights = np.random.normal(165, 5, 20)\n\n# Perform two-sample t-test\nt_stat, p_value = stats.ttest_ind(men_heights, women_heights)\n\nprint(f\"t-statistic: {t_stat:.4f}, p-value: {p_value:.4f}\")\n\n# Decision\nif p_value &lt; alpha:\n    print(\"Reject the null hypothesis: There is a significant difference between men's and women's heights.\")\nelse:\n    print(\"Fail to reject the null hypothesis: No significant difference in heights.\")\n</code></pre> <pre><code>t-statistic: 6.1236, p-value: 0.0000\nReject the null hypothesis: There is a significant difference between men's and women's heights.\n</code></pre>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#d-chi-square-test","title":"(d) Chi-square test","text":"<p>Scenario: You want to test whether gender and preference for a product (Yes/No) are independent in a survey.</p> <pre><code># Contingency table (sample data)\n# Rows: Gender (Male, Female), Columns: Preference (Yes, No)\ndata = [[30, 10], [25, 15]]\n\n# Perform chi-square test\nchi2_stat, p_value, dof, expected = stats.chi2_contingency(data)\n\nprint(f\"Chi-square statistic: {chi2_stat:.4f}, p-value: {p_value:.4f}\")\n\n# Decision\nif p_value &lt; alpha:\n    print(\"Reject the null hypothesis: Gender and preference are not independent.\")\nelse:\n    print(\"Fail to reject the null hypothesis: Gender and preference are independent.\")\n</code></pre> <pre><code>Chi-square statistic: 0.9309, p-value: 0.3346\nFail to reject the null hypothesis: Gender and preference are independent.\n</code></pre>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#e-anova-analysis-of-variance","title":"(e) ANOVA (Analysis of Variance)","text":"<p>Scenario: You want to compare the average heights of people from three different cities to see if there is a statistically significant difference in their means.</p> <p>Hypotheses: - H\u2080 (Null Hypothesis): The means of all groups are equal. - H\u2081 (Alternative Hypothesis): At least one group has a different mean.</p> <pre><code># Simulate data for three cities (heights)\nnp.random.seed(42)\ncity1_heights = np.random.normal(168, 5, 30)\ncity2_heights = np.random.normal(170, 6, 30)\ncity3_heights = np.random.normal(165, 4, 30)\n\n# Combine data into a pandas DataFrame\ndf = pd.DataFrame({\n    'city1': city1_heights,\n    'city2': city2_heights,\n    'city3': city3_heights\n})\n\n# Visualize the data\ndf.boxplot()\nplt.title('Height Distributions for Three Cities')\nplt.ylabel('Height (cm)')\nplt.show()\n\n# Perform ANOVA test\nf_stat, p_value = stats.f_oneway(city1_heights, city2_heights, city3_heights)\n\nprint(f\"F-statistic: {f_stat:.4f}, p-value: {p_value:.4f}\")\n\n# Decision\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"Reject the null hypothesis: At least one city has a different mean height.\")\nelse:\n    print(\"Fail to reject the null hypothesis: All cities have the same mean height.\")\n</code></pre> <p></p> <pre><code>F-statistic: 5.9711, p-value: 0.0037\nReject the null hypothesis: At least one city has a different mean height.\n</code></pre>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#explanation-of-anova","title":"Explanation of ANOVA","text":"<p> F-statistic: A ratio of the variance between the group means to the variance within the groups. A higher F-statistic indicates a greater disparity between group means.</p>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#5-type-i-and-type-ii-errors","title":"5. Type I and Type II Errors","text":"<p>When performing hypothesis tests, we can make two types of errors:</p>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#type-i-error-false-positive","title":"Type I Error (False Positive)","text":"<ul> <li>Definition: Rejecting the null hypothesis when it is actually true</li> <li>Probability: Denoted by \u03b1 (alpha), the significance level</li> <li>Example: Concluding a drug is effective when it's not</li> </ul>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#type-ii-error-false-negative","title":"Type II Error (False Negative)","text":"<ul> <li>Definition: Failing to reject the null hypothesis when it is actually false</li> <li>Probability: Denoted by \u03b2 (beta)</li> <li>Example: Concluding a drug is not effective when it actually is</li> </ul> <pre><code># Visualizing Type I and Type II errors\nfrom scipy.stats import norm\nimport matplotlib.patches as mpatches\n\n# Example: Testing if mean = 100\n# H0: \u03bc = 100, H1: \u03bc \u2260 100\nmu0 = 100\nmu1 = 105  # True mean (if H0 is false)\nsigma = 10\nn = 30\nalpha = 0.05\n\n# Critical values for two-tailed test\nz_critical = stats.norm.ppf(1 - alpha/2)\nse = sigma / np.sqrt(n)\ncritical_lower = mu0 - z_critical * se\ncritical_upper = mu0 + z_critical * se\n\n# Plot distributions\nx = np.linspace(85, 120, 1000)\ny0 = norm.pdf(x, mu0, se)  # Distribution under H0\ny1 = norm.pdf(x, mu1, se)   # Distribution under H1\n\nplt.figure(figsize=(12, 6))\nplt.plot(x, y0, 'b-', linewidth=2, label='H0: \u03bc = 100')\nplt.plot(x, y1, 'r-', linewidth=2, label='H1: \u03bc = 105')\n\n# Shade Type I error region (reject H0 when H0 is true)\nx_type1 = np.linspace(critical_upper, 120, 100)\ny_type1 = norm.pdf(x_type1, mu0, se)\nplt.fill_between(x_type1, 0, y_type1, alpha=0.3, color='blue', label='Type I Error (\u03b1)')\n\nx_type1_lower = np.linspace(85, critical_lower, 100)\ny_type1_lower = norm.pdf(x_type1_lower, mu0, se)\nplt.fill_between(x_type1_lower, 0, y_type1_lower, alpha=0.3, color='blue')\n\n# Shade Type II error region (fail to reject H0 when H1 is true)\nx_type2 = np.linspace(critical_lower, critical_upper, 100)\ny_type2 = norm.pdf(x_type2, mu1, se)\nplt.fill_between(x_type2, 0, y_type2, alpha=0.3, color='red', label='Type II Error (\u03b2)')\n\n# Critical values\nplt.axvline(critical_lower, color='black', linestyle='--', linewidth=1.5, label='Critical Values')\nplt.axvline(critical_upper, color='black', linestyle='--', linewidth=1.5)\n\nplt.xlabel('Sample Mean')\nplt.ylabel('Probability Density')\nplt.title('Type I and Type II Errors in Hypothesis Testing')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Calculate Type II error probability\nbeta = norm.cdf(critical_upper, mu1, se) - norm.cdf(critical_lower, mu1, se)\npower = 1 - beta\n\nprint(f\"Type I Error (\u03b1): {alpha}\")\nprint(f\"Type II Error (\u03b2): {beta:.4f}\")\nprint(f\"Statistical Power (1 - \u03b2): {power:.4f}\")\n</code></pre> <p>Trade-off: - Decreasing \u03b1 (Type I error) increases \u03b2 (Type II error) - Increasing sample size decreases both errors - Power = 1 - \u03b2 (probability of correctly rejecting false H0)</p>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#6-statistical-power","title":"6. Statistical Power","text":"<p>Statistical power is the probability of correctly rejecting a false null hypothesis. It's the complement of Type II error: Power = 1 - \u03b2.</p> <p>Factors affecting power: 1. Effect size: Larger effects are easier to detect 2. Sample size: Larger samples increase power 3. Significance level (\u03b1): Higher \u03b1 increases power (but also Type I error) 4. Variability: Less variability increases power</p> <pre><code># Calculating power for different scenarios\nfrom scipy.stats import norm\n\ndef calculate_power(mu0, mu1, sigma, n, alpha=0.05, two_tailed=True):\n    \"\"\"Calculate statistical power for a one-sample z-test\"\"\"\n    se = sigma / np.sqrt(n)\n\n    if two_tailed:\n        z_critical = norm.ppf(1 - alpha/2)\n        critical_lower = mu0 - z_critical * se\n        critical_upper = mu0 + z_critical * se\n        # Power = P(reject H0 | H1 is true)\n        power = 1 - (norm.cdf(critical_upper, mu1, se) - norm.cdf(critical_lower, mu1, se))\n    else:\n        z_critical = norm.ppf(1 - alpha)\n        critical = mu0 + z_critical * se\n        power = 1 - norm.cdf(critical, mu1, se)\n\n    return power\n\n# Power analysis: How does sample size affect power?\nsample_sizes = np.arange(10, 201, 10)\npowers = [calculate_power(100, 105, 10, n) for n in sample_sizes]\n\nplt.figure(figsize=(10, 6))\nplt.plot(sample_sizes, powers, 'b-', linewidth=2, marker='o')\nplt.axhline(0.8, color='r', linestyle='--', label='80% Power (Common Target)')\nplt.xlabel('Sample Size')\nplt.ylabel('Statistical Power')\nplt.title('Statistical Power vs Sample Size\\n(H0: \u03bc=100, H1: \u03bc=105, \u03c3=10, \u03b1=0.05)')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n\n# Find sample size needed for 80% power\ntarget_power = 0.8\nfor n in sample_sizes:\n    if calculate_power(100, 105, 10, n) &gt;= target_power:\n        print(f\"Sample size needed for {target_power*100}% power: {n}\")\n        break\n</code></pre> <p>Power Analysis: - Before conducting a study, calculate required sample size for desired power - Common target: 80% power (\u03b2 = 0.20) - Low power increases risk of missing real effects</p>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#7-effect-size","title":"7. Effect Size","text":"<p>Effect size measures the magnitude of a difference or relationship, independent of sample size. Unlike p-values, effect sizes are not affected by sample size.</p>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#cohens-d-for-means","title":"Cohen's d (for means)","text":"\\[d = \\frac{\\mu_1 - \\mu_2}{\\sigma_{pooled}}\\] <p>where \\(\\sigma_{pooled} = \\sqrt{\\frac{\\sigma_1^2 + \\sigma_2^2}{2}}\\)</p> <p>Interpretation: - Small: d \u2248 0.2 - Medium: d \u2248 0.5 - Large: d \u2248 0.8</p> <pre><code># Calculate Cohen's d\ndef cohens_d(group1, group2):\n    \"\"\"Calculate Cohen's d effect size\"\"\"\n    n1, n2 = len(group1), len(group2)\n    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n\n    # Pooled standard deviation\n    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n\n    # Cohen's d\n    d = (np.mean(group1) - np.mean(group2)) / pooled_std\n    return d\n\n# Example\nnp.random.seed(42)\ngroup1 = np.random.normal(100, 15, 50)\ngroup2 = np.random.normal(105, 15, 50)\n\nd = cohens_d(group1, group2)\nprint(f\"Cohen's d: {d:.3f}\")\n\n# Interpret effect size\nif abs(d) &lt; 0.2:\n    interpretation = \"negligible\"\nelif abs(d) &lt; 0.5:\n    interpretation = \"small\"\nelif abs(d) &lt; 0.8:\n    interpretation = \"medium\"\nelse:\n    interpretation = \"large\"\n\nprint(f\"Effect size: {interpretation}\")\n\n# Compare with statistical significance\nt_stat, p_value = stats.ttest_ind(group1, group2)\nprint(f\"\\nt-statistic: {t_stat:.3f}\")\nprint(f\"p-value: {p_value:.4f}\")\nprint(f\"Statistically significant: {p_value &lt; 0.05}\")\nprint(f\"Effect size: {interpretation}\")\n</code></pre>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#effect-size-for-proportions","title":"Effect Size for Proportions","text":"<p>Cohen's h for proportions:</p> \\[h = 2 \\arcsin(\\sqrt{p_1}) - 2 \\arcsin(\\sqrt{p_2})\\]"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#effect-size-for-correlations","title":"Effect Size for Correlations","text":"<p>Pearson's r is itself an effect size: - Small: r \u2248 0.1 - Medium: r \u2248 0.3 - Large: r \u2248 0.5</p> <p>Why Effect Size Matters: - A statistically significant result may have a tiny effect size (practical significance) - Large sample sizes can make trivial differences statistically significant - Effect size helps assess practical importance</p>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Sampling methods affect the quality of inferences; random sampling is preferred when possible.</p> </li> <li> <p>Central Limit Theorem allows us to use normal distribution for inference with large enough samples.</p> </li> <li> <p>Confidence intervals provide a range of plausible values for population parameters.</p> </li> <li> <p>Hypothesis testing helps make decisions about population parameters based on sample data.</p> </li> <li> <p>Type I errors (false positives) and Type II errors (false negatives) are inherent risks in hypothesis testing.</p> </li> <li> <p>Statistical power is the probability of detecting a true effect; aim for at least 80% power.</p> </li> <li> <p>Effect size measures practical significance, independent of sample size.</p> </li> </ol>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#applications-in-data-science","title":"Applications in Data Science","text":"<ul> <li>A/B testing: Compare two versions to determine which performs better</li> <li>Feature selection: Test if features are significantly related to target</li> <li>Model validation: Test if model performance is significantly better than baseline</li> <li>Experimental design: Determine sample sizes needed for reliable results</li> </ul>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#next-steps","title":"Next Steps","text":"<p>Now that you understand inferential statistics, you're ready to learn about Statistical Models, where we'll build models to understand relationships in data.</p>"},{"location":"Maths%20%26%20Statistics/04_inferential_statistics/#conclusion","title":"ConclusionWhat's on your mind? Put it in the comments!","text":"<p>Congratulations on completing this module! Inferential statistics provides powerful tools for making data-driven decisions. This module covered:</p> <ul> <li> Sampling methods</li> <li> Central Limit Theorem</li> <li> Confidence intervals</li> <li> Hypothesis testing (t-tests, chi-square, ANOVA)</li> <li> Type I and Type II errors</li> <li> Statistical power and effect size</li> </ul>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/","title":"Statistical Models","text":""},{"location":"Maths%20%26%20Statistics/05_statistical_models/#introduction","title":"Introduction","text":"<p>Statistical models are mathematical representations of relationships between variables in a dataset. These models are used to make predictions, infer causal relationships, and understand patterns in data. This module focuses on two fundamental models: linear regression and logistic regression.</p>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ul> <li>Understand when to use linear vs logistic regression</li> <li>Build linear and logistic regression models using Python</li> <li>Check and validate model assumptions</li> <li>Interpret model coefficients and outputs</li> <li>Evaluate model performance using appropriate metrics</li> <li>Diagnose and address common model problems</li> </ul>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#prerequisites","title":"Prerequisites","text":"<p>Before starting this module, you should be familiar with: - Descriptive Statistics - Probability Foundations - Inferential Statistics - Basic Python programming with NumPy and Pandas</p> <pre><code>## Uncomment and run this cell to install the packages\n# !pip install pandas numpy statsmodels matplotlib seaborn scikit-learn\n</code></pre> <pre><code>import statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix, classification_report\n</code></pre>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#1-linear-regression","title":"1. Linear Regression","text":"<p>Linear regression models the relationship between one or more independent variables (predictors) and a continuous dependent variable (outcome).</p>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#model-equation","title":"Model Equation","text":"<p>For simple linear regression (one predictor):</p> \\[y = \\beta_0 + \\beta_1 x + \\epsilon\\] <p>For multiple linear regression:</p> \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon\\] <p>where: - \\(y\\) is the dependent variable - \\(x_i\\) are independent variables - \\(\\beta_0\\) is the intercept - \\(\\beta_i\\) are coefficients (slopes) - \\(\\epsilon\\) is the error term</p>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#assumptions-of-linear-regression","title":"Assumptions of Linear Regression","text":"<ol> <li>Linearity: The relationship between X and Y is linear</li> <li>Independence: Observations are independent</li> <li>Homoscedasticity: Constant variance of errors</li> <li>Normality: Errors are normally distributed</li> <li>No multicollinearity: Predictors are not highly correlated (for multiple regression)</li> </ol>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#building-a-linear-regression-model","title":"Building a Linear Regression Model","text":"<pre><code># Generate example data\nnp.random.seed(42)\nn = 100\n\n# Create features\nX1 = np.random.randn(n)\nX2 = np.random.randn(n)\n\n# True relationship: y = 2 + 3*X1 + 1.5*X2 + noise\ny = 2 + 3*X1 + 1.5*X2 + np.random.randn(n) * 0.5\n\n# Prepare data for statsmodels (needs constant term)\nX = pd.DataFrame({'X1': X1, 'X2': X2})\nX = sm.add_constant(X)  # Adds intercept term\n\n# Fit the model\nmodel = sm.OLS(y, X).fit()\n\n# Print summary\nprint(model.summary())\n</code></pre>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#interpreting-linear-regression-output","title":"Interpreting Linear Regression Output","text":"<pre><code># Key metrics from summary:\nprint(f\"\\nR-squared: {model.rsquared:.4f}\")\nprint(f\"Adjusted R-squared: {model.rsquared_adj:.4f}\")\nprint(f\"F-statistic: {model.fvalue:.4f}\")\nprint(f\"F-statistic p-value: {model.f_pvalue:.4f}\")\n\n# Coefficients\nprint(\"\\nCoefficients:\")\nprint(model.params)\n\n# Confidence intervals\nprint(\"\\n95% Confidence Intervals:\")\nprint(model.conf_int())\n</code></pre> <p>Key Interpretations: - R-squared: Proportion of variance in y explained by the model (0 to 1, higher is better) - Adjusted R-squared: R-squared adjusted for number of predictors (penalizes overfitting) - F-statistic: Tests if model is significantly better than intercept-only model - Coefficients: Change in y for one-unit change in predictor (holding others constant) - p-values: Significance of each coefficient</p>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#model-diagnostics","title":"Model Diagnostics","text":"<pre><code># 1. Residuals vs Fitted Values (check for homoscedasticity)\nfitted = model.fittedvalues\nresiduals = model.resid\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.scatter(fitted, residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Fitted Values')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Fitted Values')\nplt.grid(True, alpha=0.3)\n\n# 2. Q-Q Plot (check for normality of residuals)\nfrom scipy import stats\nplt.subplot(1, 3, 2)\nstats.probplot(residuals, dist=\"norm\", plot=plt)\nplt.title('Q-Q Plot (Normality Check)')\nplt.grid(True, alpha=0.3)\n\n# 3. Scale-Location Plot (check for homoscedasticity)\nplt.subplot(1, 3, 3)\nsqrt_abs_residuals = np.sqrt(np.abs(residuals))\nplt.scatter(fitted, sqrt_abs_residuals, alpha=0.6)\nplt.xlabel('Fitted Values')\nplt.ylabel('\u221a|Standardized Residuals|')\nplt.title('Scale-Location Plot')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#evaluating-model-performance","title":"Evaluating Model Performance","text":"<pre><code># Split data for evaluation\nX_train, X_test, y_train, y_test = train_test_split(\n    X.drop('const', axis=1), y, test_size=0.2, random_state=42\n)\n\n# Fit on training data\nX_train_sm = sm.add_constant(X_train)\nmodel_train = sm.OLS(y_train, X_train_sm).fit()\n\n# Predict on test data\nX_test_sm = sm.add_constant(X_test)\ny_pred = model_train.predict(X_test_sm)\n\n# Evaluation metrics\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error: {mse:.4f}\")\nprint(f\"Root Mean Squared Error: {rmse:.4f}\")\nprint(f\"R-squared: {r2:.4f}\")\n\n# Visualize predictions\nplt.figure(figsize=(8, 6))\nplt.scatter(y_test, y_pred, alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Actual vs Predicted Values')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>1. Multicollinearity <pre><code># Check for multicollinearity using correlation matrix\ncorrelation_matrix = X.drop('const', axis=1).corr()\nprint(\"Correlation Matrix:\")\nprint(correlation_matrix)\n\n# High correlation (&gt;0.8) indicates multicollinearity\n# Solution: Remove one of the highly correlated variables or use regularization\n</code></pre></p> <p>2. Non-linear Relationships <pre><code># If relationship is non-linear, consider:\n# - Polynomial features\n# - Transformations (log, sqrt)\n# - Non-linear models\n</code></pre></p> <p>3. Outliers <pre><code># Detect outliers using residuals\nstandardized_residuals = residuals / np.std(residuals)\noutliers = np.abs(standardized_residuals) &gt; 3\n\nprint(f\"Number of outliers: {outliers.sum()}\")\n# Solution: Investigate outliers, consider robust regression\n</code></pre></p>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#2-logistic-regression","title":"2. Logistic Regression","text":"<p>Logistic regression is used when the dependent variable is binary (0/1, Yes/No, Success/Failure). It models the probability of the outcome.</p>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#model-equation_1","title":"Model Equation","text":"<p>The logistic function (sigmoid) transforms linear combination to probabilities:</p> \\[P(y=1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n)}}\\] <p>Or in log-odds form:</p> \\[\\log\\left(\\frac{P(y=1)}{1-P(y=1)}\\right) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n\\]"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#assumptions-of-logistic-regression","title":"Assumptions of Logistic Regression","text":"<ol> <li>Binary outcome: Dependent variable is binary</li> <li>Independence: Observations are independent</li> <li>Linearity: Log-odds are linear in predictors</li> <li>No multicollinearity: Predictors are not highly correlated</li> <li>Large sample size: Generally need n \u2265 10 per predictor</li> </ol>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#building-a-logistic-regression-model","title":"Building a Logistic Regression Model","text":"<pre><code># Generate example data for logistic regression\nnp.random.seed(42)\nn = 200\n\n# Create features\nX1 = np.random.randn(n)\nX2 = np.random.randn(n)\n\n# True relationship: log-odds = -1 + 2*X1 + 1.5*X2\nlog_odds = -1 + 2*X1 + 1.5*X2\nprob = 1 / (1 + np.exp(-log_odds))  # Sigmoid function\n\n# Generate binary outcome\ny = (np.random.rand(n) &lt; prob).astype(int)\n\n# Prepare data\nX = pd.DataFrame({'X1': X1, 'X2': X2})\nX = sm.add_constant(X)\n\n# Fit logistic regression model\nlogit_model = sm.Logit(y, X).fit()\n\n# Print summary\nprint(logit_model.summary())\n</code></pre>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#interpreting-logistic-regression-output","title":"Interpreting Logistic Regression Output","text":"<pre><code># Coefficients are in log-odds scale\nprint(\"Coefficients (log-odds):\")\nprint(logit_model.params)\n\n# Convert to odds ratios\nodds_ratios = np.exp(logit_model.params)\nprint(\"\\nOdds Ratios:\")\nprint(odds_ratios)\n\n# Interpretation example\nprint(f\"\\nFor X1:\")\nprint(f\"  Coefficient: {logit_model.params['X1']:.4f}\")\nprint(f\"  Odds Ratio: {odds_ratios['X1']:.4f}\")\nprint(f\"  Interpretation: One unit increase in X1 multiplies odds by {odds_ratios['X1']:.4f}\")\n</code></pre> <p>Key Interpretations: - Coefficients: Change in log-odds for one-unit change in predictor - Odds Ratio: Multiplicative change in odds (e^(coefficient)) - Pseudo R-squared: Measures model fit (McFadden's, Nagelkerke's) - Log-likelihood: Measure of model fit (higher is better)</p>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#model-evaluation-for-classification","title":"Model Evaluation for Classification","text":"<pre><code># Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X.drop('const', axis=1), y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Fit on training data\nX_train_sm = sm.add_constant(X_train)\nlogit_train = sm.Logit(y_train, X_train_sm).fit()\n\n# Predict probabilities\nX_test_sm = sm.add_constant(X_test)\ny_pred_proba = logit_train.predict(X_test_sm)\n\n# Convert to binary predictions (using 0.5 threshold)\ny_pred = (y_pred_proba &gt;= 0.5).astype(int)\n\n# Evaluation metrics\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.4f}\")\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\n\n# Classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# ROC Curve\nfrom sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#model-diagnostics-for-logistic-regression","title":"Model Diagnostics for Logistic Regression","text":"<pre><code># 1. Check for separation (perfect prediction)\n# If model has very large coefficients, may indicate separation\n\n# 2. Residuals analysis (deviance residuals)\ndeviance_residuals = logit_model.resid_dev\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.scatter(logit_model.fittedvalues, deviance_residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Fitted Values')\nplt.ylabel('Deviance Residuals')\nplt.title('Deviance Residuals vs Fitted Values')\nplt.grid(True, alpha=0.3)\n\n# 3. Influence measures\ninfluence = logit_model.get_influence()\ncooks_d = influence.cooks_distance[0]\n\nplt.subplot(1, 2, 2)\nplt.scatter(range(len(cooks_d)), cooks_d, alpha=0.6)\nplt.axhline(y=4/len(cooks_d), color='r', linestyle='--', label='Threshold')\nplt.xlabel('Observation')\nplt.ylabel(\"Cook's Distance\")\nplt.title(\"Cook's Distance (Influence)\")\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#3-model-comparison-and-selection","title":"3. Model Comparison and Selection","text":""},{"location":"Maths%20%26%20Statistics/05_statistical_models/#information-criteria","title":"Information Criteria","text":"<pre><code># AIC (Akaike Information Criterion) - lower is better\n# BIC (Bayesian Information Criterion) - lower is better\n\nprint(\"Linear Regression:\")\nprint(f\"  AIC: {model.aic:.2f}\")\nprint(f\"  BIC: {model.bic:.2f}\")\n\nprint(\"\\nLogistic Regression:\")\nprint(f\"  AIC: {logit_model.aic:.2f}\")\nprint(f\"  BIC: {logit_model.bic:.2f}\")\n\n# Compare models using AIC/BIC (only for same type of outcome)\n</code></pre>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#cross-validation","title":"Cross-Validation","text":"<pre><code>from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\n# Using scikit-learn for easier cross-validation\nlr_sklearn = LogisticRegression()\nX_features = X.drop('const', axis=1)\n\n# 5-fold cross-validation\ncv_scores = cross_val_score(lr_sklearn, X_features, y, cv=5, scoring='accuracy')\nprint(f\"Cross-validation scores: {cv_scores}\")\nprint(f\"Mean CV accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n</code></pre>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Linear regression is for continuous outcomes; logistic regression is for binary outcomes.</p> </li> <li> <p>Always check assumptions before interpreting results; violations can lead to incorrect conclusions.</p> </li> <li> <p>Model diagnostics help identify problems like non-linearity, heteroscedasticity, and outliers.</p> </li> <li> <p>Interpret coefficients carefully: </p> </li> <li>Linear regression: change in outcome per unit change in predictor</li> <li> <p>Logistic regression: change in log-odds (or odds ratio) per unit change in predictor</p> </li> <li> <p>Use appropriate evaluation metrics:</p> </li> <li>Regression: MSE, RMSE, R\u00b2</li> <li> <p>Classification: Accuracy, Precision, Recall, F1-score, ROC-AUC</p> </li> <li> <p>Cross-validation helps assess model generalizability to new data.</p> </li> </ol>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#applications-in-data-science","title":"Applications in Data Science","text":"<ul> <li>Predictive modeling: Forecast continuous or categorical outcomes</li> <li>Feature importance: Identify which predictors matter most</li> <li>Causal inference: Understand relationships (with caution)</li> <li>Risk assessment: Predict probabilities of events</li> <li>A/B testing: Compare treatment effects</li> </ul>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#next-steps","title":"Next Steps","text":"<p>Now that you understand statistical models, you can explore: - Advanced Linear Algebra for dimensionality reduction techniques - Machine Learning models for more complex relationships - Time series models for temporal data</p>"},{"location":"Maths%20%26%20Statistics/05_statistical_models/#conclusion","title":"ConclusionWhat's on your mind? Put it in the comments!","text":"<p>Statistical models provide powerful tools for understanding relationships in data. This module covered: - Building and interpreting linear regression models - Building and interpreting logistic regression models - Model diagnostics and assumption checking - Model evaluation and comparison - Common issues and solutions</p> <p>Remember: A good model is not just about high R\u00b2 or accuracy\u2014it should be interpretable, generalizable, and based on sound statistical principles.</p>"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/","title":"Advanced Linear Algebra - Eigenvalues and Eigenvectors","text":""},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#introduction","title":"Introduction","text":"<p>Eigenvalues and eigenvectors are fundamental concepts in linear algebra with wide applications in data science, particularly in dimensionality reduction techniques like Principal Component Analysis (PCA). This module builds on the linear algebra basics covered in the Introduction.</p>"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this lesson, you will be able to:</p> <ul> <li>Define eigenvalues and eigenvectors</li> <li>Compute eigenvalues and eigenvectors of a 2x2 matrix</li> <li>Understand the significance of these concepts in linear algebra and data science</li> <li>Apply eigenvalues and eigenvectors in Python</li> <li>Recognize applications in dimensionality reduction</li> </ul>"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#prerequisites","title":"Prerequisites","text":"<p>Before starting this module, you should be familiar with: - Introduction to Mathematics &amp; Statistics - Linear algebra basics - Matrix operations (multiplication, transpose) - Basic Python programming with NumPy</p>"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#next-steps","title":"Next Steps","text":"<p>After understanding eigenvalues and eigenvectors, you can explore: - Principal Component Analysis (PCA) in machine learning - Singular Value Decomposition (SVD) - Advanced dimensionality reduction techniques</p>"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#what-are-eigenvalues-and-eigenvectors","title":"What Are Eigenvalues and Eigenvectors?","text":"<p>Let\u2019s consider a square matrix \\(A\\). An eigenvector \\(\\vec{v}\\) and its corresponding eigenvalue \\(\\lambda\\) satisfy the equation:</p> \\[ A \\vec{v} = \\lambda \\vec{v} \\] <p>Where:</p> <ul> <li>\\(A\\) is an \\(n \\times n\\) matrix.</li> <li>\\(\\vec{v}\\) is a non-zero vector.</li> <li>\\(\\lambda\\) is a scalar (just a number).</li> </ul> <p>\ud83d\udca1 Intuition: The vector \\(\\vec{v}\\) doesn't change its direction when multiplied by matrix \\(A\\); it only gets stretched or compressed by \\(\\lambda\\).</p>"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#step-by-step-computation","title":"Step-by-Step Computation","text":"<p>Let\u2019s work with this matrix:</p> \\[ A = \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix} \\]"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#step-1-characteristic-equation","title":"Step 1: Characteristic Equation","text":"<p>To find the eigenvalues, we solve:</p> \\[ \\det(A - \\lambda I) = 0 \\] <p>Where \\(I\\) is the identity matrix. Subtract \\(\\lambda I\\) from \\(A\\):</p> \\[ A - \\lambda I =  \\begin{bmatrix} 2 - \\lambda &amp; 1 \\\\ 1 &amp; 2 - \\lambda \\end{bmatrix} \\] <p>Compute the determinant:</p> \\[ \\det = (2 - \\lambda)^2 - 1 = \\lambda^2 - 4\\lambda + 3 \\] <p>Solve the characteristic equation:</p> \\[ \\lambda^2 - 4\\lambda + 3 = 0 \\Rightarrow \\lambda = 3 \\text{ or } 1 \\]"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#step-2-find-eigenvectors","title":"Step 2: Find Eigenvectors","text":""},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#for-lambda-3","title":"For \\(\\lambda = 3\\):","text":"<p>Solve \\((A - 3I)\\vec{v} = 0\\):</p> \\[ \\begin{bmatrix} -1 &amp; 1 \\\\ 1 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = 0 \\Rightarrow -x + y = 0 \\Rightarrow y = x \\] <p>So the eigenvector is any scalar multiple of:</p> \\[ \\vec{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\]"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#for-lambda-1","title":"For \\(\\lambda = 1\\):","text":"<p>Solve \\((A - I)\\vec{v} = 0\\):</p> \\[ \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = 0 \\Rightarrow x + y = 0 \\Rightarrow y = -x \\] <p>So the eigenvector is any scalar multiple of:</p> \\[ \\vec{v}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\]"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#final-result","title":"Final Result","text":"<ul> <li>Eigenvalues:</li> </ul> <p>$$   \\lambda_1 = 3, \\quad \\lambda_2 = 1   $$</p> <ul> <li>Eigenvectors:</li> </ul> <p>$$   \\vec{v}_1 = \\begin{bmatrix} 1 \\ 1 \\end{bmatrix}, \\quad \\vec{v}_2 = \\begin{bmatrix} 1 \\ -1 \\end{bmatrix}   $$</p>"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#why-it-matters-applications","title":"Why It Matters (Applications)","text":"<ul> <li>In Principal Component Analysis (PCA), we use eigenvectors of the covariance matrix to identify directions of maximum variance.</li> <li>In machine learning, eigenvalues help reduce dimensionality and noise.</li> <li>In differential equations, eigenvalues determine system stability.</li> </ul>"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#step-3-implement-in-python","title":"Step 3: Implement in Python","text":"<p>Let\u2019s verify our work using Python and NumPy.</p> <pre><code>import numpy as np\n\n# Define the matrix A\nA = np.array([[2, 1],\n              [1, 2]])\n\n# Compute eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# Display results\nprint(\"Eigenvalues:\")\nprint(eigenvalues)\n\nprint(\"\\nEigenvectors (columns):\")\nprint(eigenvectors)\n</code></pre>"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#expected-output","title":"Expected Output","text":"<p>The output should be:</p> <pre><code>Eigenvalues:\n[3. 1.]\n\nEigenvectors (columns):\n[[ 0.70710678 -0.70710678]\n [ 0.70710678  0.70710678]]\n</code></pre> <p>\ud83d\udca1 Note:</p> <ul> <li>The eigenvectors are normalized (unit vectors).</li> <li>The columns of the output matrix are the eigenvectors corresponding to each eigenvalue.</li> </ul>"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#interpreting-the-output","title":"Interpreting the Output","text":"<ul> <li>Eigenvalues: <code>3</code> and <code>1</code> match our manual calculation.</li> <li> <p>Eigenvectors:</p> </li> <li> <p>The first column \\(\\begin{bmatrix} 0.707 \\\\ 0.707 \\end{bmatrix}\\) is equivalent to \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) normalized.</p> </li> <li>The second column \\(\\begin{bmatrix} -0.707 \\\\ 0.707 \\end{bmatrix}\\) corresponds to \\(\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\) normalized.</li> </ul>"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#key-takeaway","title":"Key Takeaway","text":"<p>Python can be used to quickly and accurately compute eigenvalues and eigenvectors, which is especially helpful for larger matrices or data sets.</p>"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#applications-in-data-science","title":"Applications in Data Science","text":""},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<p>PCA uses eigenvalues and eigenvectors of the covariance matrix to find directions of maximum variance in data, enabling dimensionality reduction while preserving as much information as possible.</p>"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#other-applications","title":"Other Applications","text":"<ul> <li>Image compression: Reduce image dimensions while maintaining quality</li> <li>Feature extraction: Identify important features in high-dimensional data</li> <li>Recommendation systems: Matrix factorization techniques</li> <li>Network analysis: Understanding graph structures</li> <li>Quantum mechanics: Fundamental in quantum computing</li> </ul>"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Eigenvalues (\u03bb) represent how much an eigenvector is scaled when multiplied by the matrix.</li> <li>Eigenvectors (v) are special vectors that don't change direction when multiplied by the matrix.</li> <li>Eigenvalues and eigenvectors satisfy: \\(A \\vec{v} = \\lambda \\vec{v}\\).</li> <li>These concepts are fundamental to many data science techniques, especially dimensionality reduction.</li> <li>Python's NumPy provides efficient computation of eigenvalues and eigenvectors.</li> </ol>"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#next-steps_1","title":"Next Steps","text":"<p>Now that you understand eigenvalues and eigenvectors, you can explore: - Machine learning techniques that use these concepts (PCA, SVD) - Advanced linear algebra topics - Applications in deep learning and neural networks</p>"},{"location":"Maths%20%26%20Statistics/06_linear_algebra_advanced/#conclusion","title":"Conclusion","text":"<p>Eigenvalues and eigenvectors are powerful mathematical tools that enable us to understand and transform data in meaningful ways. They form the foundation for many advanced data science techniques and are essential for anyone working with high-dimensional data.</p>"},{"location":"Maths%20%26%20Statistics/hypothesis_testing/","title":"Hypothesis Testing Quiz","text":"<ol> <li>What is the primary purpose of inferential statistics?</li> <li>A. To describe the main features of a dataset</li> <li>B. To draw conclusions about a population based on a sample</li> <li>C. To organize and summarize data</li> <li> <p>D. To determine the reliability of data</p> </li> <li> <p>Which of the following is an example of a parameter?</p> </li> <li>A. Sample mean</li> <li>B. Sample standard deviation</li> <li>C. Population mean</li> <li> <p>D. Sample variance</p> </li> <li> <p>Which statistical test is used to compare the means of two independent groups?</p> </li> <li>A. Paired t-test</li> <li>B. Independent t-test</li> <li>C. ANOVA</li> <li> <p>D. Chi-square test</p> </li> <li> <p>A Type I error occurs when:</p> </li> <li>A. A true null hypothesis is incorrectly rejected</li> <li>B. A false null hypothesis is incorrectly rejected</li> <li>C. A true null hypothesis is incorrectly accepted</li> <li> <p>D. A false null hypothesis is incorrectly accepted</p> </li> <li> <p>The p-value represents:</p> </li> <li>A. The probability that the null hypothesis is true</li> <li>B. The probability of observing the data, or something more extreme, assuming the null hypothesis is true</li> <li>C. The probability of making a Type I error</li> <li> <p>D. The power of the test</p> </li> <li> <p>Which of the following is true about the confidence interval?</p> </li> <li>A. It provides the exact value of the population parameter</li> <li>B. It indicates the range within which the sample mean will fall 95% of the time</li> <li>C. It provides a range of values within which the population parameter is likely to fall</li> <li> <p>D. It determines the sample size needed for a study</p> </li> <li> <p>What is the central limit theorem?</p> </li> <li>A. The distribution of sample means will be approximately normally distributed, regardless of the distribution of the population, given a sufficiently large sample size</li> <li>B. The sum of a large number of independent and identically distributed random variables will be approximately normally distributed</li> <li>C. The probability distribution of a continuous random variable will be normal if the sample size is large enough</li> <li> <p>D. The mean of a large sample will equal the mean of the population</p> </li> <li> <p>When should you use a chi-square test?</p> </li> <li>A. To compare the means of more than two groups</li> <li>B. To test the association between two categorical variables</li> <li>C. To compare the variances of two samples</li> <li> <p>D. To analyze the relationship between a continuous and a categorical variable</p> </li> <li> <p>If the 95% confidence interval for a mean difference includes zero, what does this imply?</p> </li> <li>A. The difference is statistically significant</li> <li>B. The null hypothesis cannot be rejected</li> <li>C. The sample size is too small</li> <li> <p>D. The p-value is less than 0.05</p> </li> <li> <p>The power of a statistical test is:</p> <ul> <li>A. The probability of rejecting the null hypothesis when it is true</li> <li>B. The probability of accepting the null hypothesis when it is false</li> <li>C. The probability of rejecting the null hypothesis when it is false</li> <li>D. The probability of making a Type I error</li> </ul> </li> <li> <p>In hypothesis testing, the null hypothesis is:</p> <ul> <li>A. A statement of no effect or no difference</li> <li>B. A statement that there is an effect or a difference</li> <li>C. Always the hypothesis that the researcher wants to prove</li> <li>D. Less important than the alternative hypothesis</li> </ul> </li> <li> <p>The sample statistic that is most commonly used to estimate a population parameter is called:</p> <ul> <li>A. Estimator</li> <li>B. Point estimate</li> <li>C. Confidence interval</li> <li>D. Hypothesis</li> </ul> </li> <li> <p>Which method is used to determine the sample size for a study?</p> <ul> <li>A. Power analysis</li> <li>B. T-test</li> <li>C. Chi-square test</li> <li>D. Correlation coefficient</li> </ul> </li> <li> <p>The F-test in ANOVA is used to:</p> <ul> <li>A. Test for the equality of two variances</li> <li>B. Test for the equality of three or more means</li> <li>C. Test for the equality of proportions</li> <li>D. Test for independence between two categorical variables</li> </ul> </li> <li> <p>Which of the following is true regarding the normal distribution?</p> <ul> <li>A. It is skewed to the right</li> <li>B. It is defined by its mean and standard deviation</li> <li>C. It can only take on positive values</li> <li>D. It is bimodal</li> </ul> </li> </ol>"},{"location":"Maths%20%26%20Statistics/hypothesis_testing/#correct-answers","title":"Correct Answers","text":"<ol> <li>B</li> <li>C</li> <li>B</li> <li>A</li> <li>B</li> <li>C</li> <li>A</li> <li>B</li> <li>B</li> <li>C</li> <li>A</li> <li>B</li> <li>A</li> <li>B</li> <li>B</li> </ol> What's on your mind? Put it in the comments!"},{"location":"Python/11_python_tutorial/","title":"Overview","text":"Meta Data title: Python Quick Review author: Juma Shafara date: \"2024-01\" date-modified: \"2024-08-14\" description: This crash course will teach you the basics and advanced concepts of Python Programming keywords: [python basics, variables, numbers, operators, containers, flow control, advanced, modules, file handling]  <p>Python is a great general-purpose programming language on its own, but with the help of a few popular libraries (numpy, scipy, matplotlib) it becomes a powerful environment for scientific computing.</p> <p>Whether you're a total beginner or seasoned programmer, this lesson will serve as a Python Quick Crash Course to refresh your Python knowledge</p> <p>In this tutorial, we will cover:</p> <p> Basic Python: Basic data types (Containers, Lists, Dictionaries, Sets, Tuples), Functions, Classes</p> <pre><code># checking python version\n!python --version\n</code></pre> <pre>\n<code>Python 3.12.3\n</code>\n</pre> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>Python is a high-level, dynamically typed multiparadigm programming language. Python code is often said to be almost like pseudocode, since it allows you to express very powerful ideas in very few lines of code while being very readable. As an example, here is an implementation of the classic quicksort algorithm in Python:</p> <pre><code>def quicksort(array):\n    if len(array) &amp;lt;= 1:\n        return array\n    pivot = array[len(array) // 2]\n    left = [number for number in array if number &amp;lt; pivot]\n    middle = [number for number in array if number == pivot]\n    right = [number for number in array if number &amp;gt; pivot]\n    return quicksort(left) + middle + quicksort(right)\n\nquicksort([3,6,8,10,1,2,1])\n</code></pre> <pre>\n<code>[1, 1, 2, 3, 6, 8, 10]</code>\n</pre> <pre><code>sorted('listen', reverse=True)\n</code></pre> <pre>\n<code>['t', 's', 'n', 'l', 'i', 'e']</code>\n</pre> <pre><code>name = 'Eva'\nlist_of_names = ['Eva', 'Shafara', 'Bob'] # snake case\n</code></pre> <p>we'll use camel case for function variables</p> <pre><code>def calculateBMI(weight_kg, height_m): # camel case\n    bmi = weight_kg / height_m ** 2\n    rounded_bmi = round(bmi, 3)\n    return rounded_bmi\n</code></pre> <p>finally, we'll use pascal case for class variables</p> <pre><code>class MathFunction:    # Pascal Case\n    def __init__(self, number):\n        self.number = number\n\n    def square(self):\n        return self.number ** 2\n\n    def cube(self):\n        return self.number ** 3\n</code></pre> <p>Integers and floats work as you would expect from other languages:</p> <pre><code>number = 3\nprint('Number: ', number)\nprint('Type: ', type(number))\n</code></pre> <pre>\n<code>Number:  3\nType:  &lt;class 'int'&gt;\n</code>\n</pre> <pre><code># Quick number arithmetics\n\nprint(number + 1)   # Addition\nprint(number - 1)   # Subtraction\nprint(number * 2)   # Multiplication\nprint(number ** 2)  # Enumberponentiation\n</code></pre> <pre>\n<code>4\n2\n6\n9\n</code>\n</pre> <pre><code># Some compound assingment operators\n\nnumber += 1 # number = number + 1\nprint(number)\nnumber *= 2\nprint(number)\nnumber /= 1 # number = number / 1\nprint(number)\nnumber -= 2\nprint(number)\n</code></pre> <pre>\n<code>4\n8\n8.0\n6.0\n</code>\n</pre> <pre><code>number = 2.5\nprint(type(number))\nprint(number, number + 1, number * 2, number ** 2)\n</code></pre> <pre>\n<code>&lt;class 'float'&gt;\n2.5 3.5 5.0 6.25\n</code>\n</pre> <pre><code># complex numbers\nvector = 2 + 6j\ntype(vector)\n</code></pre> <pre>\n<code>complex</code>\n</pre> <p>Python implements all of the usual operators for Boolean logic, but uses English words rather than symbols:</p> <pre><code>t, f = True, False\ntype(t)\n</code></pre> <pre>\n<code>bool</code>\n</pre> <p>Now we let's look at the operations:</p> <pre><code># Logical Operators\n\nprint(t and f) # Logical AND;\nprint(t or f)  # Logical OR;\nprint(not t)   # Logical NOT;\nprint(t != f)  # Logical XOR;\n</code></pre> <pre>\n<code>False\nTrue\nFalse\nTrue\n</code>\n</pre> <pre><code>hello = 'hello'   # String literals can use single quotes\nworld = \"world\"   # or double quotes; it does not matter\nprint(hello, len(hello))\n</code></pre> <pre>\n<code>hello 5\n</code>\n</pre> <pre><code># We can string in python\nfull = hello + ' ' + world  # String concatenation\nprint(full)\n</code></pre> <pre>\n<code>hello world\n</code>\n</pre> <pre><code>hw12 = '{} {} {}'.format(hello, world, 12)  # string formatting\nprint(hw12)\n</code></pre> <pre>\n<code>hello world 12\n</code>\n</pre> <pre><code>statement = 'I love to code in {}'\nmodified = statement.format('JavaScript')\nprint(modified)\n</code></pre> <pre>\n<code>I love to code in JavaScript\n</code>\n</pre> <pre><code># formatting by indexing\nstatement = '{0} loves to code in {2} and {1}'\nstatement.format('Juma', 'Python', 'JavaScript')\n</code></pre> <pre>\n<code>'Juma loves to code in JavaScript and Python'</code>\n</pre> <pre><code># formatting by name\nstatement = '{name} loves to code in {language1} and {language2}'\nstatement.format(language2='Python', name='Juma', language1='JavaScript')\n</code></pre> <pre>\n<code>'Juma loves to code in JavaScript and Python'</code>\n</pre> <pre><code># String Literal Interpolation\nname = 'Juma'\nlanguage1 = 'JavaScript'\nlanguage2 = 'Python'\n\nstatement = f'{name} loves to code in {language1} and {language2}'\n\nprint(statement)\n</code></pre> <pre>\n<code>Juma loves to code in JavaScript and Python\n</code>\n</pre> <p>String objects have a bunch of useful methods; for example:</p> <pre><code>string_ = \"hello\"\nprint(string_.capitalize())  # Capitalize a string\nprint(string_.upper())       # Convert a string to uppercase; prints \"HELLO\"\nprint(string_.rjust(7))      # Right-justify a string, padding with spaces\nprint(string_.center(7))     # Center a string, padding with spaces\nprint(string_.replace('l', '(ell)'))  # Replace all instances of one substring with another\nprint('  world '.strip())  # Strip leading and trailing whitespace\n</code></pre> <pre>\n<code>Hello\nHELLO\n  hello\n hello \nhe(ell)(ell)o\nworld\n</code>\n</pre> <pre><code>statement = 'i love to code in Python '\n\ncapitalized = statement.capitalize()\nupped = statement.upper()\nreplaced = statement.replace('Python', 'javascript')\nstatement.strip()\n</code></pre> <pre>\n<code>'i love to code in Python'</code>\n</pre> <p>You can find a list of all string methods in the documentation.</p> <ul> <li> Python containers (collections) are objects that we use to group other objects</li> <li> Python includes several built-in container types: lists, dictionaries, sets, and tuples.</li> </ul> <p>A list is an ordered collection of python objects or elements. A list can contain objects of different data types</p> <pre><code>list_of_numbers = [3, 1, 2]   # Create a list\nprint(list_of_numbers)\nprint(list_of_numbers[2])\nprint(list_of_numbers[-1])     # Negative indices count from the end of the list; prints \"2\"\n</code></pre> <pre>\n<code>[3, 1, 2]\n2\n2\n</code>\n</pre> <pre><code>list_of_numbers[2] = 'foo'    # replacing a specific value in a list\nprint(list_of_numbers)\n</code></pre> <pre>\n<code>[3, 1, 'foo']\n</code>\n</pre> <pre><code>list_of_numbers.append('bar') # Add a new element to the end of the list\nprint(list_of_numbers)  \n</code></pre> <pre>\n<code>[3, 1, 'foo', 'bar']\n</code>\n</pre> <pre><code>last_item = list_of_numbers.pop()     # Remove and return the last element of the list\nprint(last_item)    # returns the last item \nprint(list_of_numbers) # Modifies the original list\n</code></pre> <pre>\n<code>bar\n[3, 1, 'foo']\n</code>\n</pre> <p>Research on:</p> <ul> <li> `del` </li> <li> `remove()`</li> </ul> <p>As usual, you can find all the gory details about lists in the documentation.</p> <p>In addition to accessing list elements one at a time, Python provides concise syntax to access a range of values in a list; this is known as slicing:</p> <pre><code>list_of_numbers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint(list_of_numbers)\n</code></pre> <pre>\n<code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n</code>\n</pre> <pre><code>print(list_of_numbers)         # Prints \"[0, 1, 2, 3, 4]\"\nprint(list_of_numbers[2:4])    # Get a slice from index 2 to 4 (exclusive); prints \"[2, 3]\"\nprint(list_of_numbers[2:])     # Get a slice from index 2 to the end; prints \"[2, 3, 4]\"\nprint(list_of_numbers[:2])     # Get a slice from the start to index 2 (exclusive); prints \"[0, 1]\"\nprint(list_of_numbers[:])      # Get a slice of the whole list; prints [\"0, 1, 2, 3, 4]\"\nprint(list_of_numbers[:-1])    # Slice indices can be negative; prints [\"0, 1, 2, 3]\"\nlist_of_numbers[2:4] = [8, 9] # Assign a new sublist to a slice\nprint(list_of_numbers)         # Prints \"[0, 1, 8, 9, 4]\"\n</code></pre> <pre>\n<code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[2, 3]\n[2, 3, 4, 5, 6, 7, 8, 9]\n[0, 1]\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[0, 1, 2, 3, 4, 5, 6, 7, 8]\n[0, 1, 8, 9, 4, 5, 6, 7, 8, 9]\n</code>\n</pre> <p>You can loop over the elements of a list like this:</p> <pre><code>list_of_animals = ['cat', 'dog', 'monkey']\n\nfor animal in list_of_animals:\n    print(animal)\n</code></pre> <pre>\n<code>cat\ndog\nmonkey\n</code>\n</pre> <pre><code>list_of_numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]\nlist_of_squared_numbers = []\n\nfor number in list_of_numbers:\n    list_of_squared_numbers.append(pow(number, 2))\n\nlist_of_squared_numbers\n</code></pre> <pre>\n<code>[1, 4, 9, 16, 25, 36, 49, 64, 81, 0]</code>\n</pre> <p>If you want access to the index of each element within the body of a loop, use the built-in <code>enumerate</code> function:</p> <pre><code>animals = ['cat', 'dog', 'monkey']\n\nfor index, animal in enumerate(animals):\n    print(f'{index}: {animal}')\n</code></pre> <pre>\n<code>0: cat\n1: dog\n2: monkey\n</code>\n</pre> <pre><code>numbers = [0, 1, 2, 3, 4]\nsquares = []\n\nfor number in numbers:\n    squares.append(pow(number, 2))\n\nprint(squares)\n</code></pre> <pre>\n<code>[0, 1, 4, 9, 16]\n</code>\n</pre> <p>You can make this code simpler using a list comprehension:</p> <pre><code>list_of_numbers = [0, 1, 2, 3, 4]\n\nsquares = [pow(number, 2) for number in list_of_numbers]\n\nprint(squares)\n</code></pre> <pre>\n<code>[0, 1, 4, 9, 16]\n</code>\n</pre> <p>List comprehensions can also contain conditions:</p> <pre><code>numbers = [0, 1, 2, 3, 4]\n\neven_squares = [pow(number, 2) for number in numbers if number % 2 == 0]\n\nprint(even_squares)\n</code></pre> <pre>\n<code>[0, 4, 16]\n</code>\n</pre> <p>Research:  How to combine lists</p> <pre><code># creating a dictionary\nperson = {\n    'first_name': 'Juma',\n    'last_name': 'Shafara',\n    'age': 51,\n    'married': True\n}\nperson\n</code></pre> <pre>\n<code>{'first_name': 'Juma', 'last_name': 'Shafara', 'age': 51, 'married': True}</code>\n</pre> <pre><code># accessing items in a dictionary\nfirst_name = person['first_name']\nlast_name = person['last_name']\nfull_name = first_name + ' ' + last_name\n\n# display\nfull_name\n</code></pre> <pre>\n<code>'Juma Shafara'</code>\n</pre> <pre><code># add items to a dictionary\nperson['hobby'] = 'Coding'\nperson\n</code></pre> <pre>\n<code>{'first_name': 'Juma',\n 'last_name': 'Shafara',\n 'age': 51,\n 'married': True,\n 'hobby': 'Coding'}</code>\n</pre> <pre><code>email = person.get('email', 'email not available')\nprint(email)\n</code></pre> <pre>\n<code>email not available\n</code>\n</pre> <pre><code># modifying a value in a dictionay\nperson['married'] = False\nperson\n</code></pre> <pre>\n<code>{'first_name': 'Juma',\n 'last_name': 'Shafara',\n 'age': 51,\n 'married': False,\n 'hobby': 'Coding'}</code>\n</pre> <pre><code># remove an item from a dictionary\nperson.pop('age')\nperson\n</code></pre> <pre>\n<code>{'first_name': 'Juma',\n 'last_name': 'Shafara',\n 'married': False,\n 'hobby': 'Coding'}</code>\n</pre> <p>Research:</p> <ul> <li> How to remove an item using the `del` method</li> <li> How to iterate over objects in a dictionary</li> <li> Imitate list comprehension with dictionaries</li> </ul> <p>You can find all you need to know about dictionaries in the documentation.</p> <ul> <li> A set is an unordered, immutable collection of distinct elements. </li> <li> A set is created using curly braces</li> <li> The objects are placed inside the brackets and are separated by commas</li> <li> As a simple example, consider the following:</li> </ul> <pre><code>animals = {'cat', 'dog'}\n\nprint('cat' in animals)   # Check if an element is in a set; prints \"True\"\nprint('fish' not in animals)  # prints \"True\"\n</code></pre> <pre>\n<code>True\nTrue\n</code>\n</pre> <pre><code>animals.add('fish')      # Add an element to a set\n\nprint('fish' in animals) # Returns \"True\"\n\nprint(len(animals))       # Number of elements in a set;\n</code></pre> <pre>\n<code>True\n3\n</code>\n</pre> <pre><code>animals.add('cat')       # Adding an element that is already in the set does nothing\nprint(len(animals)) \n\nanimals.remove('cat')    # Remove an element from a set\nprint(len(animals))       \n</code></pre> <pre>\n<code>3\n2\n</code>\n</pre> <p>Research:</p> <ul> <li> How to remove with `discard()`</li> <li> How to remove with `pop()`</li> <li> How to combine sets/li&gt;  <li> How to get the difference between 2 sets</li> <li> What happens when we have repeated elements in a set</li> </li> </ul> <p>Loops: Iterating over a set has the same syntax as iterating over a list; however since sets are unordered, you cannot make assumptions about the order in which you visit the elements of the set:</p> <pre><code>animals = {'cat', 'dog', 'fish'}\n\nfor index, animal in enumerate(animals):\n    print(f'{index}: {animal}')\n</code></pre> <pre>\n<code>0: fish\n1: cat\n2: dog\n</code>\n</pre> <p>Set comprehensions: Like lists and dictionaries, we can easily construct sets using set comprehensions:</p> <pre><code>from math import sqrt\n\nprint({int(sqrt(x)) for x in range(30)})\n</code></pre> <pre>\n<code>{0, 1, 2, 3, 4, 5}\n</code>\n</pre> <ul> <li> A tuple is an (immutable) ordered list of values. </li> <li> A tuple is in many ways similar to a list; one of the most important differences is that tuples can be used as keys in dictionaries and as elements of sets, while lists cannot. Here is a trivial example:</li> </ul> <pre><code>d = {(x, x + 1): x for x in range(10)}  # Create a dictionary with tuple keys\nt = (5, 6)       # Create a tuple\nprint(type(t))\nprint(d[t])       \nprint(d[(1, 2)])\n</code></pre> <pre>\n<code>&lt;class 'tuple'&gt;\n5\n1\n</code>\n</pre> <pre><code># t[0] = 1\n</code></pre> <p>Research:</p> <ul> <li> Creating a tuple</li> <li> Access items in a tuple</li> <li> Negative indexing tuples</li> <li> Using range of indexes</li> <li> Getting the length of items in a tuple</li> <li> Looping through a tuple</li> <li> Checking if an item exists in a tuple</li> <li> How to combine tuples</li> <li> Prove that tuples are immutable</li> </ul> <p> A function is a group of statements that performs a particular task</p> <p> Python functions are defined using the <code>def</code> keyword. For example:</p> <pre><code>def overWeightOrUnderweightOrNormal(weight_kg:float, height_m:float) -&amp;gt; str:\n    '''\n    Tells whether someone is overweight or underweight or normal\n    '''\n    height_m2 = pow(height_m, 2)\n    bmi = weight_kg / height_m2\n    rounded_bmi = round(bmi, 3)\n    if bmi &amp;gt; 24:\n        return 'Overweight'\n    elif bmi &amp;gt; 18:\n        return 'Normal'\n    else:\n        return 'Underweight'\n\noverWeightOrUnderweightOrNormal(67, 1.7)\n</code></pre> <pre>\n<code>'Normal'</code>\n</pre> <p>We will often use functions with optional keyword arguments, like this:</p> <pre><code>bmi = calculateBMI(height_m=1.7, weight_kg=67)\n\nprint(bmi)\n</code></pre> <pre>\n<code>23.183\n</code>\n</pre> <pre><code>def greet(name:str='You')-&amp;gt;str:\n    \"\"\"\n    This function greets people by name\n    Example1:\n    &amp;gt;&amp;gt;&amp;gt; greet(name='John Doe')\n    &amp;gt;&amp;gt;&amp;gt; 'Hello John Doe'\n    Example2:\n    &amp;gt;&amp;gt;&amp;gt; greet()\n    &amp;gt;&amp;gt;&amp;gt; 'Hello You'\n    \"\"\"\n    return f'Hello {name}'\n\n# greet('Eva')\n?greet\n</code></pre> <pre>\n<code>Signature: greet(name: str = 'You') -&gt; str\nDocstring:\nThis function greets people by name\nExample1:\n&gt;&gt;&gt; greet(name='John Doe')\n&gt;&gt;&gt; 'Hello John Doe'\nExample2:\n&gt;&gt;&gt; greet()\n&gt;&gt;&gt; 'Hello You'\nFile:      /tmp/ipykernel_25670/2049930273.py\nType:      function</code>\n</pre> <ul> <li> In python, everything is an object</li> <li> We use classes to help us create new object</li> <li> The syntax for defining classes in Python is straightforward:</li> </ul> <pre><code>class Person:\n    first_name = 'John'\n    last_name = 'Tong'\n    age = 20\n</code></pre> <pre><code># Instantiating a class\nobject1 = Person()\n\nprint(object1.first_name)\nprint(object1.last_name)\nprint(object1.age)\n\nprint(f'object1 type: {type(object1)}')\n</code></pre> <pre>\n<code>John\nTong\n20\nobject1 type: &lt;class '__main__.Person'&gt;\n</code>\n</pre> <pre><code># Instantiating a class\nobject2 = Person()\n\nprint(object2.first_name)\nprint(object2.last_name)\nprint(object2.age)\n</code></pre> <pre>\n<code>John\nTong\n20\n</code>\n</pre> <pre><code>class Person:\n    def __init__(self, first_name, last_name, age):\n        self.first_name = first_name\n        self.last_name = last_name\n        self.age = age\n\n    def greet(self, name):\n        return f'Hello {name}'\n</code></pre> <pre><code>object1 = Person('Juma', 'Shafara', 24)\nprint(object1.first_name)\nprint(object1.last_name)\nprint(object1.age)\nprint(type(object1))\n</code></pre> <pre>\n<code>Juma\nShafara\n24\n&lt;class '__main__.Person'&gt;\n</code>\n</pre> <pre><code>object2 = Person('Eva', 'Ssozi', 24)\nprint(object2.first_name)\nprint(object2.last_name)\nprint(object2.age)\nprint(object2.greet('Shafara'))\nprint(type(object2))\n</code></pre> <pre>\n<code>Eva\nSsozi\n24\nHello Shafara\n&lt;class '__main__.Person'&gt;\n</code>\n</pre> <pre><code>class Student(Person):\n    def __init__(self, first_name, last_name, age, id_number, subjects=[]):\n        super().__init__(first_name, last_name, age)\n        self.id_number = id_number\n        self.subjects = subjects\n\n    def addSubject(self, subject):\n        self.subjects.append(subject) \n</code></pre> <pre><code>student1 = Student('Calvin', 'Masaba', 34, '200045', ['math', 'science'])\n</code></pre> <pre><code>student1.addSubject('english')\n</code></pre> <pre><code>student1.subjects\n</code></pre> <pre>\n<code>['math', 'science', 'english']</code>\n</pre> <p>Research:</p> <p> Inheritance: This allows to create classes that inherit the attributes and methods of another class</p>"},{"location":"Python/11_python_tutorial/#introduction","title":"Introduction","text":""},{"location":"Python/11_python_tutorial/#a-brief-note-on-python-versions","title":"A Brief Note on Python Versions","text":"<p>We'll be using Python 3.10 for this iteration of the course. You can check your Python version at the command line by running <code>python --version</code>.</p>"},{"location":"Python/11_python_tutorial/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/11_python_tutorial/#basics-of-python","title":"Basics of Python","text":""},{"location":"Python/11_python_tutorial/#variables","title":"Variables","text":"<p>Variables are stores of value, \"Juma\" will be stored and become accessible from <code>name</code></p> Python Variables Example<pre><code>name = 'Juma'\nage = 19\nid_number = 190045\n</code></pre>"},{"location":"Python/11_python_tutorial/#rules-to-consider","title":"Rules to consider","text":"<ul> <li> Variable names should be meaningful eg \"number\" instead of \"x\"</li> <li> Variable names should contain only alpha-numberic characters, and maybe under_scores</li> <li> Variable names can only start with letters or an underscore</li> <li> Variable name cannot contain special characters</li> <li> Variables names are case sensitive</li> </ul>"},{"location":"Python/11_python_tutorial/#examples-of-variables","title":"Examples of variables","text":"<p>For this course, we'll use snake case for quick variables</p>"},{"location":"Python/11_python_tutorial/#basic-data-types","title":"Basic data types","text":""},{"location":"Python/11_python_tutorial/#numbers","title":"Numbers","text":""},{"location":"Python/11_python_tutorial/#booleans","title":"Booleans","text":""},{"location":"Python/11_python_tutorial/#strings","title":"Strings","text":"<p>A string is a sequence of characters under some quotes. Eg.</p>"},{"location":"Python/11_python_tutorial/#containers","title":"Containers","text":""},{"location":"Python/11_python_tutorial/#lists","title":"Lists","text":""},{"location":"Python/11_python_tutorial/#slicing","title":"Slicing","text":""},{"location":"Python/11_python_tutorial/#loops","title":"Loops","text":"<p>A <code>for loop</code> is used to loop through (or iterate) over a sequence of objects (iterable objects). Iterable objects in python include strings, lists, sets etc </p>"},{"location":"Python/11_python_tutorial/#list-comprehensions","title":"List comprehensions:","text":""},{"location":"Python/11_python_tutorial/#dictionaries","title":"Dictionaries","text":"<ul> <li> A dictionary is an unordered and mutable collection of items</li> <li> A dictionary is created using curly brackets</li> <li> Each item in a dictionary contains a key/value pair</li> </ul>"},{"location":"Python/11_python_tutorial/#sets","title":"Sets","text":""},{"location":"Python/11_python_tutorial/#tuples","title":"Tuples","text":""},{"location":"Python/11_python_tutorial/#functions","title":"Functions","text":""},{"location":"Python/11_python_tutorial/#classes","title":"Classes","text":""},{"location":"Python/11_python_tutorial/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Basics/00_python_programming_outline/","title":"Outline","text":"<p>title: Python3 Outline author: Juma Shafara date: \"2023-11\" description: Python 3 Beginner Course Outline keywords: [Python Course Outline]</p> <p></p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Basics/00_python_programming_outline/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Basics/00_python_programming_outline/#basics","title":"Basics","text":"<ul> <li>Python Overview</li> <li>Introduction</li> <li>Installing</li> <li>Writing Code</li> <li>Displaying Output</li> <li>Statements</li> <li>Syntax</li> <li>Comments</li> <li>Exercise</li> </ul> <p>Get Started </p>"},{"location":"Python/Basics/00_python_programming_outline/#variables","title":"Variables","text":"<ul> <li>Variables</li> <li>Data Types</li> <li>Numbers</li> <li>Number Methods</li> <li>Strings</li> <li>Type Conversion</li> <li>Python Booleans</li> <li>Exercise</li> </ul> <p>Get Started </p>"},{"location":"Python/Basics/00_python_programming_outline/#operations","title":"Operations","text":"<ul> <li>Operators Intro</li> <li>Arithmetics</li> <li>Assignment</li> <li>Comparison</li> <li>Logical</li> <li>Identity</li> <li>Membership</li> <li>Exercise</li> </ul> <p>Get Started </p>"},{"location":"Python/Basics/00_python_programming_outline/#collections","title":"Collections","text":"<ul> <li>Containers</li> <li>List</li> <li>Tuple</li> <li>Set</li> <li>Dictionary</li> <li>Exercise</li> </ul> <p>Get Started </p>"},{"location":"Python/Basics/00_python_programming_outline/#flow-control","title":"Flow Control","text":"<ul> <li>Functions</li> <li>Lambda functions</li> <li>If else</li> <li>If else shorthand</li> <li>For Loop</li> <li>While Loop</li> <li>Break and Continue</li> <li>Pass</li> <li>Exercise</li> </ul> <p>Get Started </p>"},{"location":"Python/Basics/00_python_programming_outline/#advanced","title":"Advanced","text":"<ul> <li>Functions</li> <li>Classes and Objects</li> <li>Inheritance</li> <li>Variable Scope</li> <li>Formatting Strings</li> <li>Try \u2026 Except</li> <li>Iterators</li> <li>User Input</li> <li>Exercise</li> </ul> <p>Get Started </p>"},{"location":"Python/Basics/00_python_programming_outline/#modules","title":"Modules","text":"<ul> <li>Intro</li> <li>Math</li> <li>Random</li> <li>Date and Time</li> <li>JSON</li> <li>Regular Expressions</li> <li>Exercise</li> </ul> <p>Get Started </p>"},{"location":"Python/Basics/00_python_programming_outline/#working-with-files","title":"Working With Files","text":"<ul> <li>Intro</li> <li>File Handling</li> <li>File Reading</li> <li>File Writing/Creating/Appending</li> <li>File Deleting</li> <li>Exercise</li> </ul> <p>Get Started </p>"},{"location":"Python/Basics/00_python_programming_outline/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Basics/01_basics/","title":"Introduction","text":"<p>title: Python Basics keywords: [What is Python, What is Python used for?, Python Displaying output] description: In this notebook, I want to observe any trends related to customers. author: Juma Shafara date: \"2023-09\" date-modified: \"2024-11-25\"</p> <p></p> <p>This course will teach you the basics and advanced concepts of Python Programming. </p> <p>What do you need before learning Python?</p> <ol> <li>Computer Literacy</li> <li>Knowledge of installing a software</li> <li>A compiler</li> </ol> <pre><code>x = 4\ny = 3\nsumm = x + y\nprint(summ)\n</code></pre> <pre>\n<code>7\n</code>\n</pre> <p>Although we have not taught you how to code in Python yet, you can still easily pick up what the code is doing</p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>To display an output in Python, use the <code>print()</code> function.</p> <pre><code>print('Hello world!')\n</code></pre> <pre>\n<code>Hello world!\n</code>\n</pre> <pre><code>print(27)\n</code></pre> <pre>\n<code>27\n</code>\n</pre> <pre><code>print(3 + 27)\n</code></pre> <pre>\n<code>30\n</code>\n</pre> <pre><code>print('Hello', 'Juma')\n</code></pre> <pre>\n<code>Hello Juma\n</code>\n</pre> <pre><code>x = 3\ny = 7\nsumm = x + y\nprint('the sum is ', summ)\n</code></pre> <pre>\n<code>the sum is  10\n</code>\n</pre> <pre><code>x = 4; y = 3; print(x + y)\n</code></pre> <pre>\n<code>7\n</code>\n</pre> <pre><code>x = 5\ny = 3\nsumm = x + y\nprint(summ)\n</code></pre> <pre>\n<code>8\n</code>\n</pre> <p>In the example above, we have 4 lines of code. In python, each line typically contains one statement</p> <pre><code>a = 4; b = 3; sum = a + b; print(sum) \n</code></pre> <pre>\n<code>7\n</code>\n</pre> <pre><code>number1 = 4\nnumber2 = 3\n\nif number1 &amp;gt; number2:\n  x = 'Hello, world'\n  print(x)\n</code></pre> <pre>\n<code>Hello, world\n</code>\n</pre> <pre><code># this is a comment\nx = 4 \ny = 3\n\n# some comment\n# second comment\n# third comment\n\nprint(x + y) # prints out the sum\n</code></pre> <pre>\n<code>7\n</code>\n</pre> <p>Write a Python program to display the following text on the screen.</p> <pre><code>DATAIDEA\nSir Apollo Kagwa Road,\nKampala,\nUganda\n-------------------\nwww.dataidea.org\n</code></pre>"},{"location":"Python/Basics/01_basics/#overview","title":"Overview","text":""},{"location":"Python/Basics/01_basics/#table-of-contents","title":"Table of Contents","text":"<ul> <li> Python Overview</li> <li> Introduction</li> <li> Installing</li> <li> Writing Code</li> <li> Displaying Output</li> <li> Statements</li> <li> Syntax</li> <li> Comments</li> <li> Exercise</li> </ul> <p>You can watch the full crash course on our YouTube channel Programming for Data Science </p>"},{"location":"Python/Basics/01_basics/#prerequisites","title":"Prerequisites","text":""},{"location":"Python/Basics/01_basics/#python-is-easy","title":"Python is Easy","text":"<ul> <li> To learn Python, you don't need any prior knowledge of experience on programming.</li> <li> Python is human readable, making it easier to understand.</li> <li>  Take alook at this example</li> </ul>"},{"location":"Python/Basics/01_basics/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Basics/01_basics/#what-is-python","title":"What is Python","text":"<p> Python is a programming language.</p> <p> Python is one of the most popular programming languages</p>"},{"location":"Python/Basics/01_basics/#who-created-python","title":"Who created Python?","text":"<p>Python was created by Guido van Rossum and it was first implemented in 1989</p>"},{"location":"Python/Basics/01_basics/#what-is-python-used-for","title":"What is Python used for?","text":"<p>Python is used for: 1. Web Development 2. Machine Learning 3. Data Science 4. Scripting 5. And many more</p>"},{"location":"Python/Basics/01_basics/#what-is-the-latest-version-of-python","title":"What is the latest version of Python?","text":"<ul> <li> Python 3 is the latest version of Python</li> <li> This tutorial is based on the standards of Python 3</li> </ul>"},{"location":"Python/Basics/01_basics/#installing-python","title":"Installing Python","text":"<p>Before you can run Python on your PC, you need to install it first. </p> <p>To install Python in a PC, go to https://www.python.org/downloads/ then download the latest version.</p> <p>After that, install it just like how you install other apps.</p> <p>Make sure that you check \"Add Python to PATH\" for easier installation.</p>"},{"location":"Python/Basics/01_basics/#writing-python-code","title":"Writing Python Code","text":"<p>In order to learn Python, you need to be able to write and execute code.</p>"},{"location":"Python/Basics/01_basics/#python-console-shell","title":"Python Console (Shell)","text":"<p>Python console also known as shell allows you to execute Python code line by line</p> <p>Assuming that you have already installed Python on your PC, you can access the Python console by opening the command prompt and typing <code>python</code></p> <p>Let's start using the console</p> <p>Type the following and hit enter</p> <p><pre><code>name = 'Juma Shafara'\n</code></pre> Again, type the following and hit enter <pre><code>print(name)\n</code></pre> After that, you should see this <pre><code>Juma Shafara\n</code></pre></p> <p></p>"},{"location":"Python/Basics/01_basics/#python-files","title":"Python Files","text":"<p>Python files are saved with <code>.py</code> file extension</p> <p>You can use any text editor (even notepad) to create Python files</p> <p>Just make sure that you save them with the <code>.py</code> extension, forexample <code>hello.py</code>.</p> <p>Copy this code and save it as <code>hello.py</code>: <pre><code>print('Hello World!')\n</code></pre> To run this Python file on a PC, navigate to the folder where is is located using the command prompt.</p> <p>Then type the following and hit enter <pre><code>python hello.py\n</code></pre> The console should then output: <pre><code>Hello World!\n</code></pre></p>"},{"location":"Python/Basics/01_basics/#integrated-development-enviroment","title":"Integrated Development Enviroment","text":"<p>To continue practicing Python smoothly, I advice using an Integrated Development Environment. This is just a software that has features built in for you to get moving with your coding practice with ease. </p> <p>Examples include:</p> <ul> <li> PyCharm</li> <li> Jupyter Notebook</li> <li> Thonny</li> <li> Visual Studio Code</li> </ul> <p>Watch this video to see how to set up Visual Studio Code for Python Programming</p>"},{"location":"Python/Basics/01_basics/#python-displaying-output","title":"Python Displaying output","text":""},{"location":"Python/Basics/01_basics/#printing-two-objects","title":"Printing two objects","text":"<p>The <code>print()</code> function can be used to print two objects. Eg.</p>"},{"location":"Python/Basics/01_basics/#python-statements","title":"Python Statements","text":"<p>A python statement is used to write a value, compute a value, assign a value to a variable, call a functino and many more. Eg.</p>"},{"location":"Python/Basics/01_basics/#multiple-statements-in-one-line","title":"Multiple statements in one line","text":"<p>You can also write multiple statements in a single of code. Simply separate the statements with semicolons <code>;</code></p>"},{"location":"Python/Basics/01_basics/#python-syntax","title":"Python Syntax","text":"<p>When coding in Python, we follow a syntax. Syntax is the set of rules followed when writing programs</p>"},{"location":"Python/Basics/01_basics/#python-indentation","title":"Python indentation","text":"<ul> <li> In python, indentation indicates a block(group) of statements</li> <li> Tabs or leading whitespaces are used to compute the indentation level of a line</li> <li> It depends on you whether you want to use tabs or whitespaces, in the example below, we use 2 whitespaces</li> </ul>"},{"location":"Python/Basics/01_basics/#python-comments","title":"Python Comments","text":"<ul> <li>Comments in Python are used to clarify or explain codes</li> <li>Comments are not interpreted by Python, meaning comments will not be executed</li> </ul>"},{"location":"Python/Basics/01_basics/#excercise","title":"Excercise","text":""},{"location":"Python/Basics/01_basics/#end-of-first-module","title":"End of first module","text":"<p>The nice introduction ends here, in the next section, we will look at variables in Python.</p>"},{"location":"Python/Basics/01_basics/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Basics/02_variables/","title":"Variables","text":"<p>title: Python Variables author: Juma Shafara date: \"2023-11\" date-modified: \"2024-08-14\" description: What are variables and how do we use them in Python? keywords: [python, python programming, python variables]</p> <p></p> <ul> <li> Varibles are used to store values.</li> <li> To create a variable, use the equal sign `(=)`.</li> </ul> <p>In the examples below, we create varibales name <code>fruit</code> and <code>name</code> and we assign them values <code>'mango'</code> and <code>'viola'</code> respectively</p> <pre><code>fruit = 'mango'\nname = 'voila'\n\n# printing out\nprint(name, ' likes ', fruit )\n</code></pre> <pre>\n<code>voila  likes  mango\n</code>\n</pre> <p>Examples of good variable names</p> <pre><code>character = 'Peter Griffin'\nmy_favorite_character = 'Stewie Griffin' # snake case\nmyFavoriteCharacter = 'Meg Griffin' # camel case\nMyFavoriteCharacter = 'Brian Griffin' # Pascal case\n</code></pre> <pre><code># python numbers\n# Integers\nage = 45\npopulation = 45000000\n</code></pre> <pre><code># Floating point numbers\nheight = 1.7\nweight = 147.45\n</code></pre> <p>Find More on Numbers Here </p> <pre><code># Strings\nname = 'Juma'\nother_name = \"Masaba Calvin\"\nstatement = 'I love coding'\n</code></pre> <p>Find More on Strings Here </p> <pre><code>print(5 == 5)\nprint(10 &amp;gt; 5)\nprint(20 &amp;lt; 10)\n</code></pre> <pre>\n<code>True\nTrue\nFalse\n</code>\n</pre> <p>These are often used in <code>if</code> statments.</p> <p>In this example, if the value of the variable <code>age</code> is more than <code>18</code>, the program will tell the user that they are allowed to enter</p> <pre><code>age = 19\n\nif age &amp;gt; 18:\n    print('You are allowed to enter.')\n</code></pre> <pre>\n<code>You are allowed to enter.\n</code>\n</pre>  Note!<p>You will learn more about if statements later in the course</p> <pre><code>married = True\nprint(type(married))\n</code></pre> <pre>\n<code>&lt;class 'bool'&gt;\n</code>\n</pre> <pre><code># Lists\nnames = ['juma', 'john', 'calvin']\nprint(names)\n</code></pre> <pre>\n<code>['juma', 'john', 'calvin']\n</code>\n</pre> <p>Find More on Lists Here </p> <p>A list can contain mixed data types</p> <pre><code>other_stuff = ['mango', True, 38]\nprint(other_stuff)\n</code></pre> <pre>\n<code>['mango', True, 38]\n</code>\n</pre> <pre><code># Which data type\nprint(type(names))\n</code></pre> <pre>\n<code>list</code>\n</pre> <pre><code># convert an integer to a string\nage = 45\nmessage = 'Peter Griffin is '+ str(age) + ' years old'\nprint(message)\n</code></pre> <pre>\n<code>Peter Griffin is 45 years old\n</code>\n</pre> <pre><code># Convert floating point to integer\npi = 3.14159\nprint(int(pi))\n</code></pre> <pre>\n<code>3\n</code>\n</pre> <pre><code>side = 4\nprint(float(side))\n</code></pre> <pre>\n<code>4.0\n</code>\n</pre>"},{"location":"Python/Basics/02_variables/#variables","title":"Variables","text":"<p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Basics/02_variables/#table-of-contents","title":"Table of Contents","text":"<ul> <li> Variables</li> <li> Data Types</li> <li> Numbers</li> <li> Number Methods</li> <li> Strings</li> <li> Type Conversion</li> <li> Python Booleans</li> <li> Exercise</li> </ul>"},{"location":"Python/Basics/02_variables/#rules-to-consider","title":"Rules to consider","text":"<p>Before choosing a variable name, consider the following.</p> <ul> <li> Spaces are not allowed in a variable name eg `my age = 34`</li> <li> A variable name can not start with number eg `1name = 'Chris'`</li> <li> Variable names can not have special characters eg `n@me = 'Comfort'`</li> <li> Are case sensitive. Ie Name and name are not the same!</li> </ul>"},{"location":"Python/Basics/02_variables/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Basics/02_variables/#data-types","title":"Data Types","text":"<p>In this section of the tutorial, you will learn the most basic data types in Python</p>"},{"location":"Python/Basics/02_variables/#numbers","title":"Numbers","text":"<p>These are two basic types of numbers and they are called:</p> <ul> <li> integer(numbers without decimal places)</li> <li> floating point numbers(numbers with decimal places)</li> </ul>"},{"location":"Python/Basics/02_variables/#strings","title":"Strings","text":"<p>Strings are simply text. A string must be surrounded by single or double quotes</p>"},{"location":"Python/Basics/02_variables/#booleans","title":"Booleans","text":"<p>Boolean data type can only have on fo these values: <code>True</code> or <code>False</code></p> Note!<p>The first letter of a Boolean is in upper case.</p> <p>Booleans are often the result of evaluated expressions.</p> <p>Forexample, when you compare two numbers, Python evaluates the expression and returns either <code>True</code> or <code>False</code></p>"},{"location":"Python/Basics/02_variables/#checking-the-type-of-a-boolean","title":"Checking the Type of a Boolean","text":"<p>We can check the data type of a Boolean variable using the <code>type()</code> method.</p>"},{"location":"Python/Basics/02_variables/#lists","title":"Lists","text":"<ul> <li> A list is an ordered collection of data</li> <li> It can contain strings, numbers or even other lists</li> <li> Lists are written with square brackets (`[]`)</li> <li> The values in lists (also called elements) are separated by commas (`,`)</li> </ul>"},{"location":"Python/Basics/02_variables/#checking-data-types","title":"Checking data types","text":"<p>To check the data type of an object in python, use <code>type(object)</code>, for example, below we get the data type of the object stored in names</p>"},{"location":"Python/Basics/02_variables/#converting-types","title":"Converting Types","text":""},{"location":"Python/Basics/02_variables/#convert-to-string","title":"Convert to String","text":"<p>The <code>str()</code> method returns the string version of a given object</p>"},{"location":"Python/Basics/02_variables/#convert-to-integer","title":"Convert to Integer","text":"<p>The <code>int()</code> method returns the integer version of the given object.</p>"},{"location":"Python/Basics/02_variables/#convert-to-float","title":"Convert to Float","text":"<p>The <code>float()</code> method returns the floating point version of the given object.</p>"},{"location":"Python/Basics/02_variables/#exercise","title":"Exercise","text":"<p>Write a program to assign the number 34.5678 to a variable named \u201cnumber\u201d then display the number rounded to the nearest integer value.</p>"},{"location":"Python/Basics/02_variables/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Basics/03_numbers/","title":"Numbers","text":"<p>title: Python Numbers author: Juma Shafara date: \"2023-11\" date-modified: '2024-11-25' description: What are the different types of Python Numbers keywords: [python numbers, number types, number arithmetics, number methods]</p> <p></p> <p>In python, there are three types of numbers</p> <ul> <li> Integer - `int`</li> <li> Floating Point - `float`</li> <li> Complex - `complex`</li> </ul> <pre><code># Python Numbers: intgers\n\na = 3\nb = 4\nnumber = 5\n\nprint('a:', a)\nprint('b:', b)\nprint('number:', number)\n</code></pre> <pre>\n<code>a: 3\nb: 4\nnumber: 5\n</code>\n</pre> <pre><code># Python Numbers: floating point\na = 3.0\nb = 4.21\nnumber = 5.33\n\nprint('a:', a)\nprint('b:', b)\nprint('number:', number)\n</code></pre> <pre>\n<code>a: 3.0\nb: 4.21\nnumber: 5.33\n</code>\n</pre> <pre><code># Python Numbers: complex\n\na = 3j\nb = 5.21j\nnumber = 4 + 5.33j\n\nprint('a:', a)\nprint('b:', b)\nprint('number:', number)\n</code></pre> <pre>\n<code>a: 3j\nb: 5.21j\nnumber: (4+5.33j)\n</code>\n</pre> <pre><code># Python Numbers: arthmetics\n\nsummation = 4 + 2\nprint('sum:', summation)\n\ndifference = 4 - 2\nprint('difference:', difference)\n\nproduct = 4 * 2\nprint('product:', product)\n\nquotient = 4 / 2\nprint('quotient:', quotient)\n</code></pre> <pre>\n<code>sum: 6\ndifference: 2\nproduct: 8\nquotient: 2.0\n</code>\n</pre> <pre><code># sum() can add many numbers at once\nsummation = sum([1,2,3,4,5,6,7,8,9,10])\nprint(summation) # 55\n</code></pre> <pre>\n<code>55\n</code>\n</pre> <pre><code># round() rounds a number to a \n# specified number of decimal places\npi = 3.14159265358979\nrounded_pi = round(pi, 3)\n\nprint('PI: ', pi) # 3.14159265358979\nprint('Rounded PI: ', rounded_pi) # 3.142\n</code></pre> <pre>\n<code>PI:  3.14159265358979\nRounded PI:  3.142\n</code>\n</pre> <pre><code># abs() returns the absolute value of a number\nnumber = -5\nabsolute_value = abs(number)\nprint(absolute_value) # 5\n</code></pre> <pre>\n<code>absolute value of -5 is 5\n</code>\n</pre> <pre><code># pow() returns the value of \n# x to the power of y\nfour_power_two = pow(4, 2)\nprint(four_power_two) # 16\n</code></pre> <pre>\n<code>16\n</code>\n</pre> <pre><code># divmod() returns the quotient and \n# remainder of a division\nquotient, remainder = divmod(10, 3)\nprint(quotient) # 3\nprint(remainder) # 1\n</code></pre> <pre>\n<code>Quotient: 3\nRemainder: 1\n</code>\n</pre>"},{"location":"Python/Basics/03_numbers/#numbers","title":"Numbers","text":"<p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Basics/03_numbers/#table-of-contents","title":"Table of Contents","text":"<p>In this notebook, you will learn the following</p> <ul> <li> Number types </li> <li> Number arithmetics</li> <li> Number methods</li> </ul>"},{"location":"Python/Basics/03_numbers/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Basics/03_numbers/#types","title":"Types","text":""},{"location":"Python/Basics/03_numbers/#integer","title":"Integer","text":"<p>An integer is a number without decimals</p>"},{"location":"Python/Basics/03_numbers/#floating-point","title":"Floating Point","text":"<p>A floating point number of just a float is a number with decimals</p>"},{"location":"Python/Basics/03_numbers/#complex","title":"Complex","text":"<p>A comple number is an imaginary number. To yield a complex number, append a <code>j</code> o <code>J</code> to a numeric value</p>"},{"location":"Python/Basics/03_numbers/#number-arthmetics","title":"Number Arthmetics","text":"<p>Below are some of the basic arithmetic operations that can be performed with numbers</p>"},{"location":"Python/Basics/03_numbers/#number-methods","title":"Number Methods","text":"<p>Number methods are special inbuilt functions used to work with numbers</p>"},{"location":"Python/Basics/03_numbers/#exercise","title":"Exercise","text":"<p>Write a program to assign the number 64.8678 to a variable named \u201cnumber\u201d then display the number rounded to two decimal places.</p>"},{"location":"Python/Basics/03_numbers/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Basics/03_strings/","title":"Strings","text":"<p>title: Python Strings author: Juma Shafara date: \"2023-11\" date-modified: \"2024-11-25\" description: What are strings and how do we use them in Python? keywords: [python, python programming, python variables, strings]</p> <p></p> <p>In this notebook, you will learn the following about strings</p> <pre><code># Strings\nname = 'Juma'\nother_name = \"Masaba Calvin\"\nstatement = 'I love coding'\n</code></pre> <pre><code># when to use which quotes\nreport = 'He said, \"I will not go home\"'\n</code></pre> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code>text = 'shafara loves coding'\ncapitalized_text = text.capitalize()\nprint(capitalized_text)\n</code></pre> <pre>\n<code>Shafara loves coding\n</code>\n</pre> <pre><code>text = 'JavaScript'\nupper_text = text.upper()\nprint(upper_text)\n</code></pre> <pre>\n<code>JAVASCRIPT\n</code>\n</pre> <pre><code>text = 'JavaScript'\nlower_text = text.lower()\nprint(lower_text)\n</code></pre> <pre>\n<code>javascript\n</code>\n</pre> <pre><code>text = 'JavaScript'\ntext_length = len(text)\nprint(text_length)\n</code></pre> <pre>\n<code>10\n</code>\n</pre> <pre><code>text = 'shafara is a good girl'\ncorrected_text = text.replace('shafara', 'viola')\nprint(corrected_text)\n</code></pre> <pre>\n<code>viola is a good girl\n</code>\n</pre> <pre><code>text = 'shafara loves coding'\nprint('shafara' not in text)\n</code></pre> <pre>\n<code>False\n</code>\n</pre>"},{"location":"Python/Basics/03_strings/#strings","title":"Strings","text":"<p>Strings are simply text. A string must be surrounded by single or double quotes</p>"},{"location":"Python/Basics/03_strings/#table-of-contents","title":"Table of Contents","text":"<ul> <li> How to create strings</li> <li> String methods</li> <li> Exercise</li> </ul>"},{"location":"Python/Basics/03_strings/#house-to-create-strings","title":"House to create strings","text":"<p>We can create a string by writing any text and we enclose it in quotes. An example is as below</p>"},{"location":"Python/Basics/03_strings/#single-or-double-quotes","title":"Single or double quotes?","text":"<p>Use single quotes when your string contains double quotes, or the vice versa.</p>"},{"location":"Python/Basics/03_strings/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Basics/03_strings/#string-methods","title":"String Methods","text":"<p>In this lesson, we will study some methods(functions) that can be used to work with strings.</p>"},{"location":"Python/Basics/03_strings/#capitalize-string","title":"Capitalize String","text":"<p>The <code>capitalize()</code> function returns the string where the first letter is in upper case. </p> <p>Note that it doesn't modify the original string. </p>"},{"location":"Python/Basics/03_strings/#convert-to-upper-case","title":"Convert to Upper Case","text":"<p>The <code>upper()</code> function returns a copy of the given string but all the letters are in upper case.</p> <p>Note that this does not modify the original string.</p>"},{"location":"Python/Basics/03_strings/#convert-to-lower-case","title":"Convert to Lower Case","text":"<p>The <code>lower()</code> function returns a copy of the given string but all the letter are in lower case.</p> <p>Note that it does not modify the original text/string</p>"},{"location":"Python/Basics/03_strings/#get-the-lenght-of-a-string","title":"Get the lenght of a String","text":"<p>The length of a  string is the number of characters it contains</p> <p>The <code>len()</code> function returns the length of a string. It takes on paramter, the string.</p>"},{"location":"Python/Basics/03_strings/#replace-parts-of-a-string","title":"Replace Parts of a String.","text":"<p>The <code>replace()</code> method/function replaces the occurrences of a specified substring with another substring.</p> <p>It doesn't modify the original string.</p>"},{"location":"Python/Basics/03_strings/#check-if-a-value-is-present-in-a-string","title":"Check if a Value is Present in a String","text":"<p>To check if a substring is present in a string, use the <code>in</code> keyword.</p> <p>It returns <code>True</code> if the substring is found, otherwise <code>False</code>.</p>"},{"location":"Python/Basics/03_strings/#exercise","title":"Exercise","text":"<p>Write a program to convert a given character from uppercase to lowercase and vice versa.</p>"},{"location":"Python/Basics/03_strings/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Basics/04_operators/","title":"Operators","text":"<p>title: Python Operators author: Juma Shafara date: \"2023-11\" date-modified: \"2024-09-16\" description: This lesson will give an overview of Python Operators keywords: [Python Operators] body-header: This page was developed by Juma Shafara</p> <p></p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># Addition\nx = 10 \ny = 5\n\nsummation = x + y\nprint(summation)\n</code></pre> <pre>\n<code>15\n</code>\n</pre> <p>It can also be used to concatenate or join strings together</p> <pre><code>first_name = 'Juma '\nlast_name = 'Shafara'\n\nfull_name = first_name + last_name\nprint(full_name)\n</code></pre> <pre>\n<code>Juma Shafara\n</code>\n</pre> <pre><code># Subraction\nx = 10 \ny = 5\n\ndifference = x - y\nprint(difference)\n</code></pre> <pre>\n<code>5\n</code>\n</pre> <pre><code># Multiplication\nx = 10 \ny = 5\n\nproduct = x * y\nprint(product)\n</code></pre> <pre>\n<code>50\n</code>\n</pre> <pre><code># Division\nx = 10 \ny = 5\n\nquotient = x / y\nprint(quotient)\n</code></pre> <pre>\n<code>2.0\n</code>\n</pre> <pre><code># Exponentiation\nx = 10 \ny = 5\n\nexponent = x ** y\nprint(exponent)\n</code></pre> <pre>\n<code>100000\n</code>\n</pre> <pre><code># Remainder\nx = 10 \ny = 5\n\nremainder = x % y\nprint(remainder)\n</code></pre> <pre>\n<code>0\n</code>\n</pre> <pre><code># Floor Division\nx = 10 \ny = 5\n\nfloor = 10 // 4\nprint(floor)\n</code></pre> <pre>\n<code>2\n</code>\n</pre> <pre><code>answer = 10 * 3 / 2 + 1\nprint(answer)\n</code></pre> <pre>\n<code>16.0\n</code>\n</pre> <pre><code># Assignment\nnumber = 10\nprint(number)\n</code></pre> <pre>\n<code>10\n</code>\n</pre> <pre><code># Addition Ass\nx += 5 # x = x + 5 =&amp;gt; x = 10 + 5 =&amp;gt; x = 15\nprint(x)\n</code></pre> <pre>\n<code>15\n</code>\n</pre> <pre><code># Subraction Ass\nx = 10\nx -= 5 # x = x - 5 =&amp;gt; x = 10 - 5 =&amp;gt; x = 5\nprint(x)\n</code></pre> <pre>\n<code>5\n</code>\n</pre> <pre><code># Equality \nprint('Voila' == 'Viola') # False\nprint(5 == 5) # True\n</code></pre> <pre>\n<code>False\nTrue\n</code>\n</pre> <pre><code># Inequality \nprint('Voila' != 'Viola')\nprint(10 != 10)\n</code></pre> <pre>\n<code>True\nFalse\n</code>\n</pre> <pre><code>print(8 &amp;gt; 4) # True\nprint(5 &amp;gt; 10) # False\n</code></pre> <pre>\n<code>True\nFalse\n</code>\n</pre> <pre><code>print(10 &amp;lt; 15) # True\nprint(20 &amp;lt; 10) # False\n</code></pre> <pre>\n<code>True\nFalse\n</code>\n</pre> <pre><code># Greater or Equal\n34 &amp;gt;= 43\n</code></pre> <pre>\n<code>False</code>\n</pre> <pre><code>print(10 &amp;lt;= 15) # True\nprint(10 &amp;lt;= 10) # True\nprint(20 &amp;lt;= 10) # False\n</code></pre> <pre>\n<code>True\nTrue\nFalse\n</code>\n</pre> <pre><code>weight = 56\nheight = 1.5\n\nbmi = weight / (height**2)\n\nprint('BMI: ', bmi)\n</code></pre> <pre>\n<code>BMI:  24.88888888888889\n</code>\n</pre> <pre><code># Example 1\nx = 5\ny = 4\n\nprint(x is y)\n</code></pre> <pre>\n<code>False\n</code>\n</pre> <pre><code># Example 2\nx = 5 \ny = 4\nz = x\n\nprint(x is y) # False\nprint(x is z) # True\n</code></pre> <pre>\n<code>False\nTrue\n</code>\n</pre> <pre><code># Example\n# is\nx = 5\ny = 4\nz = x \n\nprint(x is not y) # True\nprint(x is not z) # False\n</code></pre> <pre>\n<code>True\nFalse\n</code>\n</pre> <pre><code># Example\n# Logical and\nx = 4\nprint(x &amp;gt; 3 and 8 &amp;lt; x)\n#               True and False =&amp;gt; False\n</code></pre> <pre>\n<code>False\n</code>\n</pre> <pre><code># Logical or\ny = 7\nexpression_2 = 10 &amp;gt; y or 4 &amp;gt; y\n#                   True or False =&amp;gt; True\nprint(expression_2)\n</code></pre> <pre>\n<code>True\n</code>\n</pre> <pre><code># Logical not\nz = 8\nexpression_3 = not(10 == z)\n#               not False =&amp;gt; True\nprint(expression_3)\n</code></pre> <pre>\n<code>True\n</code>\n</pre> <pre><code># Example\nname = 'Tinye Robert'\nprint('Robert' in name)\n</code></pre> <pre>\n<code>True\n</code>\n</pre> <pre><code># Example\nname = 'Tinye Robert'\nprint('Robert' not in name)\n</code></pre> <pre>\n<code>False\n</code>\n</pre>"},{"location":"Python/Basics/04_operators/#what-are-python-operators","title":"What are Python Operators","text":"<p>Operators are symbols that perform operations on operands. Operands can be variables, strings, numbers, booleans etc</p>"},{"location":"Python/Basics/04_operators/#table-of-contents","title":"Table of Contents","text":"<ul> <li> What are Python Operators</li> <li> Arithmetic</li> <li> Assignment</li> <li> Comparison</li> <li> The BMI Example</li> <li> Identity</li> <li> Logical</li> <li> Membership</li> <li> Exercise</li> </ul>"},{"location":"Python/Basics/04_operators/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Basics/04_operators/#arithmetic","title":"Arithmetic","text":"<p>Arithemators are symbols that perform mathematical operations on operands</p> Arithmetic Operator Description <code>+</code> Addition <code>-</code> Subraction <code>/</code> Division <code>*</code> Multiplication <code>**</code> Exponentiates <code>%</code> Remainder <code>//</code> Floor Division"},{"location":"Python/Basics/04_operators/#addition-operator","title":"Addition Operator","text":"<p>The addition operator returns the sum of its numerical operands</p>"},{"location":"Python/Basics/04_operators/#subtraction-operator","title":"Subtraction Operator","text":"<p>The subtraction operator returns the difference of its numerical operands</p>"},{"location":"Python/Basics/04_operators/#multiplication-operator","title":"Multiplication Operator","text":"<p>The multiplication operator returns the product of its numerical operands</p>"},{"location":"Python/Basics/04_operators/#division-operator","title":"Division Operator","text":"<p>The division operator returns the quotient of its numerical operands.</p>"},{"location":"Python/Basics/04_operators/#exponentiation-operator","title":"Exponentiation Operator","text":"<p>The exponentiation operator raises the left operand to the power of the right operand</p>"},{"location":"Python/Basics/04_operators/#remainder-operators","title":"Remainder Operators","text":"<p>The remainder operator, also knows as the modulus operator returns the remainder after dividing the left operand by the right operand</p>"},{"location":"Python/Basics/04_operators/#floor-division-operator","title":"Floor Division Operator","text":"<p>The floor division rounds down the quotient of its numerical operands to the nearest whole number.</p>"},{"location":"Python/Basics/04_operators/#operator-sequence","title":"Operator Sequence","text":"<p>Operator sequence describes the order of performed operations in an arithmetic expression.</p>"},{"location":"Python/Basics/04_operators/#assignment","title":"Assignment","text":"<p>Assignment operators are used to assign values to variables.</p> Name Operation Same As Assignment <code>x = y</code> <code>x = y</code> Addition Ass <code>x += y</code> <code>x = x + y</code> Subtraction Ass <code>x -= y</code> <code>x = x - y</code> Mult Ass <code>x *= y</code> <code>x = x * y</code> Division Ass <code>x /= y</code> <code>x = x / y</code> Expo Ass <code>x **= y</code> <code>x = x ** y</code> Remainder Ass <code>x %= y</code> <code>x = x % y</code> Floor Div Ass <code>x //= y</code> <code>x = x // y</code>"},{"location":"Python/Basics/04_operators/#assignment_1","title":"Assignment","text":"<p>The assignment operator assigns a value to a variable</p>"},{"location":"Python/Basics/04_operators/#addition-assignment","title":"Addition Assignment","text":"<p>The addition assignment operator adds the left and right operands and assigns the sum to the left operand (the variable)</p>"},{"location":"Python/Basics/04_operators/#subtraction-assignment","title":"Subtraction Assignment","text":"<p>The subtraction assignment operators deducts the right operand and assigns the difference to the left operand (the variable)</p>"},{"location":"Python/Basics/04_operators/#comparison","title":"Comparison","text":"<p>A comparison operator compares its operands and returns a Boolean value based on whether the comparison is  True of False</p> Name Operation Equality <code>==</code> Inequality <code>!=</code> Greater than <code>&gt;</code> Less than <code>&lt;</code> Greater or equal <code>&gt;=</code> Less or equal <code>&lt;=</code>"},{"location":"Python/Basics/04_operators/#equality","title":"Equality","text":"<p>The equality operator compares two values and returns <code>True</code> if the operands are equal, otherwise returns <code>False</code></p>"},{"location":"Python/Basics/04_operators/#inequality","title":"Inequality","text":"<p>The inequality operator compares two values and returns <code>True</code> if the operands are Not equal, otherwise returns <code>False</code></p>"},{"location":"Python/Basics/04_operators/#greater-than","title":"Greater than","text":"<p>The greater than operator returns <code>True</code> if the left operand is greater than the right operand, otherwise returns <code>False</code>.</p>"},{"location":"Python/Basics/04_operators/#less-than","title":"Less than","text":"<p>The less than operator returns <code>True</code> if the left operand is less than the right operand, otherwise returns <code>False</code></p>"},{"location":"Python/Basics/04_operators/#greater-than-or-equal-to","title":"Greater than or equal to","text":"<p>The greater than or equal to operator returns <code>True</code> if the left operand is greater than or equal to the right operand, otherwise returns <code>False</code></p>"},{"location":"Python/Basics/04_operators/#less-than-or-equal-to","title":"Less than or equal to","text":"<p>The less than or equal to operator returns <code>True</code> if the left operand is less than or equal to the right operand, otherwise returns <code>False</code></p>"},{"location":"Python/Basics/04_operators/#the-body-mass-index-example","title":"The Body Mass Index Example","text":"<p>In this example, we use the operators to calculate the body mass index of a person.</p>"},{"location":"Python/Basics/04_operators/#identity","title":"Identity","text":"<p>Identity operators are used to compare two values to determine if they point to the same object</p> Operator Name <code>is</code> The is operator <code>is not</code> The is not operator"},{"location":"Python/Basics/04_operators/#the-is-operator","title":"The <code>is</code> operator","text":"<p>This operator returns <code>True</code> if both operands point to the same object.</p>"},{"location":"Python/Basics/04_operators/#the-is-not-operator","title":"The <code>is not</code> operator","text":"<p>This operator return <code>True</code> if both operands do Not point to the same object in memory</p>"},{"location":"Python/Basics/04_operators/#logical","title":"Logical","text":"<p>Logical operators are commonly used with Booleans. In Python, there are 3 logical operators</p> Operator Description <code>and</code> Logical and operator <code>or</code> Logical or <code>not</code> Logical not"},{"location":"Python/Basics/04_operators/#logical-and","title":"Logical <code>and</code>","text":"<p>The logical <code>and</code> operator returns <code>True</code> if both operands are <code>True</code></p>"},{"location":"Python/Basics/04_operators/#logical-or","title":"Logical <code>or</code>","text":"<p>The logical <code>or</code> operator returns <code>True</code> if one of the operands is <code>True</code></p>"},{"location":"Python/Basics/04_operators/#logical-not","title":"Logical <code>not</code>","text":"<p>The logical <code>not</code> operator returns <code>True</code> if the operand is <code>False</code>, otherwise returns <code>False</code> if the operand is <code>True</code></p>"},{"location":"Python/Basics/04_operators/#membership","title":"Membership","text":"<p>Membership operators are used to check if a sequence is present in an object like a string, list etc</p> Operator Name <code>in</code> The in operator <code>not in</code> The not in operator"},{"location":"Python/Basics/04_operators/#the-in-operator","title":"The <code>in</code> operator","text":"<p>The <code>in</code> operator returns <code>True</code> if a sequence or value is present in an object</p>"},{"location":"Python/Basics/04_operators/#the-not-in-operator","title":"The <code>not in</code> operator","text":"<p>The <code>not in</code> operator returns <code>True</code> if a sequence or value is NOT present in an object</p>"},{"location":"Python/Basics/04_operators/#exercise","title":"Exercise","text":"<p>A car increases it velocity from u ms-1 to v ms-1 within t seconds. Write a program to calculate the acceleration.</p>"},{"location":"Python/Basics/04_operators/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Basics/05_containers/","title":"Containers","text":"<p>title: Python Containers description: Learn Programming for Data Science keywords: [python containers, python collections, list, set, object, tuple, dictionary, indexing, negative indexing, range of indexes, adding items, deleting items, Juma Shafara, What is an object?] author: Juma Shafara date: \"2023-11\" date-modified: \"2024-11-25\"</p> <p></p> <p>Containers in Python are objects that contain other objects</p> <pre><code>x = 'My favorite number is '\ny = 21\n\nprint(x, y)\n</code></pre> <pre>\n<code>My favorite number is  21\n</code>\n</pre> <pre><code>fruits = ['bananas', 'apples', 'grapes']\n\nprint(fruits)\n</code></pre> <pre>\n<code>['bananas', 'apples', 'grapes']\n</code>\n</pre> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <ul> <li> A python list is an ordered container</li> <li> A list is created by using square brackets (`[]`)</li> <li> Objects are poaced inside those brackets and are separated by commas (`,`)</li> </ul> <pre><code>pets = ['dog', 'cat', 'rabbit', 'monkey']\nprint(pets)\nprint(type(pets))\n</code></pre> <pre>\n<code>['dog', 'cat', 'rabbit', 'monkey']\n&lt;class 'list'&gt;\n</code>\n</pre> <p>A list can contain mixed data types.</p> <pre><code>x = ['dog', 21, True]\nprint(x)\n</code></pre> <pre>\n<code>['dog', 21, True]\n</code>\n</pre> <pre><code># indexing\npets = ['dog', 'cat', 'rabbit', 'monkey']\nprint(pets[0])\nprint(pets[1])\nprint(pets[2])\n</code></pre> <pre>\n<code>dog\ncat\nrabbit\n</code>\n</pre> <pre><code># negative indexing\npets = ['dog', 'cat', 'rabbit', 'monkey']\nprint(pets[-1])\nprint(pets[-2])\nprint(pets[-3])\n</code></pre> <pre>\n<code>monkey\nrabbit\ncat\n</code>\n</pre> <pre><code>#range of indexes\npets = ['dog', 'cat', 'rabbit', 'monkey']\nprint(pets[1:3])\n</code></pre> <pre>\n<code>['cat', 'rabbit']\n</code>\n</pre> <p>If you don't specify the last index, the range ends with the last item of the list</p> <p>In this case, the range includes the last item.</p> <pre><code>pets = ['dog', 'cat', 'rabbit', 'monkey']\nprint(pets[2:])\n</code></pre> <pre>\n<code>['rabbit', 'monkey']\n</code>\n</pre> <pre><code>pets = ['dog', 'cat']\npets.append('fish')\n\nprint(pets)\n</code></pre> <pre>\n<code>['dog', 'cat', 'fish']\n</code>\n</pre> <p>The <code>insert()</code> method inserts an item at the specified index.</p> <p>In this case, we will insert <code>'rabbit'</code> to the index 0 and <code>'hamster'</code> to index 2</p> <pre><code>pets = ['dog', 'cat', 'monkey']\npets.insert(0, 'rabbit')\npets.insert(2, 'hamster')\n\nprint(pets)\n</code></pre> <pre>\n<code>['rabbit', 'dog', 'hamster', 'cat', 'monkey']\n</code>\n</pre> <pre><code>pets = ['dog', 'cat', 'rabbit', 'monkey']\npets.pop()\n\nprint(pets)\n</code></pre> <pre>\n<code>['dog', 'cat', 'rabbit']\n</code>\n</pre> <p>The <code>remove()</code> method removes the specified item value.</p> <pre><code>pets = ['dog', 'cat', 'rabbit', 'monkey']\npets.remove('rabbit')\n\nprint(pets)\n</code></pre> <pre>\n<code>['dog', 'cat', 'monkey']\n</code>\n</pre> <p>To delete a specified index, use the <code>del</code> keyword</p> <pre><code>pets = ['dog', 'cat', 'rabbit', 'monkey']\ndel pets [2]\n\nprint(pets)\n</code></pre> <pre>\n<code>['dog', 'cat', 'monkey']\n</code>\n</pre> <pre><code>pets = ['dog', 'cat', 'rabbit', 'monkey']\npets[2] = 'fish'\n\nprint(pets)\n</code></pre> <pre>\n<code>['dog', 'cat', 'fish', 'monkey']\n</code>\n</pre> <pre><code>pets = ['dog', 'cat']\nother_pets = ['rabbit', 'monkey']\npets.extend(other_pets)\nprint(pets)\n</code></pre> <pre>\n<code>['dog', 'cat', 'rabbit', 'monkey']\n</code>\n</pre> <pre><code>pets = ('dog', 'cat', 'rabbit')\nprint(pets)\nprint(type(pets))\n</code></pre> <pre>\n<code>('dog', 'cat', 'rabbit')\n&lt;class 'tuple'&gt;\n</code>\n</pre> <ul> <li> A set is a container/collection that is unordered and immutable</li> <li> We create a set using `{}`</li> <li> The objects are placed inside those brackets and are separated by commas </li> </ul> <pre><code>pets = {'dog', 'cat', 'rabbit'}\nprint(pets)\n</code></pre> <pre>\n<code>{'rabbit', 'dog', 'cat'}\n</code>\n</pre> <p>A set can contain mixed data types, but can NOT contain mutable items like lists, sets and dictionaries</p> <pre><code># A set can contain objects of different data types\nmixed = {'dog', 21, True}\nprint(mixed)\nprint(type(mixed))\n</code></pre> <pre>\n<code>{True, 'dog', 21}\n&lt;class 'set'&gt;\n</code>\n</pre> <pre><code># Accessing\npets = {'dog', 'cat', 'rabbit'}\nfor pet in pets:\n    print(pet)\n</code></pre> <pre>\n<code>rabbit\ndog\ncat\n</code>\n</pre> <pre><code># Adding items to a set\npets = {'dog', 'cat', 'rabbit'}\npets.add('fish')\nprint(pets)\n</code></pre> <pre>\n<code>{'rabbit', 'dog', 'cat', 'fish'}\n</code>\n</pre> <pre><code># Removing items from a set\npets = {'dog', 'cat', 'rabbit'}\npets.remove('cat') # remove\nprint(pets)\n</code></pre> <pre>\n<code>{'rabbit', 'dog'}\n</code>\n</pre> <p>You can also use the <code>discard()</code> method.</p> <pre><code>pets = {'dog', 'cat', 'rabbit'}\npets.discard('rabbit') #discard\nprint(pets)\n</code></pre> <pre>\n<code>{'dog', 'cat'}\n</code>\n</pre> <p>The difference between the <code>remove()</code> and <code>discard()</code> methods is that the <code>discard()</code> method does not raise an error if the specified item is not present.</p> <p>You can also use the <code>pop()</code> method to remove an item.</p> <p>But we cannot determine which item will be removed because a set is unordered.</p> <pre><code>pets = {'dog', 'cat', 'rabbit'}\npets.pop() # pop removes the last item from the set\nprint(pets)\n</code></pre> <pre>\n<code>{'dog', 'cat'}\n</code>\n</pre> <pre><code># Getting the difference\nfirst_numbers = {1, 2, 3, 4}\nsecond_numbers = {3, 4, 5, 6}\n\ndifference = first_numbers - second_numbers\n# another way\ndifference2 = first_numbers.difference(second_numbers)\nprint(difference)\n</code></pre> <pre>\n<code>{1, 2}\n</code>\n</pre> <pre><code># Creating \nperson = {\n    'first_name': 'Voila', \n    'last_name': 'Akullu',\n    'age': 16\n    }\n\nprint(person)\n</code></pre> <pre>\n<code>{'first_name': 'Voila', 'last_name': 'Akullu', 'age': 16}\n</code>\n</pre> <p>In the above example we have 3 items:</p> <ul> <li> The first item has a key name of `'first_name'`, and its value is `'Viola'`.</li> <li> The second item has a key name of `'last_name'`, and its value is `'Akullu'`</li> <li> The third item has a key name of `'age'`, and its value is `30`</li> </ul> <pre><code># Accessing items\nperson = {\n    'first_name': 'Voila', \n    'last_name': 'Akullu',\n    'age': 16\n    }\n\nprint(person['last_name'])\n</code></pre> <pre>\n<code>Akullu\n</code>\n</pre> <p>If you try to access an item using a key name that does not exist, an error will be raised </p> <pre><code># Adding items \nperson = {\n    'first_name': 'Voila', \n    'last_name': 'Akullu',\n    'age': 16\n    }\nperson['middle_name'] = 'Vee'\n\nprint(person)\n</code></pre> <pre>\n<code>{'first_name': 'Voila', 'last_name': 'Akullu', 'age': 16, 'middle_name': 'Vee'}\n</code>\n</pre> <pre><code># Changing items \nperson = {\n    'first_name': 'Voila', \n    'last_name': 'Akullu',\n    'age': 16\n    }\nperson['last_name'] = 'Kibekityo'\n\nprint(person)\n</code></pre> <pre>\n<code>{'first_name': 'Voila', 'last_name': 'Kibekityo', 'age': 16}\n</code>\n</pre> <pre><code># Remove items\nperson = {\n    'first_name': 'Voila', \n    'last_name': 'Akullu',\n    'age': 16\n    }\n\nperson.pop('age')\nprint(person)\n</code></pre> <pre>\n<code>{'first_name': 'Voila', 'last_name': 'Akullu'}\n</code>\n</pre> <p>Another way to remove an item is to use the <code>del</code> keyword.</p> <pre><code># del keyword\nperson = {\n    'first_name': 'Voila', \n    'last_name': 'Akullu',\n    'age': 16\n    }\n\ndel person['age']\nprint(person)\n</code></pre> <pre>\n<code>{'first_name': 'Voila', 'last_name': 'Akullu'}\n</code>\n</pre> <pre><code># Nesting dictionaries\nemployees = {\n    'manager': {\n        'name': 'Akullu Viola',\n        'age': 29\n    },\n    'programmer': {\n        'name': 'Juma Shafara',\n        'age': 30\n    }\n}\n\nprint(employees)\n</code></pre> <pre>\n<code>{'manager': {'name': 'Akullu Viola', 'age': 29}, 'programmer': {'name': 'Juma Shafara', 'age': 30}}\n</code>\n</pre> <p>To access an item in a nested dictionary, access the key name of the dictionary then the key name of the item</p> <pre><code># Accessing nested dictionary\nemployees = {\n    'manager': {\n        'name': 'Akullu Viola',\n        'age': 29\n    },\n    'programmer': {\n        'name': 'Juma Shafara',\n        'age': 30\n    }\n}\n\nprogrammer = employees['programmer']\nprint(programmer['name'])\n</code></pre> <pre>\n<code>Juma Shafara\n</code>\n</pre> <pre><code># Using a dictionary constructer\nnames = ('a1', 'b2', 'c3')\ndictionary = dict(names)\nprint(dictionary)\n</code></pre> <pre>\n<code>{'a': '1', 'b': '2', 'c': '3'}\n</code>\n</pre>"},{"location":"Python/Basics/05_containers/#table-of-contents","title":"Table of Contents","text":"<ul> <li> What is an object?</li> <li> Containers</li> <li> Lists</li> <li> Tuple</li> <li> Sets</li> <li> Dictionaries</li> <li> Free Dictionary Tip</li> <li> Exercise</li> </ul>"},{"location":"Python/Basics/05_containers/#what-is-an-object","title":"What is an object?","text":"<p>In python, everything is an object. Even the simplest strings and numbers are considered as objects</p>"},{"location":"Python/Basics/05_containers/#containers","title":"Containers","text":"<p>Containers (also called collections) are objects that contain objects</p> <p>In the example below, the <code>fruits</code> variable contains three strings</p>"},{"location":"Python/Basics/05_containers/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Basics/05_containers/#lists","title":"Lists","text":""},{"location":"Python/Basics/05_containers/#indexing","title":"Indexing","text":"<ul> <li>Indexing is used to access items of a list</li> <li>Indexing uses square brackets and numbers to access individual items of a list</li> <li>Where <code>0</code> refers to the first item, 1 refers to the second item, and so on</li> </ul>"},{"location":"Python/Basics/05_containers/#negative-indexing","title":"Negative Indexing","text":"<p>Negative indexing is used to access the items of a list using negative numbers.</p> <p>Where <code>-1</code> refers to the last item, <code>-2</code> refers to the second last item, and so on.</p>"},{"location":"Python/Basics/05_containers/#range-of-indexes","title":"Range of Indexes","text":"<p>By using a colon ie <code>:</code>, we can access a range of items at once.</p> <p>Simply separate two indexes using the colon.</p> <p>The first index is the start of the range, while the second index is the end of the range (not included)</p>"},{"location":"Python/Basics/05_containers/#adding-items-to-a-list","title":"Adding items to a list","text":"<p>The <code>append()</code> method adds an item to the end of the list.</p> <p>In this example, we will add <code>'fish'</code> to our pets list</p>"},{"location":"Python/Basics/05_containers/#deleting-items-from-a-list","title":"Deleting Items from a list","text":"<p>The <code>pop()</code> method removes the last item from a list </p>"},{"location":"Python/Basics/05_containers/#getting-the-length-of-a-list","title":"Getting the length of a list","text":"<p>The length of a list refers to the number of items in a list, use the <code>len()</code> method</p>"},{"location":"Python/Basics/05_containers/#changing-an-items-value","title":"Changing an item's value","text":"<p>To change an item's value, access the index first and use the assignment operator.</p>"},{"location":"Python/Basics/05_containers/#homework","title":"Homework","text":"<ul> <li>Check if an item exist</li> </ul>"},{"location":"Python/Basics/05_containers/#extending-a-list","title":"Extending a list","text":"<p>The <code>extend()</code> methods adds all items from one list to another</p>"},{"location":"Python/Basics/05_containers/#tuple","title":"Tuple","text":"<ul> <li> Python tuple is an ordered container</li> <li> Its the same as a list but the items of tuples cannot be changed</li> <li> We create a tuple using round brackets `()`</li> </ul>"},{"location":"Python/Basics/05_containers/#sets","title":"Sets","text":""},{"location":"Python/Basics/05_containers/#accessing-set-elements","title":"Accessing set elements","text":"<ul> <li>Unlike lists and tuples, you cannot access the items in a set using indexes</li> <li>This is because a set is unordered and not indexed</li> <li>However, we can use a <code>for</code> loop to access all its items one-by-one</li> </ul> <p>Note: We'll discuss a for loop in the next chapter</p>"},{"location":"Python/Basics/05_containers/#adding-elements-to-a-set","title":"Adding elements to a set","text":"<p>To add items to a set, use the <code>add()</code> or <code>update()</code> method.</p> <p>The <code>add()</code> method adds one item to a set.</p>"},{"location":"Python/Basics/05_containers/#changing-an-item","title":"Changing an item","text":"<p>The items of a set can NOT be changed because a set is immutable or unchangeable.</p>"},{"location":"Python/Basics/05_containers/#removing-set-elements","title":"Removing set elements","text":"<p>To remove an item from a set, use the <code>remove()</code> method.</p> <p>You should specify the value of the item you want to remove.</p>"},{"location":"Python/Basics/05_containers/#homework_1","title":"Homework","text":"<ul> <li>Find the length of a set</li> <li>Check if an element exists</li> <li>Combine sets</li> </ul>"},{"location":"Python/Basics/05_containers/#getting-the-difference-between-sets","title":"Getting the difference between sets","text":"<p>To get the difference between two sets, use the subtraction operator (<code>-</code>)</p>"},{"location":"Python/Basics/05_containers/#dictionaries","title":"Dictionaries","text":"<p>A dictionary is an unordered and mutable colletion of items.</p> <p>A dictionary is written with curly brackets.</p> <p>Each item in a dictionary contains a <code>key/value</code> pair.</p>"},{"location":"Python/Basics/05_containers/#accessing-items","title":"Accessing items","text":"<p>To access an item, specify the key name of an item inside square brackets.</p>"},{"location":"Python/Basics/05_containers/#adding-items","title":"Adding items","text":"<p>To add new items, specify a new index key name inside the square brackets and assign a  value using the assignment operator.</p>"},{"location":"Python/Basics/05_containers/#changing-an-items-value_1","title":"Changing an item's value","text":"<p>To change an item, refer to its key name using square brackets and use the assignment operator.</p>"},{"location":"Python/Basics/05_containers/#removing-an-items-value","title":"Removing an Item's value","text":"<p>To remove an item, use the pop() method.</p> <p>The <code>pop()</code> method removes an item with the specified key name.</p>"},{"location":"Python/Basics/05_containers/#homework_2","title":"Homework","text":"<ul> <li>Check if an element exists</li> <li>Find the length of a dictionary</li> </ul>"},{"location":"Python/Basics/05_containers/#nested-dictionary","title":"Nested Dictionary","text":"<p>A dictionary can contain another dictionary.</p>"},{"location":"Python/Basics/05_containers/#free-dictionary-tip","title":"Free Dictionary Tip","text":""},{"location":"Python/Basics/05_containers/#exercise","title":"Exercise","text":"<p>Write a program to store marks of 5 students for 5 subjects given through the keyboard. Calculate the average of each students marks and the average of marks taken by all the students</p>"},{"location":"Python/Basics/05_containers/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Basics/06_flow_control/","title":"Flow Control","text":"<p>title: Python Flow Control description: Learn Programming for Data Science keywords: [function, conditional statements, if, if else, if elif, lambda functions, loops, for loop, while loop] author: Juma Shafara date: \"2023-11\" date-modified: \"11-07-2024\"</p> <p></p> <p>Python control flow tools change the flow of how code is executed by the Python interpreter.</p> <p>Since the Python interpreter executes code in a line-by-line manner, Python control flow tools help dictate what line(s) of code should run in a Python program. There are different types of control flow tools available to us in Python and we will go through them in detail in this lesson.</p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>The function below calculates and returns one's body mass index rounded to 2 decimal places</p> <pre><code># This function calculates Body Mass Index\ndef calculateBodyMassIndex(weight_kg, height_m):\n\n    body_mass_index = weight_kg / pow(height_m, 2)\n    rounded_bmi = round(body_mass_index, 2)\n\n    return rounded_bmi\n</code></pre> <pre><code># lets try\ncalculateBodyMassIndex(67, 1.6)\n</code></pre> <pre>\n<code>26.17</code>\n</pre> <pre><code>def greeter():\n    message = 'Hello'\n    print(message)\n</code></pre>  Note!A group of statements must have the same indentation level, in the example above, 4 whitespaces were used to indent the function body <pre><code>greeter()\n</code></pre> <pre>\n<code>Hello\n</code>\n</pre> <pre><code># define the function\ndef addNumbers(number1, number2):\n    sum = number1 + number2\n    print(sum)\n\n# Call the function\naddNumbers(3, 27)\n</code></pre> <pre>\n<code>30\n</code>\n</pre> <pre><code># setting a default argument\ndef greet(name='you'):\n    message = 'Hello ' + name\n    print(message)\n\ngreet('Tinye')\ngreet()\n</code></pre> <pre>\n<code>Hello Tinye\nHello you\n</code>\n</pre> <pre><code>def hello(name = 'Agaba'):\n    print('Hello ' + name)\n\nhello('John') # calling with John\nhello() # calling with no name\n</code></pre> <pre>\n<code>Hello John\nHello Agaba\n</code>\n</pre> <pre><code>def addNumbers(number1, number2):\n    sum = number1 + number2\n    return sum\n\nsummation = addNumbers(56, 4)\nprint(summation)\n</code></pre> <pre>\n<code>60\n</code>\n</pre>  Note!<p>The return statement stops the execution of a function.</p> <p>### lambda functions  </p> <ul> <li> Lambda functions (also called anonymous functions) are functions that donot have names</li> <li> The body of a lambda function can only have one expression, but can have multiple arguments</li> <li> The result of the expression is automatically returned</li> </ul> <p>Syntax: <pre><code>lambda parameters: expression\n</code></pre></p> <pre><code># Example of lambda function\ncalculateBMI = lambda weight_kg, height_m: round((weight_kg/(height_m ** 2)), 2)\n\n# Calling a labda function\ncalculateBMI(67, 1.7)\n</code></pre> <pre>\n<code>23.18</code>\n</pre>  Note!<p>In the example above, the body mass index is automatically return, even without using the return statement</p> <pre><code># Assume 4 course units\n# 1. Math - A\n# 2. Science - B\n# 3. SST - B\n# 4. English - C\n\n\ndef calculate_CGPA(GPs_list, CUs_list):\n    length = len(GPs_list)\n    product_sum = 0\n\n    for item in range(length):\n        product_sum += GPs_list[item] * CUs_list[item]\n\n    CUs_sum = sum(CUs_list)\n\n    CGPA = product_sum / CUs_sum\n\n    return CGPA\n\n# calculate_CGPA(4, 5)\n</code></pre> <pre><code>def getAge(month, year):\n    month_diff = 12 - month\n    year_diff = 2023 - year\n\n    return str(year_diff) + ' years ' + str(month_diff) + ' months'  \n\nage = getAge(year=2000, month=10) # keyword argument\nage2 = getAge(10, 2000) # positional argument\n\nprint(age)\n</code></pre> <pre>\n<code>23 years 2 months\n</code>\n</pre> <ul> <li> Loops are used to repetitively execute a group of statements</li> <li> we have 2 types, `for` and `while` loop</li> </ul> <pre><code>pets = ['cat', 'dog', 'rabbit']\n# iterate through pets\nfor pet in pets:\n    print(pet)\n</code></pre> <pre>\n<code>cat\ndog\nrabbit\n</code>\n</pre> <p>Here's another example to convert many weights from kilograms(kgs) to pounds(pds)</p> <pre><code># convert all weights in list from kg to pounds\nweights_kg = [145, 100, 76, 80]\nweights_pds = []\n\nfor weight in weights_kg:\n    pounds = weight * 2.2\n    rounded_pds = round(pounds, 2)\n    weights_pds.append(rounded_pds)\n\nprint(weights_pds)\n</code></pre> <p>This example displays all letters in my name.</p> <pre><code># Display all letters in a name\nname = 'Shafara'\n\nfor letter in name:\n    print(letter)\n</code></pre> <pre>\n<code>S\nh\na\nf\na\nr\na\n</code>\n</pre> <pre><code>counter = 0\n\nwhile counter &amp;lt; 5:\n    print('Hello you')\n    counter += 1\n</code></pre> <pre>\n<code>Hello you\nHello you\nHello you\nHello you\nHello you\n</code>\n</pre> <pre><code># Convert the weights in the list from kgs to pounds\nweights_kg = [145, 100, 76, 80]\nweights_pds = []\n\ncounter = 0\nend = len(weights_kg)\n\nwhile counter &amp;lt; end:\n\n    pound = weights_kg[counter] * 2.2\n    rounded_pds = round(pound, 3)\n    weights_pds.append(rounded_pds)\n\n    counter += 1\n\nprint(weights_pds)\n</code></pre> <pre>\n<code>[319.0, 220.0, 167.2, 176.0]\n</code>\n</pre> <p>Conditional statements in Python are fundamental building blocks for controlling the flow of a program based on certain conditions. They enable the execution of specific blocks of code when certain conditions are met. The primary conditional statements in Python include <code>if</code>, <code>elif</code>, and <code>else</code>.</p> <pre><code>if condition:\n    # block of code\n</code></pre> <p>Example: <pre><code>x = 10\nif x &amp;gt; 5:\n    print(\"x is greater than 5\")\n</code></pre></p>"},{"location":"Python/Basics/06_flow_control/#table-of-contents","title":"Table of Contents","text":"<p>In this notebook, you'll learn the following</p> <ul> <li> Functions</li> <li> Loops</li> <li> Conditional Statements</li> </ul>"},{"location":"Python/Basics/06_flow_control/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Basics/06_flow_control/#functions","title":"Functions","text":"<p>A function in python is a group statements that perform a particular task</p>"},{"location":"Python/Basics/06_flow_control/#creating-a-function","title":"Creating a function","text":"<p>To create a function, we need the following:</p> <ul> <li> The `def` keyword</li> <li> A function name</li> <li> Round brackets `()` and a colon `:`</li> <li> A function body- a group of statements</li> </ul>"},{"location":"Python/Basics/06_flow_control/#calling-a-function","title":"Calling a Function","text":"<ul> <li> To execute a function, it needs to be called</li> <li> To call a function, use its function name with parentheses `()`</li> </ul>"},{"location":"Python/Basics/06_flow_control/#function-parametersarguments","title":"Function Parameters/Arguments","text":"<p> When calling a function, we can pass data using parameters/arguments</p> <p> A parameter is a variable declared in the function. In the example below, <code>number1</code> and <code>number2</code> are parameter</p> <p> The argument is the value passed to the function when its called. In the example below <code>3</code> and <code>27</code> are the arguments</p>"},{"location":"Python/Basics/06_flow_control/#default-arguments","title":"Default Arguments:","text":"<p>A function can have default arguments.</p> <p>It can be done using the assignment operator (<code>=</code>).</p> <p>If you don't pass the argument, the default argument will be used instead.</p>"},{"location":"Python/Basics/06_flow_control/#return-statement","title":"Return Statement","text":"<p>The <code>return</code> statement is used to return a value to a function caller</p>"},{"location":"Python/Basics/06_flow_control/#practice-functions","title":"Practice functions","text":""},{"location":"Python/Basics/06_flow_control/#calculate-cgpa","title":"Calculate CGPA","text":""},{"location":"Python/Basics/06_flow_control/#get-someones-age-given-birth-month-and-year","title":"Get someones age given birth month and year","text":""},{"location":"Python/Basics/06_flow_control/#loops","title":"Loops","text":""},{"location":"Python/Basics/06_flow_control/#for-loop","title":"For Loop","text":"<p>A <code>for</code> loop is used to loop through or iterate over a sequence or iterable objects</p> <p>Syntax: <pre><code>for variable in sequence:\n    statements\n</code></pre></p>"},{"location":"Python/Basics/06_flow_control/#looping-through-a-list","title":"Looping through a list","text":"<p>The for loop is commonly used with lists.</p>"},{"location":"Python/Basics/06_flow_control/#while-loop","title":"While loop","text":"<p>The <code>while</code> loop executes a given group of statements as long as the given expression is <code>True</code></p> <p>Syntax: <pre><code>while expression:\n    statements\n</code></pre></p> <p>In the example below, <code>Hello you</code> will be printed 5 times, that is for each time counter is still less than 5.</p>"},{"location":"Python/Basics/06_flow_control/#conditional-statements","title":"Conditional Statements","text":""},{"location":"Python/Basics/06_flow_control/#basic-syntax","title":"Basic Syntax","text":""},{"location":"Python/Basics/06_flow_control/#if-statement","title":"If Statement","text":"<p>The <code>if</code> statement is used to test a condition. If the condition evaluates to <code>True</code>, the block of code inside the <code>if</code> statement is executed.</p>"},{"location":"Python/Basics/06_flow_control/#else-statement","title":"Else Statement","text":"<p>The <code>else</code> statement is used to execute a block of code if the condition in the <code>if</code> statement evaluates to <code>False</code>.</p> <pre><code>if condition:\n    # block of code if condition is True\nelse:\n    # block of code if condition is False\n</code></pre> <p>Example: <pre><code>x = 3\nif x &amp;gt; 5:\n    print(\"x is greater than 5\")\nelse:\n    print(\"x is not greater than 5\")\n</code></pre></p>"},{"location":"Python/Basics/06_flow_control/#elif-statement","title":"Elif Statement","text":"<p>The <code>elif</code> (short for else if) statement allows you to check multiple conditions. If the first condition is <code>False</code>, it checks the next <code>elif</code> condition, and so on. If all conditions are <code>False</code>, the <code>else</code> block is executed.</p> <pre><code>if condition1:\n    # block of code if condition1 is True\nelif condition2:\n    # block of code if condition2 is True\nelse:\n    # block of code if none of the above conditions are True\n</code></pre> <p>Example: <pre><code>x = 7\nif x &amp;gt; 10:\n    print(\"x is greater than 10\")\nelif x &amp;gt; 5:\n    print(\"x is greater than 5 but less than or equal to 10\")\nelse:\n    print(\"x is 5 or less\")\n</code></pre></p>"},{"location":"Python/Basics/06_flow_control/#nested-conditional-statements","title":"Nested Conditional Statements","text":"<p>Conditional statements can be nested within each other to handle more complex decision-making processes.</p> <p>Example: <pre><code>x = 15\nif x &amp;gt; 10:\n    if x &amp;gt; 20:\n        print(\"x is greater than 20\")\n    else:\n        print(\"x is greater than 10 but not greater than 20\")\nelse:\n    print(\"x is 10 or less\")\n</code></pre></p>"},{"location":"Python/Basics/06_flow_control/#conditional-expressions-ternary-operator","title":"Conditional Expressions (Ternary Operator)","text":"<p>Python also supports conditional expressions, which allow for a more concise way to write simple <code>if-else</code> statements.</p> <pre><code>variable = value_if_true if condition else value_if_false\n</code></pre> <p>Example: <pre><code>x = 10\nresult = \"greater than 5\" if x &amp;gt; 5 else \"5 or less\"\nprint(result)  # Output: greater than 5\n</code></pre></p>"},{"location":"Python/Basics/06_flow_control/#combining-conditions","title":"Combining Conditions","text":"<p>Multiple conditions can be combined using logical operators (<code>and</code>, <code>or</code>, <code>not</code>).</p> <p>Example: <pre><code>x = 8\nif x &amp;gt; 5 and x &amp;lt; 10:\n    print(\"x is between 5 and 10\")\n</code></pre></p>"},{"location":"Python/Basics/06_flow_control/#practical-usage","title":"Practical Usage","text":"<p>Conditional statements are used in a wide variety of scenarios, such as:</p> <ul> <li> Validating user input.</li> <li> Controlling the flow of loops.</li> <li> Implementing different behaviors in functions or methods.</li> <li> Handling exceptions or special cases in data processing.</li> </ul> <p>Understanding and effectively using conditional statements are crucial for writing efficient and readable code in Python. They enable developers to build programs that can make decisions and respond dynamically to different inputs and situations.</p>"},{"location":"Python/Basics/06_flow_control/#exercise","title":"Exercise","text":"<p>Write a program which accepts a number (an amount of money to be paid by a customer in rupees) entered from the keyboard. If the amount is greater than or equal to 1000 rupees, a 5% discount is given to the customer. Then display the final amount that the customer has to pay.</p>"},{"location":"Python/Basics/06_flow_control/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Basics/07_advanced/","title":"Advanced Concepts","text":"<p>title: Python Advanced description: Learn Advanced Python concepts like classes and objects, formatted strings, handling errors and variable scopes. keywords: [python advanced, classes and objects, formatted strings, handling errors, variable scopes] author: Juma Shafara date: \"2023-11\" date-modified: \"2024-07-23\"</p> <p></p> <p>In this notebook, we will explore some fundamental concepts in Python that are essential for writing clean, efficient, and maintainable code. These concepts include:</p> <ul> <li> Classes and Objects</li> <li> Formatted Strings</li> <li> Handling Errors</li> <li> Variable Scopes</li> </ul> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>In Python, everything is an object. A class helps us create objects.</p> <p>Use the <code>class</code> keyword to create a class</p> <p>Here is the syntax: <pre><code>class className:\n    statement(s)\n</code></pre></p> <p>Below is an example:</p> <pre><code>class Person:\n    first_name = \"Betty\"\n    last_name = \"Kawala\"\n    age = 30\n</code></pre> <pre><code>person_obj1 = Person()\n\ntype(person_obj1)\n</code></pre> <pre>\n<code>__main__.Person</code>\n</pre> <p>After instantiating a class, we can now access the object's properties.</p> <pre><code># print attributes\nprint(person_obj1.first_name)\nprint(person_obj1.last_name)\nprint(person_obj1.age)\n</code></pre> <pre>\n<code>Betty\nKawala\n30\n</code>\n</pre> <pre><code>class Person:\n    def __init__(self, name, height, feet):\n        self.name = name\n        self.height = height\n        self.feet = feet\n</code></pre>  Note!<p>For now, focus on the syntax. Later we will explain the <code>__init__()</code> function and the <code>self</code> parameter.</p> <p>Now that our class is ready, we can now instantiate it and provide values to it's attributes.</p> <p>This process can also be called \"creating an instance of a class\".</p> <p>An instance is simply the object created from a class</p> <p>In this example, <code>person_obj1</code> is a unique instance of the person class. </p> <pre><code># create a class instance\nperson_obj = Person(\n    name='Betty Kawala', \n    height=1.57, \n    feet=4\n    )\n</code></pre> <p>After that, we can now access the properties of the instance (object)</p> <pre><code># accessing the properties\nprint('Name:', person_obj.name)\nprint('Height:', person_obj.height)\nprint('Feet:', person_obj.feet)\n</code></pre> <pre>\n<code>Name: Betty Kawala\nHeight: 1.57\nFeet: 4\n</code>\n</pre> <p>The <code>self</code> parameter allows us to access the attributes and methods of a class</p> <p>The <code>__init__()</code> function allows us to provide values for the attributes of a class</p> <pre><code>class Student:\n  def __init__(self, id_number, name, age):\n    self.id_number = id_number\n    self.name = name\n    self.age = age\n\nstudent1 = Student(5243, \"Mary Doe\", 18)\nstudent2 = Student(3221, \"John Doe\", 18)\n\nprint(\"Student 1 ID:\", student1.id_number)\nprint(\"Student 1 Name:\", student1.name)\nprint(\"Student 1 Age:\", student1.age)\n\nprint(\"---------------------\")\n\nprint(\"Student 2 ID:\", student2.id_number)\nprint(\"Student 2 Name:\", student2.name)\nprint(\"Student 2 Age:\", student2.age)\n</code></pre> <pre>\n<code>Student 1 ID: 5243\nStudent 1 Name: Mary Doe\nStudent 1 Age: 18\n---------------------\nStudent 2 ID: 3221\nStudent 2 Name: John Doe\nStudent 2 Age: 18\n</code>\n</pre> <pre><code>class Person:\n    def __init__(self, name, height, feet):\n        self.name = name\n        self.height = height\n        self.feet = feet\n\n    def jump(self):\n        return \"I'm jumping \" + str(self.feet) + \" Feet\"\n</code></pre> <pre><code>person_obj1 = Person(name='Juma', height=1.59, feet=5)\n\nprint(person_obj1.jump())\n</code></pre> <pre>\n<code>I'm jumping 5 Feet\n</code>\n</pre> <p>As you may notice, we used the <code>self</code> parameter to access the <code>feet</code> attribute</p> <p>You can also pass an argument to a method.</p> <pre><code>class Student:\n    def __init__(self, id_number, name, age):\n        self.id_number = id_number\n        self.name = name\n        self.age = age\n\n    def greet_student(self, greetings):\n        print(\"Hello\" + self.name + \", \" + greetings)\n\nstudent1 = Student(43221, \"Agaba Calvin\", 18)\n\n# the string below will be passed as \n# the value of the greetings parameter\nstudent1.greet_student(\"Welcome to this Python Tutorial!\")\n</code></pre> <pre>\n<code>HelloAgaba Calvin, Welcome to this Python Tutorial!\n</code>\n</pre> <pre><code>class Animal:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def eat(self):\n        print(f\"{self.name} is eating.\")\n</code></pre> <p>Let's say we need to create another class called Dog.</p> <p>Since a dog is also an animal, it's more efficient to have access to all the properties and methods of the <code>Animal</code> class than to create another</p> <p>This example creates a class named <code>Dog</code> and inherits from the <code>Animal</code> class</p> <pre><code>class Dog(Animal):\n    def __init__(self, name, age, color):\n        super().__init__(name, age)\n        self.color = color\n\n    def sound(self):\n        print(self.name, \"barks\")\n</code></pre>  Note!<p>As you may notice, to inherit from a parent, we simply pass the name of that class as a parameter of the child class.</p> <p>Now we can use the properties and methods of both the <code>Animal</code> and the <code>Dog</code> classes using just one instance</p> <pre><code>dog1 = Dog(name='Brian', age=8, color='White')\ndog1.eat()\ndog1.sound()\n</code></pre> <pre>\n<code>Brian is eating.\nBrian barks\n</code>\n</pre> <p>The <code>super()</code> and <code>__init__</code> functions found in the <code>Dog</code> class allow us to inherit the properties and methods of the <code>Animal</code> class.</p> <p>In Python, we can format a string by adding substring/s within it.</p> <p>The <code>format()</code> function allows us to format strings.</p> <pre><code>text = \"I love {} very much!\"\nformatted_text = text.format(\"Python\")\n\nprint(formatted_text)\n</code></pre> <pre>\n<code>I love Python very much!\n</code>\n</pre> <pre><code>text = '{} loves to code in {}'\nformatted_text = text.format('Juma', 'JavaScript')\n\nprint(formatted_text)\n</code></pre> <pre>\n<code>Juma loves to code in JavaScript\n</code>\n</pre> <pre><code>text = 'I love {2}, {1} and {0} very much!'\nformatted_text = text.format('Python', 'JavaScript', 'HTML')\n\nprint(formatted_text)\n</code></pre> <pre>\n<code>I love HTML, JavaScript and Python very much!\n</code>\n</pre>  Note!<p>0 represents the first value, 1 represents the second value and so on.</p> <pre><code>text = 'The color of the {fruit} is {color}.'\nformatted_text = text.format(fruit='banana', color='yellow')\n\nprint(formatted_text)\n</code></pre> <pre>\n<code>The color of the banana is yellow.\n</code>\n</pre> <pre><code>name = 'Juma'; \nlanguage = 'JavaScript'\n\nstatement = f'{name} loves to code in {language}'\n\nprint(statement)\n</code></pre> <pre>\n<code>Juma loves to code in JavaScript\n</code>\n</pre> <p>Here's another example</p> <pre><code>number1 = 5\nnumber2 = 7\nanswer = f'The summation of 5 and 7 is {number1 + number2}'\n\nprint(answer)\n</code></pre> <pre>\n<code>The summation of 5 and 7 is 12\n</code>\n</pre> <p>When coding in Python, you will encounter errors.</p> <p>When errors occur, the program crashes or stops executing.</p> <p>Fortunately, errors can be handled in Python</p> <pre><code>try:\n    # age = input('Enter your age: ')\n    age = '32'\n\n    if age &amp;gt;= 18:\n        print('Your vote has been cast')\n    else:\n        print('You are not eligible to vote')\nexcept:\n    print('A problem occured while picking your age \\n'\n          'You did not enter a number')\n\ntext = \"\\nHello world!\"\nprint(text)\n</code></pre> <pre>\n<code>A problem occured while picking your age \nYou did not enter a number\n\nHello world!\n</code>\n</pre>  Note!<p>Even when the exception was thrown, the codes after the try...except were still executed</p> <pre><code>try:\n    text = \"Hello World!\"\n    print(text)\nexcept:\n    print(\"An error occurred.\")\nelse:\n    print(\"No exception was thrown!\")\n</code></pre> <pre>\n<code>Hello World!\nNo exception was thrown!\n</code>\n</pre> <pre><code>try: \n    text = \"hello world\"\n    print(text)\nexcept:\n    print(\"An error occured.\")\nfinally:\n    print(\"Hello, I am still printed.\")\n\nprint(\"----------------------\")\n\n# an exception will be thrown here\ntry: \n    print(undefined_variable)\nexcept:\n    print(\"An error occured.\")\nfinally:\n    print(\"Hello, I am still printed.\")\n</code></pre> <pre>\n<code>hello world\nHello, I am still printed.\n----------------------\nAn error occured.\nHello, I am still printed.\n</code>\n</pre> <pre><code># Creating your own errors\ntry: \n    age = 14\n    if age &amp;lt; 18:\n        raise Exception('Not an adult')\nexcept Exception as error:\n    print('A problem occurred \\n'\n          f'Error: {error}')\n</code></pre> <pre>\n<code>A problem occurred \nError: Not an adult\n</code>\n</pre> <pre><code>try:\n    print(rand_var)\nexcept NameError:\n    print('You used a variable that is not defined!')\n</code></pre> <pre>\n<code>You used a variable that is not defined!\n</code>\n</pre> <pre><code>name = 'Viola'\n# name can be accessed here\nprint(name)\n\ndef greet():\n    # name can be accessed here\n    print('Hello ' + name)\n\ngreet()\n</code></pre> <pre>\n<code>Viola\nHello Viola\n</code>\n</pre> <pre><code>def greet():\n    local_name = 'Viola'\n    print('Hello ' + local_name)\n\ngreet()\n\ntry:\n    # local_name cannot be accessed here\n    print(local_name)\nexcept Exception as e:\n    print(e)\n</code></pre> <pre>\n<code>Hello Viola\nname 'local_name' is not defined\n</code>\n</pre> <pre><code># Global Keyword\n\ndef add():\n    global summ\n    number1 = 5\n    number2 = 7\n    summ = number1 + number2\n    return summ\n\nadd()\n\n# summ is accessible even outside the function\nprint(summ)\n</code></pre> <pre>\n<code>12\n</code>\n</pre>"},{"location":"Python/Basics/07_advanced/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Basics/07_advanced/#classes-and-objects","title":"Classes and Objects","text":""},{"location":"Python/Basics/07_advanced/#creating-a-class","title":"Creating a Class","text":""},{"location":"Python/Basics/07_advanced/#instantiating-a-class","title":"Instantiating a class","text":"<p>Now we can ceate an object from the class by instantiating it.</p> <p>To instantiate a class, add round brackets to the class name.</p>"},{"location":"Python/Basics/07_advanced/#class-attributes","title":"Class Attributes","text":"<p>A class can have attributes. Forexample the Person Class can have attributes like the <code>name</code>, <code>height</code> and <code>feet</code></p>"},{"location":"Python/Basics/07_advanced/#instances-are-unique","title":"Instances are unique","text":"<p>Let's say you have 500 people and you need to manage their data.</p> <p>It is inefficient to create a variable for each of them, instead, you can create unique instances of a class.</p> <p>In this example, the student1 and student2 instances are different from each other</p>"},{"location":"Python/Basics/07_advanced/#methods","title":"Methods","text":"<p>Methods are functions that can access the class attributes. </p> <p>These methods should be defined (created) inside the class</p>"},{"location":"Python/Basics/07_advanced/#python-inheritance","title":"Python Inheritance","text":"<p>Inheritance is a feature that allows us to create a class that inherits the attributes or properties and methods of another class</p>"},{"location":"Python/Basics/07_advanced/#example","title":"Example","text":"<p>The <code>Animal</code> class below can be used to tell that an animal can eat</p>"},{"location":"Python/Basics/07_advanced/#parent-and-child-class","title":"Parent and Child Class","text":"<p>The parent class is the class from whick the other class inherits from.</p> <p>The child class is the the class that inherits from another class</p> <p>In our example above, the <code>Animal</code> is the parent class while the <code>Dog</code> class is the child class</p>"},{"location":"Python/Basics/07_advanced/#formatted-strings","title":"Formatted Strings","text":""},{"location":"Python/Basics/07_advanced/#placeholders","title":"Placeholders <code>{}</code>","text":"<p>Placeholders help us control which part of the string should be formated.</p> <p>They are defined using  curly braces <code>{}</code>.</p> <p>In this example, we will concatenate (add) a substring to where the curly braces are placed</p>"},{"location":"Python/Basics/07_advanced/#multiple-placeholders","title":"Multiple placeholders","text":"<p>If you want to format multiple parts of a string, use multiple placeholders.</p>"},{"location":"Python/Basics/07_advanced/#using-indexes","title":"Using Indexes","text":"<p>We can use index numbers to specify exactly where the values should be placed. </p> <p>The index numbers should be inside the curly braces: <code>{index_numbers}</code></p>"},{"location":"Python/Basics/07_advanced/#using-named-indexes","title":"Using Named Indexes","text":"<p>We can also use named indexes to specify exactly where the values should be placed.</p> <p>The arguments of the <code>format()</code> function should be in <code>key/value</code> pairs ie <code>key=value</code>.</p> <p>The <code>key/value</code> pairs should be separated by commas.</p>"},{"location":"Python/Basics/07_advanced/#literal-string-interpolation","title":"Literal String Interpolation","text":"<p>Literal string interpolation allows you to use expression inside your strings. </p> <p>Simply add <code>f</code> before you opening quote, then surround your expressions with curly braces <code>{}</code>.</p>"},{"location":"Python/Basics/07_advanced/#errors-in-python","title":"Errors in Python","text":""},{"location":"Python/Basics/07_advanced/#the-tryexcept-statment","title":"The <code>try...except</code> statment","text":"<p>The <code>try...except</code> statement is used to handle exceptions(errors)</p> <p>The <code>try</code> statement takes a block of code to test for errors</p> <p>The <code>except</code> statement handles the exceptions.</p>"},{"location":"Python/Basics/07_advanced/#the-else-statement","title":"The <code>else</code> statement","text":"<p>The else statement is executed if there are no exceptions thrown.</p>"},{"location":"Python/Basics/07_advanced/#the-finally-statement","title":"The <code>finally</code> statement","text":"<p>The <code>finally</code> statement is executed whether or not an exception is thrown.</p>"},{"location":"Python/Basics/07_advanced/#throw-exceptions","title":"Throw Exceptions","text":"<p>We can intentionally throw and exception to stop the execution of a program.</p> <p>The <code>raise</code> keyword throws an excrption.</p>"},{"location":"Python/Basics/07_advanced/#kinds-of-exceptions","title":"Kinds of Exceptions","text":"<p>In Python, there are different kinds of exceptions and we can handle them individually with the <code>try...except</code> statement.</p> <pre><code>try:\n    # statements\nexcept ExceptionKind:\n    #statments\n</code></pre> <p>One of the most common kind of exceptions is the <code>NameError</code>. This is thrown when you use a variable that is not defined</p>"},{"location":"Python/Basics/07_advanced/#variable-scope","title":"Variable Scope","text":""},{"location":"Python/Basics/07_advanced/#python-variable-scopes","title":"Python Variable Scopes","text":"<p>The accessibility of variable depends on its scope. In Python, there are two variable scopes:</p> <ul> <li> Global Scope</li> <li> Local Scope</li> </ul>"},{"location":"Python/Basics/07_advanced/#global-scope","title":"Global Scope","text":"<p>A variable that is defined (created) outside a function has a global scope</p> <p>A global variable can be accessed anywhere in a program</p>"},{"location":"Python/Basics/07_advanced/#local-scope","title":"Local Scope","text":"<p>A variable that is defined (created) inside a function has a local scope. A local scope variable can only be accessed and used inside the function.</p>"},{"location":"Python/Basics/07_advanced/#the-global-keyword","title":"The <code>global</code> Keyword","text":"<p>We can force a local variable to be a global variable by using the <code>global</code> keyword.</p>"},{"location":"Python/Basics/07_advanced/#exercise","title":"Exercise","text":"<p>Develop a simple calculator to accept two floating point numbers from the keyboard. Then display a menu to the user and let him/her select a mathematical operation to be performed on those two numbers. Then display the answer. A sample run of you program should be similar to the following:</p> <pre><code>Enter number 1: 20\nEnter number 2: 12\nMathematical Operation\n-----------------------------------\n1 - Add\n2 - Subtract\n3 - Multiply\n4 - Divide\n-----------------------------------\nEnter your preference: 2\nAnswer : 8.00\n</code></pre>"},{"location":"Python/Basics/07_advanced/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Basics/08_modules/","title":"Modules","text":"<p>title: Python Modules description: Learn Programming for Data Science keywords: [python modules, math module, json module, date time module, random module, what is a python module, why use modules] author: Juma Shafara date: \"2023-11\"</p> <p></p> <pre><code># this is my_module.py\n\nfirst_name = 'Viola'\nlast_name = 'Akullu'\n\ndef add(number1, number2):\n    return number1 + number2\n\ndef multiply(number1, number2):\n    return number1 * number2\n</code></pre> <p>After that, to use <code>my_module.py</code>, we need to import it.</p> <p>To import, use the <code>import</code> statement and the module name.</p> <p>Then we can use the variables and functions in the module.</p> <p>In this example, the code below is saved as <code>main_code.py</code> and it imports the <code>module.py</code>.</p> <pre><code># this is main_code.py\n\nimport my_module\n\nfull_name = my_module.first_name + my_module.last_name\nprint('Full name:', full_name)\n\nsummation = my_module.add(3, 7)\nprint('Summation:', summation)\n</code></pre> <pre>\n<code>Full name: ViolaAkullu\nSummation: 10\n</code>\n</pre> <pre><code># this is main_code.py\n\nimport my_module as mm\n\nfull_name = mm.first_name + mm.last_name\nprint('Full name:', full_name)\n\nsummation = mm.add(3, 7)\nprint('Summation:', summation)\n</code></pre> <pre>\n<code>Full name: ViolaAkullu\nSummation: 10\n</code>\n</pre> <pre><code>from my_module import first_name\n\n# now we can use it directly as \nprint(first_name)\n</code></pre> <pre>\n<code>Viola\n</code>\n</pre> <pre><code>import my_module\n\ndir_ = dir(my_module)\n\nprint(dir_)\n</code></pre> <pre>\n<code>['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'add', 'first_name', 'last_name', 'multiply']\n</code>\n</pre> <pre><code>import sysconfig\n\npython_version = sysconfig.get_python_version()\nprint(python_version)\n</code></pre> <pre>\n<code>3.10\n</code>\n</pre> <p>We can use the math module to find the square root of a number using the <code>math.sqrt()</code> method</p> <pre><code>import math\n\nnumber = 16\nnumber_sqrt = math.sqrt(number)\n\nprint('Number:', number)\nprint('Square root of number:', number_sqrt)\n</code></pre> <pre>\n<code>Number: 16\nSquare root of number: 4.0\n</code>\n</pre> <p>We can use the math module to get the factorial of a number by using the <code>math.factorial()</code> method</p> <pre><code>import math\n\nnumber = 5\nnumber_factorial = math.factorial(number)\n\nprint('Number:', number)\nprint('Factorial:', number_factorial)\n</code></pre> <pre>\n<code>Number: 5\nFactorial: 120\n</code>\n</pre> <p>The math module also contains some constants like <code>pi</code> and <code>e</code></p> <pre><code>import math\n\nprint('e:', math.e)\nprint('pi:', math.pi)\n</code></pre> <pre>\n<code>e: 2.718281828459045\npi: 3.141592653589793\n</code>\n</pre> <p>The math module can do those and so much more</p> <p>We can generate a random number that falls within a specified range by using the <code>random.randint()</code> method</p> <pre><code>import random\n\nrandom_integer = random.randint(1,100)\nprint('Random Integer:', random_integer)\n</code></pre> <pre>\n<code>Random Integer: 4\n</code>\n</pre> <p>We can generate numbers from a gaussian distribution with mean (<code>mu</code>) as 0 and standard deviation (<code>sigma</code>) as 1</p> <pre><code>numbers = []\n\ncounter = 0\nwhile counter &amp;lt; 100:\n    numbers.append(random.gauss(mu=0, sigma=1))\n    counter += 1\n\nprint(numbers)\n</code></pre> <pre>\n<code>[0.8310128735410047, 2.402375340413018, -1.2769617295659348, 0.7569506717477539, 1.6026026122392498, 1.4142936594217554, -0.3169917649104485, -0.07305941097531603, -0.7885301448554015, -0.0674611332298377, 0.28288857512573684, 0.08844216926370602, -1.249987094506388, 0.870793290313952, -0.6607737394803138, 0.3780605189691181, 0.20288623881856632, 0.8439702923769746, 1.6500270929422152, -0.5579247768953991, -0.3076290349937902, 0.8927675985413197, -2.3716599434459114, 0.23253728473684382, 0.01698634011714592, -1.506684284668113, -1.516156046117149, -0.7549199652372819, 0.4855840249497611, -1.9426218553454226, -0.5672748318805165, 1.7849639815888045, -0.4223703532919884, -1.4182523392919628, 0.3817982448773813, -1.2151583559744263, 0.21736913499460964, 0.0743448686041854, -0.6217874541247053, -0.05369712902089164, 0.06560332100098984, 0.5791279113149166, 1.5329264216964942, -1.5523813284095307, 0.256018716284597, 1.498941708596562, 0.6484203278916434, 0.956658998431066, -0.7469607705965761, 0.9093585267915438, -0.3301676177291813, -2.1020486475752564, -0.6324768823835674, -0.2621489739923403, 0.36805271395009337, -0.1987104858441708, -0.20226660046300027, -1.0227302328088852, 0.9440428943259802, 1.3499647213634605, 0.28655811659281705, -0.48212404896946465, 1.5732404576352244, 1.7024230857294205, -0.32802550098029193, 2.0808443667109597, 2.2783854541239874, -0.265626754707208, -0.04641950638081212, 0.7941371582079103, -0.36860553191079254, -0.9098450679735101, 1.234946260813307, -2.835066105841072, 1.3883254119625694, 1.2853299658795028, 1.178005875662903, 0.3186472037221876, -1.0006920744966419, -2.3745959188263885, 1.8440465299894964, -0.35610549619690796, 0.5857012223823791, 0.7400382246661824, 0.07225122970263118, -0.5508995490344698, -0.038356750477046286, -0.040997463659922434, 0.6802546773316889, -1.3861271290488735, 0.7275261286416534, 0.3729374034245036, -0.013616473457934613, -0.7620103036607296, 0.15556952852877587, -1.7898533901375224, -1.137248630020012, -1.71518120153122, -0.5817297506694047, -0.4035542913039588]\n</code>\n</pre> <pre><code>import datetime\n\ntime_now = datetime.datetime.now()\nprint(time_now)\n</code></pre> <pre>\n<code>2024-05-01 08:18:09.070054\n</code>\n</pre> <pre><code>from datetime import date\n\ntoday = date.today()\nprint('Current date:', today)\n</code></pre> <pre>\n<code>Current date: 2024-05-01\n</code>\n</pre> <pre><code>data = \"{'first_name': 'Juma','last_name': 'Shafara', 'age': 39}\"\n\nprint(data)\n</code></pre> <pre>\n<code>{'first_name': 'Juma','last_name': 'Shafara', 'age': 39}\n</code>\n</pre> <pre><code>import json \n\ndata = '{\"first_name\": \"Juma\",\"last_name\": \"Shafara\", \"age\": 39}'\n\n# convert to dictionary\ndata_dict = json.loads(data)\n\nprint('Fist name:',data_dict['first_name'])\nprint('Last name:', data_dict['last_name'])\nprint('Age:', data_dict['age'])\n</code></pre> <pre>\n<code>Fist name: Juma\nLast name: Shafara\nAge: 39\n</code>\n</pre> <pre><code>import json\n\ndata_dict = {\n    \"first_name\": \"Juma\",\n    \"last_name\": \"Shafara\", \n    \"age\": 39\n    }\n\ndata_json = json.dumps(data_dict)\n</code></pre>"},{"location":"Python/Basics/08_modules/#modules","title":"Modules","text":"<p>A module in Python is a python file that can contain variables, functions and classes</p>"},{"location":"Python/Basics/08_modules/#why-use-modules","title":"Why use Modules?","text":"<p>Modules allow us to split our code into multiple files</p> <p>Instead of writing all our codes inside a sigle Python file, we can use modules</p> Note!<p>That way, our code will be easier to read, understand and maintain</p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Basics/08_modules/#creating-a-module","title":"Creating a Module","text":"<p>There is nothing so special with creating a module, simply write you Python code and save it with the <code>.py</code> extension.</p> <p>In this example, we have a module saved as my_module.py and it contains the following code</p>"},{"location":"Python/Basics/08_modules/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Basics/08_modules/#using-aliases","title":"Using Aliases","text":"<p>We can use an alias to refer to the module</p> <p>To use an alias, use the <code>as</code> keyword</p>"},{"location":"Python/Basics/08_modules/#importing-parts-of-a-module","title":"Importing Parts of a Module","text":"<p>We can choose to import only some specific parts of a module</p> <p>&gt;Note! When we import a part of a module, we will be able to use its variables and functions directly</p> <p>Use the <code>from</code> keyword to import a part of a module.</p> <p>In this example, we will import the <code>first_name</code> variable and access it directly</p>"},{"location":"Python/Basics/08_modules/#the-dir-function","title":"The dir() Function","text":"<p>The <code>dir()</code> function returns a list of all the variables, functions and classes available in a module</p>"},{"location":"Python/Basics/08_modules/#built-in-modules","title":"Built in Modules","text":"<p>Python has many useful built-in modules that we can use to make coding easier.</p> <p>Built-in modules can be imported without having to create them</p> <p>In this example, we will import the <code>sysconfig</code> module and use its <code>get_python_version()</code> to return the Python version we're using</p>"},{"location":"Python/Basics/08_modules/#math-module","title":"Math Module","text":"<p>The math module gives us access to mathematical functions</p> <p>To use the math module, import it first, then we can start using it.</p>"},{"location":"Python/Basics/08_modules/#random-module","title":"Random Module","text":"<p>The <code>random</code> module lets us generate a random number</p> <p>As usual, to use the <code>random</code> module, import it first.</p>"},{"location":"Python/Basics/08_modules/#date-and-time","title":"Date and Time","text":"<p>The <code>datetime</code> module allows us to work with dates</p> <p>As usual we have to import the <code>datetime</code> module to be able to use it.</p>"},{"location":"Python/Basics/08_modules/#current-date-and-time","title":"Current Date and Time","text":"<p>The <code>datetime.datetime.now()</code> method returns the current date and time</p>"},{"location":"Python/Basics/08_modules/#the-date-object","title":"The <code>date</code> Object","text":"<p>The <code>date</code> object represents a date (year, month and day)</p> <p>To create a <code>date</code> object, import it from the <code>datetime</code> module first.</p>"},{"location":"Python/Basics/08_modules/#json","title":"JSON","text":"<p>JSON stands for JavaScript Object Notation.</p> <p>JSON contains data that are sent or received to and from a server</p> <p>JSON is simply a string, if follows a format similar to a Python dictionary</p> <p>Example:</p>"},{"location":"Python/Basics/08_modules/#json-to-dictionary","title":"JSON to Dictionary","text":"<p>Before we can individually access the data of a JSON, we need to convert it to a Python dictionary first.</p> <p>To do that, we need to import the <code>json</code> module</p>"},{"location":"Python/Basics/08_modules/#dictionary-to-json","title":"Dictionary to JSON","text":"<p>To convert a dictionay to JSON, use the <code>json.dumps()</code> method.</p>"},{"location":"Python/Basics/08_modules/#exercise","title":"Exercise","text":"<p>Write a program to calculate the factorial of any given positive integer.</p>"},{"location":"Python/Basics/08_modules/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Basics/09_file_handling/","title":"File Handling","text":"<p>title: Python File Handling description: Python allows us to read, write, create and delete files. keywords: [Python File Handling, creating files in python, deleting files in python, modifying files in python, reading files in python] author: Juma Shafara date: \"2023-11\"</p> <p></p> <p>Python allows us to read, write, create and delete files. This process is called file handling. This is what we will discuss in this lesson</p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># this is main code\n\nfile = open(\n    file='demo.txt',\n    mode='r'\n)\ncontent = file.read()\nprint(content)\n</code></pre> <pre>\n<code>Hello World!\nI love Python\n</code>\n</pre> <pre><code># this is main_code.py\n\nfile = open(\n    file='demo.txt',\n    mode='r'\n)\n\nfirst_line = file.readline()\nsecond_line = file.readline()\n\nprint('First line:', first_line)\nprint('Second line:', second_line)\n</code></pre> <pre>\n<code>First line: Hello World!\n\nSecond line: I love Python\n</code>\n</pre> <pre><code># this is main_code.py\n\nfile = open(\n    file='demo.txt',\n    mode='w'\n)\nfile.write('I love JavaScript')\nfile.close()\n</code></pre> <p>When the above code is run, the content of the file <code>demo.txt</code> will be this: <pre><code>I love JavaScript\n</code></pre></p> <p>Another example, this time we will use the <code>a</code> mode which will append or add content to the end of the file</p> <pre><code># this is main_code.py\n\nfile = open(\n    file='demo.txt',\n    mode='a'\n)\nfile.write(' and JavaScript')\nfile.close()\n</code></pre> <p>When the above script is run, the content of the <code>demo.txt</code> file will be this: <pre><code>I love Python and JavaScript\n</code></pre></p> <pre><code># this is main_code.py\n\n# import os\n\n# os.remove('demo.txt')\n</code></pre>"},{"location":"Python/Basics/09_file_handling/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Basics/09_file_handling/#the-open-function","title":"The <code>open()</code> function","text":"<p>The <code>open()</code> function allows us to read, create and update files</p> <p>It takes 2 parameters:</p> <ul> <li><code>file</code> - the file or file path to be opened</li> <li><code>mode</code> - the mode in which a file is opened for</li> </ul> <p>The mode is a string that can either be any of the following:</p> Mode Meaning <code>'r'</code> Open a file for reading <code>'w'</code> Open a file for writing, creates the file if it does not exist <code>'a'</code> Open a file for writing, appends to the end of the file <code>'x'</code> Open a file for creating, fails if file already exists"},{"location":"Python/Basics/09_file_handling/#python-file-reading","title":"Python File reading","text":"<p>To better explain this, let us say we have a folder named <code>my_folder</code>.</p> <p>Inside <code>my_folder</code> we have the following files:</p> <ul> <li><code>demo.txt</code></li> <li><code>main_code.py</code></li> </ul> <p>The content of the <code>demo.txt</code> file is the following</p> <p><pre><code>Hello World!\nI love Python\n</code></pre> Now our goal is to read the content of the <code>demo.txt</code> file and then print it using the <code>main_code.py</code> file</p> <p>To achieve this, we will use the <code>open()</code> function with <code>'r'</code> mode.</p>"},{"location":"Python/Basics/09_file_handling/#reading-lines","title":"Reading Lines","text":"<p>We can also read each line using the <code>readline()</code> method.</p>"},{"location":"Python/Basics/09_file_handling/#writing-a-file","title":"Writing a File","text":"<p>In simplest terms, writing a file means modifying the content of a file or creating it if it doesnot exist yet.</p> <p>In Python, there are 2 modes to write to file.</p> <ul> <li><code>'w'</code> - overwrites content of a file, creates file if it does not exist</li> <li><code>'a'</code> - appends content to the end of a file, creates the file if it does not exist</li> </ul> <p>Example To better explain this, lets say we have a folder named <code>my_folder</code>. Inside <code>my_folder</code> we have the following files</p> <ul> <li><code>demo.txt</code></li> <li><code>main_code.py</code></li> </ul> <p>The content of the <code>demo.txt</code> file is the following</p> <p><pre><code>I love Python\n</code></pre> In this example, we will use the <code>'w'</code> mode which will overwrite(replace) the content of the file</p>"},{"location":"Python/Basics/09_file_handling/#deleting-a-file","title":"Deleting a file","text":"<p>To delete a file, use the <code>os</code> module. The <code>os</code> modules contains the <code>remove()</code> method which we can use to delete files.</p>"},{"location":"Python/Basics/09_file_handling/#exercise","title":"Exercise","text":"<ol> <li>Develop a simple telephone directory which saves your friends contact information in a file named directory.txt. The program should have a menu similar to the following: <pre><code>----------------Menu-------------------------\n1. Add new friend.\n2. Display contact info.\n3. Exit\n------------------------------------------------\nEnter menu number:\n</code></pre> When you press \u201c1\u201d it should request you to enter following data: <pre><code>---------New friend info--------\nName : Saman\nPhone-No: 011-2123456\ne-Mail : saman@cse.mrt.ac.lk\n</code></pre> After adding new contact information it should again display the menu. When you press \u201c2\u201d it should display all the contact information stored in the directory.txt file as follows:</li> </ol> <pre><code>--------------Contact info---------------\nName            Tel-No          e-Mail\nKamala          077-7123123     kamala@yahoo.com\nKalani          033-4100101     kalani@gmail.com\nSaman           011-2123456     saman@cse.mrt.ac.lk\n-----------------------------------------\n</code></pre>"},{"location":"Python/Basics/09_file_handling/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Basics/13_end_of_course_exercise/","title":"End of Course Exercise","text":"<p>title: End of Course Exam keywords: [python, quiz, python quiz, dataidea, data science,] author: Juma Shafara description: Test your python basics knowledge with this Exam date: \"2024-08-12\" modified: \"2024-09-09\"</p> <p></p> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to assign the number 34.5678 to a variable named \u201cnumber\u201d then display the number rounded to the nearest integer value and next the number rounded to two decimal places.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to identify whether a number input from the keyboard is even or odd. If it is even, the program should display the message \u201cNumber is even\u201d, else it should display \u201cNumber is odd\u201d.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display the student\u2019s grade based on the following table:</li> </ol> Marks Grade &gt;= 75 A &gt; 50 and &lt;=75 B &gt; 25 and &lt;=50 C &lt;= 25 D <pre><code># your solution\n</code></pre> <ol> <li>Write a program to convert a given character from uppercase to lowercase and vice versa.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program which accepts a number (an amount of money to be paid by a customer in rupees) entered from the keyboard. If the amount is greater than or equal to 1000 rupees, a 5% discount is given to the customer. Then display the final amount that the customer has to pay.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>A car increases it velocity from u ms-1 to v ms-1 within t seconds. Write a program to calculate the acceleration.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to input a temperature reading in either Celsius(c) or Fahrenheit(f) scale and convert it to the other scale. The temperature reading consists of a decimal number followed by letter \u201dF\u201d or \u201df\u201d if the temperature is in Fahrenheit scale or letter \u201dC\u201d or \u201dc\u201d if it is in Celsius scale. You may use a similar format for the output of the program. Note: <code>c = 5( f \u2212 32) / 9</code></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Develop a simple calculator to accept two floating point numbers from the keyboard. Then display a menu to the user and let him/her select a mathematical operation to be performed on those two numbers. Then display the answer. A sample run of you program should be similar to the following:</li> </ol> <pre><code>Enter number 1: 20\nEnter number 2: 12\nMathematical Operation\n-----------------------------------\n1 - Add\n2 - Subtract\n3 - Multiply\n4 - Divide\n-----------------------------------\nEnter your preference: 2\nAnswer : 8.00\n</code></pre> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display all the integers from 100 to 200.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to calculate the sum of all the even numbers up to 100.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to compute the sum of all integers form 1 to 100.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to calculate the factorial of any given positive integer.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to compute the sum of all integers between any given two numbers. In this program both inputs should be given from the keyboard.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Modify the above so that it works only for positive integers.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display the following symbol pattern: <pre><code>*\n**\n***\n****\n*****\n******\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display the message \u201cHello World!\u201d 10000 times. The program should allow users to terminate the program at any time by pressing any key before it displays all the 10000 messages.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display a sine table. The program should display all the sine values from 0 to 360 degrees (at 5 degrees increments) and it should display only 20 rows at a time</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to store marks of 5 students for 5 subjects given through the keyboard. Calculate the average of each students marks and the average of marks taken by all the students</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to calculate the circumference and area of a circle given its radius. Implement calculation of circumference and areas as separate functions.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to read the file \u201cmy file.txt\u201d which has the message: <pre><code>Hello World!\nThis is my first file\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a Python program to store the message \u201cIntroduction Python Programming\u201d in a file named \u201cmessage.txt\u201d.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display the following menu on the screen and let the user select a menu item. Based on the user\u2019s selection display the category of software that the user selected program belongs to. <pre><code>Menu\n-----------------------------------\n1 \u2013 Microsoft Word\n2 \u2013 Yahoo messenger\n3 \u2013 AutoCAD\n4 \u2013 Java Games\n-----------------------------------\nEnter number of your preference:\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Develop a simple telephone directory which saves your friends contact information in a file named directory.txt. The program should have a menu similar to the following: <pre><code>----------------Menu-------------------------\n1. Add new friend.\n2. Display contact info.\n3. Exit\n------------------------------------------------\nEnter menu number:\n</code></pre> When you press \u201c1\u201d it should request you to enter following data: <pre><code>---------New friend info--------\nName : Saman\nPhone-No: 011-2123456\ne-Mail : saman@cse.mrt.ac.lk\n</code></pre> After adding new contact information it should again display the menu. When you press \u201c2\u201d it should display all the contact information stored in the directory.txt file as follows:</li> </ol> <pre><code>--------------Contact info---------------\nName            Tel-No          e-Mail\nKamala          077-7123123     kamala@yahoo.com\nKalani          033-4100101     kalani@gmail.com\nSaman           011-2123456     saman@cse.mrt.ac.lk\n-----------------------------------------\n</code></pre> <pre><code># your solution\n</code></pre> <ol> <li>Given a date as a triplet of numbers (y, m, d), with y indicating the year, m the month (m = 1 for January, m = 2 for February, etc.), and d the day of the month, the corresponding day of the week f (f = 0 for Sunday, f = 1 for Monday, etc.) can be found as follows: <pre><code>(a) if m &amp;lt; 3\n(b) let m = m + 12 and let y = y - 1\n(c) let a = 2m + 6 (m + 1) / 10\n(d) let b = y + y/4 \u2013 y/100 + y/400\n(e) let f 1 = d + a + b + 1\n(t) let f = f 1 mod 7\n(g) stop.\n</code></pre> Write a program that will read a date and print the corresponding day of the week. All divisions indicated above are integer divisions.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to input a series of positive integers and determine whether they are prime. The program should terminate if a negative integer is given as the input. A prime number is a number that is divisible by only one and itself. However one is not considered a prime number.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to find out whether a given number is a perfect number. The program should terminate if a negative integer is given as the input. A perfect number is a number whose factors other than itself add up to itself.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to find and display the minimum and the maximum among 10 numbers entered from the keyboard. Use a single-dimensional array to store the numbers entered. The numbers can be non-integers. An example would be as follows: <pre><code>Enter 10 numbers: 5 7.8 9.6 54 3.4 1.2 3 7 8.8 5\nMinimum = 1.2\nMaximum = 54\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Suppose there are 4 students each having marks of 3 subjects. Write a program to read the marks from the keyboard and calculate and display the total marks of each student. Use a 2D (two-dimensional) array to store the marks. An example would be as follows: <pre><code>Enter the marks of four students, on four rows:\n50 60 80\n60 75 90\n30 49 99\n66 58 67\n\nTotal marks of four students:\n190\n225\n178\n191\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to read in two matrices A and B of dimensions 3\u00d74 and 4\u00d73 respectively, and compute and display their product AB (of dimensions 3\u00d73). Assume that the elements of the matrices are integers. Use functions to while implementing this program.</li> </ol> <pre><code># your solution\n</code></pre>"},{"location":"Python/Basics/13_end_of_course_exercise/#objectives","title":"Objectives","text":"<ul> <li>Write and run Python code</li> <li>Practice problem solving using Python code.</li> <li>Learn how to detect programming errors through program testing.</li> <li>Correct/fix syntax errors and bugs.</li> </ul> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Basics/13_end_of_course_exercise/#prerequisites","title":"Prerequisites","text":"<ul> <li>Students are expected to be familiar with a text editor.</li> <li>A basic understanding of running and linking in concept.</li> <li>Knowledge in area such as Python language syntax, input output function is recommended.</li> </ul>"},{"location":"Python/Basics/13_end_of_course_exercise/#questions","title":"Questions","text":"<ol> <li>Write a Python program to display the following text on the screen.</li> </ol> <pre><code>DATAIDEA\nSir Apollo Kagwa Road,\nKampala,\nUganda\n-------------------\nwww.dataidea.org\n</code></pre>"},{"location":"Python/Basics/13_end_of_course_exercise/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Basics/13_end_of_course_exercise/#congratulation","title":"Congratulation!","text":"<p>Congratulations on completing this Python Fundamentals Course</p>"},{"location":"Python/Basics/13_end_of_course_exercise/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Exercises/02_basics_exercise/","title":"Basics Exercise","text":"<p>title: Python Basics Quiz keywords: [What is Python, What is Python used for?, Python Displaying output] description: In this notebook, I want to observe any trends related to customers. author: Juma Shafara date: \"2024-09-03\"</p> <p></p>"},{"location":"Python/Exercises/02_basics_exercise/#basic-quiz","title":"Basic Quiz","text":"<ol> <li> <p>Which command is used to install Python on Windows?    A) <code>python install</code>    B) <code>pip install python</code>    C) Download the installer from python.org and run it    D) <code>sudo apt-get install python</code></p> </li> <li> <p>How do you start writing a Python script?</p> </li> </ol> <p>A) Start with <code>main()</code> function    B) Write code directly without a main function    C) Include a header file    D) Use <code>int main()</code></p> <ol> <li> <p>Which extension is used for Python files?    A) <code>.py</code>    B) <code>.java</code>    C) <code>.c</code>    D) <code>.cpp</code></p> </li> <li> <p>Which function is used to display output in Python?</p> </li> </ol> <p>A) <code>output()</code>    B) <code>echo()</code>    C) <code>printf()</code>    D) <code>print()</code></p> <ol> <li>How do you display the string \"Hello, World!\" in Python?</li> </ol> <p>A) <code>echo \"Hello, World!\"</code>    B) <code>print(\"Hello, World!\")</code>    C) <code>printf(\"Hello, World!\")</code>    D) <code>output(\"Hello, World!\")</code></p> <ol> <li> <p>Which symbol is used for indentation in Python?    A) Curly braces <code>{}</code>    B) Semicolon <code>;</code>    C) Tab or spaces    D) Colon `:</p> </li> <li> <p>How do you write a comment in Python?</p> </li> </ol> <p>A) <code>// This is a comment</code>    B) <code># This is a comment</code>    C) <code>/* This is a comment */</code>    D) <code></code></p> <ol> <li>Which of the following is a multi-line comment in Python?    A) <code>''' This is a comment '''</code>    B) <code># This is a comment</code>    C) <code>// This is a comment</code>    D) <code>/* This is a comment */</code></li> </ol>"},{"location":"Python/Exercises/02_basics_exercise/#end","title":"END","text":""},{"location":"Python/Exercises/11_python_objective_quiz/","title":"Objective Quiz","text":"<p>title: Python Objective Quiz keywords: [python, python quiz, python data types, python functions] description: Test your knowledge in the basics of Python author: Juma Shafara date: \"2024-2\"</p> <p></p> <p>Test your knowledge in the basics of Python</p>"},{"location":"Python/Exercises/11_python_objective_quiz/#question-1-what-is-the-output-of-the-following-code","title":"Question 1: What is the output of the following code?","text":"<pre><code>print(type([]) is list)\n</code></pre> <p>a) <code>True</code> b) <code>False</code> c) <code>Error</code> d) <code>None</code></p> Reveal answer <p>a.True</p>"},{"location":"Python/Exercises/11_python_objective_quiz/#question-2-which-of-the-following-is-not-a-valid-python-data-type","title":"Question 2: Which of the following is not a valid Python data type?","text":"<p>a) <code>list</code> b) <code>tuple</code> c) <code>dictionary</code> d) <code>array</code></p> Reveal answer <p>d.array</p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Exercises/11_python_objective_quiz/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Exercises/11_python_objective_quiz/#question-3-what-does-the-len-function-do","title":"Question 3: What does the <code>len()</code> function do?","text":"<p>a) Returns the number of characters in a string b) Returns the number of items in a list or tuple c) Returns the number of key-value pairs in a dictionary d) All of the above</p> Reveal answer <p>d.All of the above</p>"},{"location":"Python/Exercises/11_python_objective_quiz/#question-4-how-do-you-create-a-set-in-python","title":"Question 4: How do you create a set in Python?","text":"<p>a) <code>my_set = {}</code> b) <code>my_set = []</code> c) <code>my_set = ()</code> d) <code>my_set = set()</code></p> Reveal answer <p>d.my_set = set()</p>"},{"location":"Python/Exercises/11_python_objective_quiz/#question-5-what-is-the-output-of-the-following-code","title":"Question 5: What is the output of the following code?","text":"<pre><code>x = \"Python\"\nprint(x[0])\n</code></pre> <p>a) <code>P</code> b) <code>y</code> c) <code>h</code> d) <code>n</code></p> Reveal answer <p>a.P</p>"},{"location":"Python/Exercises/11_python_objective_quiz/#question-6-which-of-the-following-methods-can-be-used-to-add-an-element-to-the-end-of-a-list-in-python","title":"Question 6: Which of the following methods can be used to add an element to the end of a list in Python?","text":"<p>a) <code>append()</code> b) <code>add()</code> c) <code>insert()</code> d) <code>extend()</code></p> Reveal answer <p>a.append</p>"},{"location":"Python/Exercises/11_python_objective_quiz/#question-7-what-is-the-output-of-the-following-code","title":"Question 7: What is the output of the following code?","text":"<pre><code>for i in range(3):\n    print(i)\n</code></pre> <p>a) <code>1 2 3</code> b) <code>0 1 2</code> c) <code>0 1 2 3</code> d) <code>1 2 3 4</code></p> Reveal answer <p>b.0 1 2</p>"},{"location":"Python/Exercises/11_python_objective_quiz/#question-8-how-do-you-create-a-function-in-python","title":"Question 8: How do you create a function in Python?","text":"<p>a) <code>function my_func():</code> b) <code>def my_func:</code> c) <code>def my_func():</code> d) <code>func my_func():</code></p> Reveal answer <p>b.def my_func</p>"},{"location":"Python/Exercises/11_python_objective_quiz/#question-9-what-will-be-the-output-of-the-following-code","title":"Question 9: What will be the output of the following code?","text":"<pre><code>print(2**3)\n</code></pre> <p>a) <code>6</code> b) <code>8</code> c) <code>9</code> d) <code>11</code></p> Reveal answer <p>b.8</p>"},{"location":"Python/Exercises/11_python_objective_quiz/#question-10-which-keyword-is-used-to-handle-exceptions-in-python","title":"Question 10: Which keyword is used to handle exceptions in Python?","text":"<p>a) <code>try</code> b) <code>catch</code> c) <code>except</code> d) <code>handle</code></p> Reveal answer <p>c.except</p>"},{"location":"Python/Exercises/11_python_objective_quiz/#how-many-did-you-get-put-it-in-the-comments","title":"How many did you get? Put it in the comments!","text":""},{"location":"Python/Exercises/12_true_or_false_quiz/","title":"True / False Quiz","text":"<p>title: True or False keywords: [python, quiz, python quiz, dataidea, data science,] author: Juma Shafara description: Test your python basics knowledge with this True or Fals Quiz date: \"2024-03\"</p> <p></p> <p>For this Python Quiz Respond with Either True or False only. Answers are provided at the bottom.</p> <ol> <li>True or False:    In Python, the <code>__init__</code> method is called a constructor and is automatically invoked when a new instance of a class is created.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:    The <code>global</code> keyword allows you to modify a variable outside of the current scope.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:    In Python, <code>args</code> and <code>kwargs</code> are used to pass a variable number of arguments to a function.</li> </ol> <pre><code># your answer\n</code></pre> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <ol> <li>True or False:    Python's <code>set</code> data type does not allow duplicate elements and is unordered.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:    The <code>map</code> function in Python applies a given function to each item of an iterable (like a list) and returns a list of the results.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:    List comprehensions in Python can include conditional statements.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:    In Python, the <code>finally</code> block will be executed whether or not an exception is raised in the <code>try</code> block.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:    The <code>self</code> parameter in class methods refers to the instance of the class.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:    Python supports method overloading, which means you can define multiple methods with the same name but different arguments.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:     The <code>is</code> operator compares the values of two objects to check for equality.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:     The <code>yield</code> keyword is used in Python to create a generator.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:     In Python, a dictionary key must be immutable and unique.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:     Python's <code>re</code> module can be used to perform operations on strings using regular expressions.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:     The <code>del</code> keyword in Python can be used to delete an item from a list by index.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:     In Python, <code>__name__ == \"__main__\"</code> is used to check if the script is being run on its own or being imported somewhere else.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:     Python's <code>copy</code> module provides a function <code>deepcopy</code> to create a deep copy of an object.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:     The <code>super</code> function in Python is used to call a method from the parent class.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:     Python supports both procedural and object-oriented programming paradigms.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:     The <code>join</code> method in Python is used to concatenate a list of strings into a single string, separated by a specified delimiter.</li> </ol> <pre><code># your answer\n</code></pre> <ol> <li>True or False:     Python's <code>functools</code> module provides higher-order functions that act on or return other functions.</li> </ol> <pre><code># your answer\n</code></pre>"},{"location":"Python/Exercises/12_true_or_false_quiz/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Exercises/12_true_or_false_quiz/#answers","title":"Answers","text":"<ol> <li>True</li> <li>True</li> <li>True</li> <li>True</li> <li>False (The <code>map</code> function returns a map object, which is an iterator, not a list)</li> <li>True</li> <li>True</li> <li>True</li> <li>False (Python does not support method overloading like other languages; you can define methods with default parameters instead)</li> <li>False (The <code>is</code> operator checks for object identity, not equality)</li> <li>True</li> <li>True</li> <li>True</li> <li>True</li> <li>True</li> <li>True</li> <li>True</li> <li>True</li> <li>True</li> <li>True</li> </ol>"},{"location":"Python/Exercises/12_true_or_false_quiz/#how-many-did-you-get-put-it-in-the-comments","title":"How many did you get? Put it in the comments!","text":""},{"location":"Python/Exercises/13_end_of_course_exercise/","title":"13 end of course exercise","text":"<p>title: Exam Solutions keywords: [python, quiz, python quiz, dataidea, data science,] author: Juma Shafara description: Test your python basics knowledge with this Exam date: \"2024-08-12\" modified: \"2024-09-09\"</p> <p></p> <pre><code>print('''\nDATAIDEA\nSir Apollo Kagwa Road,\nKampala,\nUganda\n-------------------\nwww.dataidea.org\n''')\n</code></pre> <pre>\n<code>\nDATAIDEA\nSir Apollo Kagwa Road,\nKampala,\nUganda\n-------------------\nwww.dataidea.org\n\n</code>\n</pre> <ol> <li>Write a program to assign the number 34.5678 to a variable named \u201cnumber\u201d then display the number rounded to the nearest integer value and next the number rounded to two decimal places.</li> </ol> <pre><code>number = 34.5678\nprint('Nearest integer:', round(number))\nprint('2 decimal places:', round(number, 2))\n</code></pre> <pre>\n<code>Nearest integer: 35\n2 decimal places: 34.57\n</code>\n</pre> <ol> <li>Write a program to identify whether a number input from the keyboard is even or odd. If it is even, the program should display the message \u201cNumber is even\u201d, else it should display \u201cNumber is odd\u201d.</li> </ol> <pre><code>number = int(input('Enter number: '))\n\nif number % 2 == 0:\n    print('Number is even')\nelse:\n    print('Number is odd')\n</code></pre> <pre>\n<code>Number is even\n</code>\n</pre> <ol> <li>Write a program to display the student\u2019s grade based on the following table:</li> </ol> Marks Grade &gt;= 75 A &gt; 50 and &lt;=75 B &gt; 25 and &lt;=50 C &lt;= 25 D <pre><code>marks = 56\n\nif marks &amp;gt;= 75:\n    print('A')\nelif marks &amp;gt; 50 and marks &amp;lt;= 75:\n    print('B')\nelif marks &amp;gt; 25 and marks &amp;lt;= 50:\n    print('C')\nelse:\n    print('D')\n</code></pre> <pre>\n<code>B\n</code>\n</pre> <ol> <li>Write a program to convert a given character from uppercase to lowercase and vice versa.</li> </ol> <pre><code># your solution\n\nchar = 'a'\n\n# to lower\nif char.isupper():\n    char = char.lower()\nelse:\n    char = char.upper()\n\nprint(char)\n</code></pre> <pre>\n<code>A\n</code>\n</pre> <ol> <li>Write a program which accepts a number (an amount of money to be paid by a customer in rupees) entered from the keyboard. If the amount is greater than or equal to 1000 rupees, a 5% discount is given to the customer. Then display the final amount that the customer has to pay.</li> </ol> <pre><code># your solution\n\namount_to_pay = float(input(\"Enter amount to pay: \"))\n\nif amount_to_pay &amp;gt;= 1000:\n    amount_to_pay = amount_to_pay * 0.95\n\nprint(amount_to_pay)\n</code></pre> <pre>\n<code>1140.0\n</code>\n</pre> <ol> <li>A car increases it velocity from u ms-1 to v ms-1 within t seconds. Write a program to calculate the acceleration.</li> </ol> <pre><code># your solution\n\nu = float(input(\"Enter the initial velocity (m/s): \"))\nv = float(input(\"Enter the final velocity (m/s): \"))\nt = float(input(\"Enter the time (s): \"))\n\ngetAcceleration = lambda u, v, t: (v - u) / t\n\nprint(\"The acceleration is:\", getAcceleration(u, v, t), \"m/s^2\")\n</code></pre> <pre>\n<code>The acceleration is: 1.0 m/s^2\n</code>\n</pre> <ol> <li>Write a program to input a temperature reading in either Celsius(c) or Fahrenheit(f) scale and convert it to the other scale. The temperature reading consists of a decimal number followed by letter \u201dF\u201d or \u201df\u201d if the temperature is in Fahrenheit scale or letter \u201dC\u201d or \u201dc\u201d if it is in Celsius scale. You may use a similar format for the output of the program. Note: <code>c = 5( f \u2212 32) / 9</code></li> </ol> <pre><code># your solution\n\ntemperature_reading = input(\"Enter the temperature: \")\n\nunit = str(temperature_reading[-1])\ntemperature_value = float(temperature_reading[:-1])\n\nif unit.lower() == \"c\":\n    fahrenheit = (temperature_value * 9/5) + 32\n    print(f\"{temperature_value}\u00b0C is equivalent to {round(fahrenheit, 3)}\u00b0F\")\nelse:\n    celsius = (temperature_value - 32) * 5/9\n    print(f\"{temperature_value}\u00b0F is equivalent to {round(celsius, 3)}\u00b0C\")\n</code></pre> <pre>\n<code>10.0\u00b0F is equivalent to -12.222\u00b0C\n</code>\n</pre> <ol> <li>Develop a simple calculator to accept two floating point numbers from the keyboard. Then display a menu to the user and let him/her select a mathematical operation to be performed on those two numbers. Then display the answer. A sample run of you program should be similar to the following:</li> </ol> <pre><code>Enter number 1: 20\nEnter number 2: 12\nMathematical Operation\n-----------------------------------\n1 - Add\n2 - Subtract\n3 - Multiply\n4 - Divide\n-----------------------------------\nEnter your preference: 2\nAnswer : 8.00\n</code></pre> <pre><code># your solution\n# Develop a simple calculator to accept two floating point numbers from the keyboard.\n# Then display a menu to the user and let him/her select a mathematical operation to be performed on\n# those two numbers. Then display the answer.\n\nnumber1 = int(input(\"Enter the number 1: \"))\nnumber2 = int(input(\"Enter the number 2: \"))\n\noperation = input(\n    '''\nMathematical Operation\n-----------------------------------\n1 - Add\n2 - Subtract\n3 - Multiply\n4 - Divide\n-----------------------------------\nEnter your preference: \n'''\n)\n\noperation_map = {\n    '1': number1 + number2,\n    '2': number1 - number2,\n    '3': number1 * number2,\n    '4': number1 / number2\n}\n\nprint(f'Answer: {operation_map[operation]}')\n</code></pre> <pre>\n<code>Answer: 10\n</code>\n</pre> <ol> <li>Write a program to display all the integers from 100 to 200.</li> </ol> <pre><code># your solution\n\nintegers = list(range(100, 201))\nprint(integers)\n</code></pre> <pre>\n<code>[100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]\n</code>\n</pre> <ol> <li>Write a program to calculate the sum of all the even numbers up to 100.</li> </ol> <pre><code># your solution\neven_integers = list(range(100, 201, 2))\nprint('Sum:', sum(even_integers))\n</code></pre> <pre>\n<code>Sum: 7650\n</code>\n</pre> <ol> <li>Write a program to compute the sum of all integers form 1 to 100.</li> </ol> <pre><code># your solution\n\nintegers = list(range(101))\nprint('Sum:', sum(integers))\n</code></pre> <pre>\n<code>Sum: 5050\n</code>\n</pre> <ol> <li>Write a program to calculate the factorial of any given positive integer.</li> </ol> <pre><code># your solution\n# factorial\ndef getFactorial(n):\n    if n == 0:\n        return 1\n    else:\n        return getFactorial(n - 1) * n\n\nprint(getFactorial(5))\n</code></pre> <pre>\n<code>120\n</code>\n</pre> <ol> <li>Write a program to compute the sum of all integers between any given two numbers. In this program both inputs should be given from the keyboard.</li> </ol> <pre><code># your solution\n\ndef getSumBetween(start, end):\n    sum = 0\n    for counter in range(start, end+1):\n        sum += counter\n    return sum\n\nprint(getSumBetween(1, 10)) # 55\n</code></pre> <pre>\n<code>55\n</code>\n</pre> <ol> <li>Modify the above so that it works only for positive integers.</li> </ol> <pre><code># your solution\n\ndef getSumBetween(start, end):    \n    if start &amp;lt; 0 or end &amp;lt; 0:\n        print('Error: Start or end cannot be negative')\n        return -1\n\n    sum = 0\n    for i in range(start, end+1):\n        sum += i\n    return sum\n\nprint(getSumBetween(-2, 5))\n</code></pre> <pre>\n<code>Error: Start or end cannot be negative\n-1\n</code>\n</pre> <ol> <li>Write a program to display the following symbol pattern: <pre><code>*\n**\n***\n****\n*****\n******\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display the message \u201cHello World!\u201d 10000 times. The program should allow users to terminate the program at any time by pressing any key before it displays all the 10000 messages.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display a sine table. The program should display all the sine values from 0 to 360 degrees (at 5 degrees increments) and it should display only 20 rows at a time</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to store marks of 5 students for 5 subjects given through the keyboard. Calculate the average of each students marks and the average of marks taken by all the students</li> </ol> <pre><code># Write a program to store marks of 5 students for 5 subjects given through \n# the keyboard. Calculate the average of each students marks and the average \n# of marks taken by all the students\n\n# your solution\nstudent_marks = []\nfor i in range(5):\n    student_marks.append([])\n    for j in range(5):\n        mark = int(input(f\"Enter marks for student {i+1}, subject {j+1}: \"))\n        student_marks[i].append(mark)\n\nfor i in range(5):\n    student_average = sum(student_marks[i]) / len(student_marks[i])\n    print(f\"Average marks for student {i+1}: {student_average:.2f}\")\n\nall_marks = []\nfor student in student_marks:\n    all_marks.extend(student)\n\noverall_average = sum(all_marks) / len(all_marks)\nprint(f\"Overall average of all students: {overall_average:.2f}\")\n</code></pre> <ol> <li>Write a program to calculate the circumference and area of a circle given its radius. Implement calculation of circumference and areas as separate functions.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to read the file \u201cmy file.txt\u201d which has the message: <pre><code>Hello World!\nThis is my first file\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a Python program to store the message \u201cIntroduction Python Programming\u201d in a file named \u201cmessage.txt\u201d.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display the following menu on the screen and let the user select a menu item. Based on the user\u2019s selection display the category of software that the user selected program belongs to. <pre><code>Menu\n-----------------------------------\n1 \u2013 Microsoft Word\n2 \u2013 Yahoo messenger\n3 \u2013 AutoCAD\n4 \u2013 Java Games\n-----------------------------------\nEnter number of your preference:\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Develop a simple telephone directory which saves your friends contact information in a file named directory.txt. The program should have a menu similar to the following: <pre><code>----------------Menu-------------------------\n1. Add new friend.\n2. Display contact info.\n3. Exit\n------------------------------------------------\nEnter menu number:\n</code></pre> When you press \u201c1\u201d it should request you to enter following data: <pre><code>---------New friend info--------\nName : Saman\nPhone-No: 011-2123456\ne-Mail : saman@cse.mrt.ac.lk\n</code></pre> After adding new contact information it should again display the menu. When you press \u201c2\u201d it should display all the contact information stored in the directory.txt file as follows:</li> </ol> <pre><code>--------------Contact info---------------\nName            Tel-No          e-Mail\nKamala          077-7123123     kamala@yahoo.com\nKalani          033-4100101     kalani@gmail.com\nSaman           011-2123456     saman@cse.mrt.ac.lk\n-----------------------------------------\n</code></pre> <pre><code># your solution\n</code></pre> <ol> <li>Given a date as a triplet of numbers (y, m, d), with y indicating the year, m the month (m = 1 for January, m = 2 for February, etc.), and d the day of the month, the corresponding day of the week f (f = 0 for Sunday, f = 1 for Monday, etc.) can be found as follows: <pre><code>(a) if m &amp;lt; 3\n(b) let m = m + 12 and let y = y - 1\n(c) let a = 2m + 6 (m + 1) / 10\n(d) let b = y + y/4 \u2013 y/100 + y/400\n(e) let f 1 = d + a + b + 1\n(t) let f = f 1 mod 7\n(g) stop.\n</code></pre> Write a program that will read a date and print the corresponding day of the week. All divisions indicated above are integer divisions.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to input a series of positive integers and determine whether they are prime. The program should terminate if a negative integer is given as the input. A prime number is a number that is divisible by only one and itself. However one is not considered a prime number.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to find out whether a given number is a perfect number. The program should terminate if a negative integer is given as the input. A perfect number is a number whose factors other than itself add up to itself.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to find and display the minimum and the maximum among 10 numbers entered from the keyboard. Use a single-dimensional array to store the numbers entered. The numbers can be non-integers. An example would be as follows: <pre><code>Enter 10 numbers: 5 7.8 9.6 54 3.4 1.2 3 7 8.8 5\nMinimum = 1.2\nMaximum = 54\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Suppose there are 4 students each having marks of 3 subjects. Write a program to read the marks from the keyboard and calculate and display the total marks of each student. Use a 2D (two-dimensional) array to store the marks. An example would be as follows: <pre><code>Enter the marks of four students, on four rows:\n50 60 80\n60 75 90\n30 49 99\n66 58 67\n\nTotal marks of four students:\n190\n225\n178\n191\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to read in two matrices A and B of dimensions 3\u00d74 and 4\u00d73 respectively, and compute and display their product AB (of dimensions 3\u00d73). Assume that the elements of the matrices are integers. Use functions to while implementing this program.</li> </ol> <pre><code># your solution\n</code></pre>"},{"location":"Python/Exercises/13_end_of_course_exercise/#objectives","title":"Objectives","text":"<ul> <li>Write and run Python code</li> <li>Practice problem solving using Python code.</li> <li>Learn how to detect programming errors through program testing.</li> <li>Correct/fix syntax errors and bugs.</li> </ul> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Exercises/13_end_of_course_exercise/#prerequisites","title":"Prerequisites","text":"<ul> <li>Students are expected to be familiar with a text editor.</li> <li>A basic understanding of running and linking in concept.</li> <li>Knowledge in area such as Python language syntax, input output function is recommended.</li> </ul>"},{"location":"Python/Exercises/13_end_of_course_exercise/#questions","title":"Questions","text":"<ol> <li>Write a Python program to display the following text on the screen.</li> </ol> <pre><code>DATAIDEA\nSir Apollo Kagwa Road,\nKampala,\nUganda\n-------------------\nwww.dataidea.org\n</code></pre>"},{"location":"Python/Exercises/13_end_of_course_exercise/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Exercises/13_end_of_course_exercise/#congratulation","title":"Congratulation!","text":"<p>Congratulations on completing this Python Fundamentals Course</p>"},{"location":"Python/Exercises/13_end_of_course_exercise/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Libraries/12_numpy/","title":"NumPy Crash Course","text":"Meta Data  title: Numpy Crash Course author: Juma Shafara date: \"2024-01\" date-modified: \"2025-12-27\" description: This crash course will teach you the basics and intermediate concepts of the Numpy Library keywords: [numpy, data types, array mathematics, aggregate functions, Subsetting, Slicing, Indexing]   <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># Python list\npy_list = [1, 2, 3]\npy_list * 2   # duplicates list\n</code></pre> <pre>\n<code>[1, 2, 3, 1, 2, 3]</code>\n</pre> <pre><code># NumPy array\nnp_arr = np.array([1, 2, 3])\nnp_arr * 2    # element-wise multiplication\n</code></pre> <pre>\n<code>array([2, 4, 6])</code>\n</pre> <pre><code>## Uncomment and run this cell to install numpy\n# !pip install numpy\n</code></pre> <pre><code> # import numpy module\nimport numpy as np\n</code></pre> <p>We can check the version we'll be using by using the <code>__version__</code> method</p> <pre><code># checking the numpy version\nnp.__version__\n</code></pre> <pre>\n<code>'2.3.4'</code>\n</pre> <p>Numpy gives us a more powerful Python List alternative data structure called a Numpy ndarray, we creat it using the <code>array()</code> from numpy</p> <pre><code># creating a numpy array\nnum_arr = np.array([1, 2, 3, 4])\n</code></pre> <p>The object that's created by <code>array()</code> is called <code>ndarray</code>. This can be shown by checking the type of the object using <code>type()</code></p> <pre><code># Checking type of object\ntype(num_arr)\n</code></pre> <pre>\n<code>numpy.ndarray</code>\n</pre> <p>Dimensions:</p> <p>A dimension is a direction or axis along which data is organized in an array. We find the the number of dimensions in our array using the <code>ndim</code> attribute. A dimension in NumPy refers to the number of axes or levels of depth in an array, determining its shape (e.g., 2D for a matrix, 3D for a tensor).</p> <pre><code># finding the number of dimensions\nnum_arr.ndim\n</code></pre> <pre>\n<code>1</code>\n</pre> <p>Shape:</p> <p>Refers to a tuple describing the size of each dimension of an array. We can check the shape of a numpy array by using the <code>shape</code> attribute as demonstrated below.</p> <pre><code># shape of array\nnum_arr.shape\n</code></pre> <pre>\n<code>(4,)</code>\n</pre> <p>Length</p> <p>In NumPy, the length refers to the size of the first axis (dimension) of an array, which is the number of elements along that axis. We can use the <code>len()</code> method to find the length.</p> <pre><code># number of elements in array\nlen(num_arr)\n</code></pre> <pre>\n<code>4</code>\n</pre> <p>Size</p> <p>Size in NumPy refers to the total number of elements in an array across all dimensions. We can use the size of a numpy array using the <code>size</code> attribute</p> <pre><code># another way to get the number of elements\nnum_arr.size\n</code></pre> <pre>\n<code>4</code>\n</pre> <p>Data Type(<code>dtype</code>)</p> <p><code>dtype</code> in NumPy refers to the data type of the elements stored in an array, such as <code>int</code>, <code>float</code>, <code>bool</code>, etc.</p> <pre><code># finding data type of array elements\nprint(num_arr.dtype.name)\n</code></pre> <pre>\n<code>int64\n</code>\n</pre> <p>Converting Array Data Types</p> <p>We cas use <code>astype()</code> method to convert an array from one type to another.</p> <pre><code># converting an array\nfloat_arr = np.array([1.2, 3.5, 7.0])\n\n# use astype() to convert to a specific\nint_arr = float_arr.astype(int)\n\nprint(f'Array: {float_arr}, Data Type: {float_arr.dtype}')\nprint(f'Array: {int_arr}, Data Type: {int_arr.dtype}')\n</code></pre> <pre>\n<code>Array: [1.2 3.5 7. ], Data Type: float64\nArray: [1 3 7], Data Type: int64\n</code>\n</pre> <pre><code>np.info(np.ndarray.shape)\n</code></pre> <pre>\n<code>Tuple of array dimensions.\n\nThe shape property is usually used to get the current shape of an array,\nbut may also be used to reshape the array in-place by assigning a tuple of\narray dimensions to it.  As with `numpy.reshape`, one of the new shape\ndimensions can be -1, in which case its value is inferred from the size of\nthe array and the remaining dimensions. Reshaping an array in-place will\nfail if a copy is required.\n\n.. warning::\n\n    Setting ``arr.shape`` is discouraged and may be deprecated in the\n    future.  Using `ndarray.reshape` is the preferred approach.\n\nExamples\n--------\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; x = np.array([1, 2, 3, 4])\n&gt;&gt;&gt; x.shape\n(4,)\n&gt;&gt;&gt; y = np.zeros((2, 3, 4))\n&gt;&gt;&gt; y.shape\n(2, 3, 4)\n&gt;&gt;&gt; y.shape = (3, 8)\n&gt;&gt;&gt; y\narray([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n&gt;&gt;&gt; y.shape = (3, 6)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nValueError: cannot reshape array of size 24 into shape (3,6)\n&gt;&gt;&gt; np.zeros((4,2))[::2].shape = (-1,)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nAttributeError: Incompatible shape for in-place modification. Use\n`.reshape()` to make a copy with the desired shape.\n\nSee Also\n--------\nnumpy.shape : Equivalent getter function.\nnumpy.reshape : Function similar to setting ``shape``.\nndarray.reshape : Method similar to setting ``shape``.\n</code>\n</pre> <pre><code>?np.ndarray.shape\n</code></pre> <pre><code>arr = np.array([[1, 2, 3], [4, 5, 6]])\n\nprint('Dimensions:', arr.ndim)\nprint('Shape:', arr.shape)\nprint('Size:', arr.size)\nprint('Dtype:', arr.dtype)\n</code></pre> <pre>\n<code>Dimensions: 2\nShape: (2, 3)\nSize: 6\nDtype: int64\n</code>\n</pre> <pre><code>arr = np.array([1, 2, 3])\narr + 10\n</code></pre> <pre>\n<code>array([11, 12, 13])</code>\n</pre> <p>Explanation:</p> <ul> <li>Scalar is stretched to match array shape</li> <li>No extra memory used</li> </ul> <p>This explains why NumPy feels magical.</p> <pre><code># creating arrays\narray1 = np.array([1, 4, 6, 7])\narray2 = np.array([3, 5, 3, 1])\n</code></pre> <pre><code># subtract\ndifference1 = array2 - array1\nprint('difference1 =', difference1)\n\n# another way\ndifference2 = np.subtract(array2, array1)\nprint('difference2 =', difference2)\n</code></pre> <pre>\n<code>difference1 = [ 2  1 -3 -6]\ndifference2 = [ 2  1 -3 -6]\n</code>\n</pre> <p>As we may notice, numpy does element-wise operations for ordinary arithmetic operations</p> <pre><code># sum\nsummation1 = array1 + array2\nprint('summation1 =', summation1)\n\n# another way\nsummation2 = np.add(array1, array2)\nprint('summation2 =', summation2)\n</code></pre> <pre>\n<code>summation1 = [4 9 9 8]\nsummation2 = [4 9 9 8]\n</code>\n</pre> <pre><code># sin\nprint('sin(array1) =', np.sin(array1))\n# cos\nprint('cos(array1) =', np.cos(array1))\n# log\nprint('log(array1) =', np.log(array1))\n</code></pre> <pre>\n<code>sin(array1) = [ 0.84147098 -0.7568025  -0.2794155   0.6569866 ]\ncos(array1) = [ 0.54030231 -0.65364362  0.96017029  0.75390225]\nlog(array1) = [0.         1.38629436 1.79175947 1.94591015]\n</code>\n</pre> <pre><code># dot product\narray1.dot(array2)\n</code></pre> <pre>\n<code>np.int64(48)</code>\n</pre> <p>The <code>dot()</code> function: - Performs a dot product for 1D arrays - Performs matrix multiplication for 2D arrays</p> <p>Research:</p> <p>another way to dot matrices (arrays)</p> <pre><code>array1 == array2\n</code></pre> <pre>\n<code>array([False, False, False, False])</code>\n</pre> <pre><code>array1 &amp;gt; 3\n</code></pre> <pre>\n<code>array([False,  True,  True,  True])</code>\n</pre> <pre><code># array sum\narray_sum = array1.sum(axis=0)\nprint('Sum: ', array_sum)\n</code></pre> <pre>\n<code>Sum:  18\n</code>\n</pre> <pre><code># average value\nmean = array1.mean()\nprint('Mean: ', mean)\n</code></pre> <pre>\n<code>Mean:  4.5\n</code>\n</pre> <pre><code># minimum value\nminimum = array1.min()\nprint('Minimum: ', minimum)\n</code></pre> <pre>\n<code>Minimum:  1\n</code>\n</pre> <pre><code># maximum value\nmaximum = array1.max()\nprint('Maximum: ', maximum)\n</code></pre> <pre>\n<code>Maximum:  7\n</code>\n</pre> <pre><code># correlation coefficient\ncorrelation_coefficient = np.corrcoef(array1, array2)\nprint('Correlation Coefficient: ', correlation_coefficient)\n</code></pre> <pre>\n<code>Correlation Coefficient:  [[ 1.         -0.46291005]\n [-0.46291005  1.        ]]\n</code>\n</pre> <pre><code># standard deviation\nstandard_deviation = np.std(array1)\nprint('Standard Deviation: ', standard_deviation)\n</code></pre> <pre>\n<code>Standard Deviation:  2.29128784747792\n</code>\n</pre> <p>Research:</p> <p> copying arrays (you might meet <code>view()</code>, <code>copy()</code>) </p> <pre><code># Creating numpy arrays of different dimension\n# 1D array\narr1 = np.array([1, 4, 6, 7])\nprint('Array1 (1D): \\n', arr1)\n</code></pre> <pre>\n<code>Array1 (1D): \n [1 4 6 7]\n</code>\n</pre> <pre><code># 2D array\narr2 = np.array([[1.5, 2, 3], [4, 5, 6]])\nprint('Array2 (2D): \\n', arr2)\n</code></pre> <pre>\n<code>Array2 (2D): \n [[1.5 2.  3. ]\n [4.  5.  6. ]]\n</code>\n</pre> <pre><code>#3D array\narr3 = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n                 [[10, 11, 12], [13, 14, 15], [16, 17, 18]]])\nprint('Array3 (3D): \\n', arr3)\n</code></pre> <pre>\n<code>Array3 (3D): \n [[[ 1  2  3]\n  [ 4  5  6]\n  [ 7  8  9]]\n\n [[10 11 12]\n  [13 14 15]\n  [16 17 18]]]\n</code>\n</pre> <pre><code># find the dimensions of an array\nprint('Array1 (1D):', arr1.shape)\nprint('Array2 (2D):', arr2.shape)\nprint('Array3 (3D):', arr3.shape)\n</code></pre> <pre>\n<code>Array1 (1D): (4,)\nArray2 (2D): (2, 3)\nArray3 (3D): (2, 3, 3)\n</code>\n</pre> <pre><code># accessing items in a 1D array\narr1[2]\n</code></pre> <pre>\n<code>np.int64(6)</code>\n</pre> <pre><code># accessing items in 2D array\narr2[1, 2]\n</code></pre> <pre>\n<code>np.float64(6.0)</code>\n</pre> <pre><code># accessing in a 3D array\narr3[0, 1, 2]\n</code></pre> <pre>\n<code>np.int64(6)</code>\n</pre> <pre><code># slicing 1D array\narr1[0:3]\n</code></pre> <pre>\n<code>array([1, 4, 6])</code>\n</pre> <pre><code># slicing a 2D array\narr2[1, 1:]\n# row index = 1\n# column index from 1 to end\n</code></pre> <pre>\n<code>array([5., 6.])</code>\n</pre> <pre><code># slicing a 3D array\nfirst = arr3[0, 2]\nsecond = arr3[1, 0]\n\nnp.concatenate((first, second))\n</code></pre> <pre>\n<code>array([ 7,  8,  9, 10, 11, 12])</code>\n</pre> <pre><code># boolean indexing\narr1[arr1 &amp;lt; 5]\n</code></pre> <pre>\n<code>array([1, 4])</code>\n</pre> <p>Research:</p> <p>Fancy Indexing</p> <pre><code>print(arr2)\n</code></pre> <pre>\n<code>[[1.5 2.  3. ]\n [4.  5.  6. ]]\n</code>\n</pre> <pre><code># transpose\narr2_transpose1 = np.transpose(arr2)\nprint('Transpose1: \\n', arr2_transpose1)\n</code></pre> <pre>\n<code>Transpose1: \n [[1.5 4. ]\n [2.  5. ]\n [3.  6. ]]\n</code>\n</pre> <pre><code># another way\narr2_transpose2 = arr2.T\nprint('Transpose2: \\n', arr2_transpose2)\n</code></pre> <pre>\n<code>Transpose2: \n [[1.5 4. ]\n [2.  5. ]\n [3.  6. ]]\n</code>\n</pre> <pre><code># combining arrays\nfirst = arr3[0, 2]\nsecond = arr3[1, 0]\n\nnp.concatenate((first, second))\n</code></pre> <pre>\n<code>array([ 7,  8,  9, 10, 11, 12])</code>\n</pre> <pre><code>test_arr1 = np.array([[7, 8, 9], [10, 11, 12]])\ntest_arr2 = np.array([[1, 2, 3], [4, 5, 6]])\n\nnp.concatenate((test_arr1, test_arr2), axis=1)\n</code></pre> <pre>\n<code>array([[ 7,  8,  9,  1,  2,  3],\n       [10, 11, 12,  4,  5,  6]])</code>\n</pre> <ol> <li>Create an array of 10 numbers</li> <li>Remove the last element</li> <li>Reshape it into a 3x3 matrix</li> <li>Find the mean of each column</li> </ol> <p>Research:</p> <p>Adding/Removing Elements - <code>resize()</code> - <code>append()</code> - <code>insert()</code> - <code>delete()</code></p> <p>Changing array shape - <code>ravel()</code> - <code>reshape()</code></p> <pre><code>#stacking\n# np.vstack((a,b))\n# np.hstack((a,b))\n# np.column_stack((a,b))\n# np.c_[a, b]\n</code></pre> <pre><code># splitting arrays\n# np.hsplit()\n# np.vsplit()\n</code></pre>"},{"location":"Python/Libraries/12_numpy/#objective","title":"Objective","text":"<p>In this lesson, you will learn all you need to know to get moving with numpy. ie:</p> <ul> <li> What is Numpy</li> <li> Inspecting Numpy arrays</li> <li> Performing array mathematics</li> <li> Subsetting, Slicing and Indexing arrays</li> <li> Array manipulation</li> </ul>"},{"location":"Python/Libraries/12_numpy/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Libraries/12_numpy/#what-is-numpy","title":"What is Numpy","text":"<ul> <li>Numpy is a python package used for scientific computing</li> <li>Numpy provides arrays which are greater and faster alternatives to traditional python lists. An array is a group of elements of the same data type</li> <li>A standard numpy array is required to have elements of the same data type.</li> </ul>"},{"location":"Python/Libraries/12_numpy/#why-numpy","title":"Why NumPy?","text":"<p>NumPy is the foundation of most Python data libraries such as:</p> <ul> <li>Pandas</li> <li>SciPy</li> <li>Scikit-learn</li> <li>TensorFlow / PyTorch</li> </ul> <p>It is fast because:</p> <ul> <li>It uses C under the hood</li> <li>It avoids Python loops using vectorization</li> </ul>"},{"location":"Python/Libraries/12_numpy/#inspecting-our-arrays","title":"Inspecting our arrays","text":"<p>To use numpy, we'll first import it (you must have it installed for this to work)</p>"},{"location":"Python/Libraries/12_numpy/#data-types","title":"Data Types","text":"<p>The table below describes some of the most common data types we use in numpy</p> Data Type Description <code>int64</code> Signed 64-bit integer <code>float64</code> Double-precision floating point <code>complex128</code> Complex numbers <code>bool</code> Boolean values <code>object</code> Python objects <code>str_</code> Fixed-length strings"},{"location":"Python/Libraries/12_numpy/#ask-for-help","title":"Ask for help","text":""},{"location":"Python/Libraries/12_numpy/#quick-array-inspection-cheatsheet","title":"Quick Array Inspection Cheatsheet","text":"Attribute Meaning <code>ndim</code> Number of dimensions <code>shape</code> Size along each dimension <code>size</code> Total number of elements <code>dtype</code> Data type of elements"},{"location":"Python/Libraries/12_numpy/#broadcasting","title":"Broadcasting","text":"<p>Broadcasting allows NumPy to perform operations on arrays of different shapes.</p>"},{"location":"Python/Libraries/12_numpy/#array-mathematics","title":"Array mathematics","text":"<p>Numpy has out of the box tools to help us perform some import mathematical operations</p>"},{"location":"Python/Libraries/12_numpy/#arithmetic-operations","title":"Arithmetic Operations","text":"<p>Arithmetic operations in NumPy are element-wise operations like addition, subtraction, multiplication, and division that can be performed directly between arrays or between an array and a scalar.</p>"},{"location":"Python/Libraries/12_numpy/#trigonometric-operations","title":"Trigonometric operations","text":"<p>Trigonometric operations in NumPy are functions like <code>np.sin()</code>, <code>np.cos()</code>, and <code>np.tan()</code> that perform element-wise trigonometric calculations on arrays.</p>"},{"location":"Python/Libraries/12_numpy/#comparison","title":"Comparison","text":"<p>In NumPy, comparison operators perform element-wise comparisons on arrays and return boolean arrays of the same shape, where each element indicates True or False based on the corresponding element-wise comparison.</p>"},{"location":"Python/Libraries/12_numpy/#aggregate-functions","title":"Aggregate functions","text":"<p>NumPy provides several aggregate functions that perform operations across the elements of an array and return a single scalar value.</p>"},{"location":"Python/Libraries/12_numpy/#subsetting-slicing-and-indexing","title":"Subsetting, Slicing and Indexing","text":"<p> Indexing is the technique we use to access individual elements in an array. 0 represents the first element, 1 the represents second element and so on.</p> <p> Slicing is used to access elements of an array using a range of two indexes. The first index is the start of the range while the second index is the end of the range. The indexes are separated by a colon ie <code>[start:end]</code></p>"},{"location":"Python/Libraries/12_numpy/#indexing","title":"Indexing","text":""},{"location":"Python/Libraries/12_numpy/#slicing","title":"slicing","text":""},{"location":"Python/Libraries/12_numpy/#boolean-indexing","title":"Boolean Indexing","text":"<p>Boolean indexing in NumPy allows you to select elements from an array based on a boolean condition or a boolean array of the same shape. The elements corresponding to True values in the boolean array/condition are selected, while those corresponding to False are discarded. </p>"},{"location":"Python/Libraries/12_numpy/#array-manipulation","title":"Array manipulation","text":"<p>NumPy provides a wide range of functions that allow you to change the shape, dimensions, and structure of arrays to suit your needs</p>"},{"location":"Python/Libraries/12_numpy/#homework","title":"Homework","text":""},{"location":"Python/Libraries/12_numpy/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Libraries/13_pandas/","title":"Pandas Crash Course","text":"Meta Data  title: Pandas Crash Course author: Juma Shafara date: \"2024-01\" date-modified: \"2024-08-19\" description: Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool. keywords: [Creating Dataframes, Concatenating DataFrames, Sampling values in the DataFrame, Selecting, Boolean Indexing and Setting, Dropping, Retrieving information about DataFrame]   <p>Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool. </p> <p>This tutoral will show you the basic and intermediate concepts in Pandas</p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># Uncomment and run this cell to install pandas\n# !pip install pandas\n# !pip install openpyxl\n</code></pre> <pre><code># import pandas\nimport pandas as pd\n</code></pre> <pre><code># to check python version\nprint(pd.__version__)\n</code></pre> <pre>\n<code>2.3.3\n</code>\n</pre> <pre><code># loading an excel file into a dataframe\ndata = pd.read_excel(io='../../assets/demo.xlsx')\n</code></pre> <p>Warning</p> <p>You must have <code>openpyxl</code> installed to be able to read excel files using pandas. You can install it with <code>pip install openpyxl</code></p> <p>The data structure that is returned by the statement is called a <code>DataFrame</code></p> <pre><code># checking the datatype of the data object\nprint(type(data))\n</code></pre> <pre>\n<code>&lt;class 'pandas.core.frame.DataFrame'&gt;\n</code>\n</pre> <pre><code># randomly sample some values\ndata.sample(n=5)\n</code></pre> Age Gender Marital Status Address Income Income Category Job Category 137 32 m 0 1 26 2 2 92 61 m 1 18 23 1 3 39 21 f 0 0 13 1 1 41 56 f 0 7 213 4 3 48 51 f 0 0 47 2 1 <p>To begin with, we're gonna create a dataframe from a dictionary. The way we create the dictionary is important, keys will be used as column names, and the values will be used as rows. So, you typically want your values to be lists or tuples.</p> <p>Now you might observe that the values (lists) are of equal length</p> <pre><code># create a pandas dataframe using a dictionary\ndata_dictionary = {\n    'age': [65, 51, 45, 38, 40],\n    'gender': ['m', 'm', 'm', 'f', 'm'],\n    'income': [42, 148, 147, 43, 89]\n}\n\ndataframe_from_dict = pd.DataFrame(data=data_dictionary)\n</code></pre> <pre><code># display the dataframe\ndataframe_from_dict\n</code></pre> age gender income 0 65 m 42 1 51 m 148 2 45 m 147 3 38 f 43 4 40 m 89 <p>Next up, we are gonna create a dataframe from a list. For this case, the list be of 2D shape. Again, the way we organize data in our list is important. We should organize that in a format close to rows and columns as showed below. </p> <p>It turns out that when creating a dataframe from a list, we need to explicitly define the column names as demonstrated below</p> <pre><code># creating a dataframe from a 2D list\ndata_list = [\n    [28, 'm', 24],\n    [59, 'm', 841],\n    [54, 'm', 741],\n    [83, 'f', 34],\n    [34, 'm', 98]\n]\n# let's specify the column names\nnames = ['age', 'gender', 'income']\n\ndataframe_from_list = pd.DataFrame(data=data_list, \n                                   columns=names)\n</code></pre> <pre><code># display the dataframe\ndataframe_from_list\n</code></pre> age gender income 0 28 m 24 1 59 m 841 2 54 m 741 3 83 f 34 4 34 m 98 <p>Before we continue, I would like to share some ways you would look for help or more information about pandas methods.</p> <ul> <li> One way is by using the `help()` method.</li> <li> Another is by using the query operator `?`</li> </ul> <pre><code># Finding more information\n# help(pd.DataFrame)\n</code></pre> <pre><code>## Another way to find more information\n# ?pd.DataFrame\n</code></pre> <p>The latter is my favorite because it works in all situations.</p> <p>Sometimes there's a need to add two or more dataframes. To perform this, for the start, we can use the <code>pd.concat()</code>. The <code>concat()</code> method takes in a list of dataframes we would like to combine</p> <p>Remember we created 2 dataframes earlier, one from a dictionary and another from a list, now let's combine them to make one dataframe</p> <pre><code>concatenated_dataframe = pd.concat(\n    objs=[dataframe_from_dict, dataframe_from_list], \n    ignore_index=True\n)\n</code></pre> <pre><code>concatenated_dataframe\n</code></pre> age gender income 0 65 m 42 1 51 m 148 2 45 m 147 3 38 f 43 4 40 m 89 5 28 m 24 6 59 m 841 7 54 m 741 8 83 f 34 9 34 m 98 <p>We set <code>ignore_index=True</code> to correct the indexing so that we can have unique indexes and hence be able to able to uniquely identify rows by index</p> <p>Exercise: Demonstrate how to concatenate two or more dataframes by column ie if dataframe A has columns a, b, c and dataframe B has columns x, y, z, the resulting dataframe should have columns a, b, c, x, y, z</p> <p>We can use <code>head()</code> to look at the top part of the data. Out of the box, it returns the top 5 rows, however modifying the value for <code>n</code> can help us pick a specific number of rows from the top.</p> <pre><code># We can have look at the top part \nconcatenated_dataframe.head(n=3)\n</code></pre> age gender income 0 65 m 42 1 51 m 148 2 45 m 147 <p>We can use <code>tail()</code> to look at the bottom part of the data. Out of the box, it returns the bottom 5 rows, however modifying the value for <code>n</code> can help us pick a specific number of rows from the bottom.</p> <pre><code># We can look at the bottom part\nconcatenated_dataframe.tail(n=3)\n</code></pre> age gender income 7 54 m 741 8 83 f 34 9 34 m 98 <p>We can use <code>sample()</code> to look random rows data rows. Out of the box, it returns only 1 row, however modifying the value for <code>n</code> can help us pick a specific number of rows at random.</p> <pre><code># We can also randomly sample out some values in a DataFrame\nconcatenated_dataframe.sample(n=3)\n</code></pre> age gender income 0 65 m 42 2 45 m 147 8 83 f 34 <pre><code>country_data = pd.DataFrame(data={\n    'Country': ['Uganda', 'Kenya', 'Tanzania'],\n    'Capital': ['Kampala', 'Nairobi', 'Dodoma'],\n    'Population': [11190846, 1303171035, 207847528]\n    })\ncountry_data\n</code></pre> Country Capital Population 0 Uganda Kampala 11190846 1 Kenya Nairobi 1303171035 2 Tanzania Dodoma 207847528 <p>We can pick a specific value (or values) from a dataframe by indexing usin the <code>iloc</code> and <code>iat</code> methods. We insert the row number and the column number of an item that we want to pick from the dataframe in the square brackets. </p> <p>We can also use these techniques to replace values in a dataframe.</p> <pre><code># position 1\nprint(country_data.iloc[0, 0])\nprint(country_data.iloc[2, 1])\n</code></pre> <pre>\n<code>Uganda\nDodoma\n</code>\n</pre> <pre><code># position 2\nprint(country_data.iat[0, 0])\nprint(country_data.iat[2, 1])\n</code></pre> <pre>\n<code>Uganda\nDodoma\n</code>\n</pre> <p>Ponder:</p> <p> How can you use the <code>pd.DataFrame.iat</code> method to replace (or modify) a specific value in a dataframe</p> <p>We can access any value(s) by their row index and column name with the help of the <code>loc[]</code> and <code>at[]</code> methods.</p> <p>As you may observe the difference now is that we are using row index and column name instead of row index and column index for <code>iloc[]</code> and <code>iat[]</code></p> <pre><code># using label\nprint(country_data.loc[0, 'Capital'])\nprint(country_data.loc[1, 'Population'])\n</code></pre> <pre>\n<code>Kampala\n1303171035\n</code>\n</pre> <pre><code># using label\nprint(country_data.at[2, 'Population'])\nprint(country_data.at[1, 'Capital'])\n</code></pre> <pre>\n<code>207847528\nNairobi\n</code>\n</pre> <p>We can be able to pick out an entire column by either using the <code>.</code> operator or <code>[]</code>.</p> <ul> <li> We use the `.` operator when a column name is one single word</li> <li> We can use the `[]` when the column name is containing more that one word</li> <li> We can also use the `[]` when creating or assigning values to columns in a dataframe</li> </ul> <pre><code># picking out data from a specific column\ncountry_data.Country\n</code></pre> <pre>\n<code>0      Uganda\n1       Kenya\n2    Tanzania\nName: Country, dtype: object</code>\n</pre> <pre><code># another way to pick data from a specific column\ncountry_data['Country']\n</code></pre> <pre>\n<code>0      Uganda\n1       Kenya\n2    Tanzania\nName: Country, dtype: object</code>\n</pre> <p>The data structure that is returned by the statement is called a <code>Series</code></p> <pre><code>lakes = ['Albert', 'Turkana', 'Tanganyika']\ncountry_data['Lake'] = lakes\n\n# lets display the updated data\ncountry_data\n</code></pre> Country Capital Population Lake 0 Uganda Kampala 11190846 Albert 1 Kenya Nairobi 1303171035 Turkana 2 Tanzania Dodoma 207847528 Tanganyika <pre><code># lets check it\ntype(country_data['Capital'])\n</code></pre> <pre>\n<code>pandas.core.series.Series</code>\n</pre> <pre><code># Get specific row data (using indexing)\ncountry_data.iloc[0]\n</code></pre> <pre>\n<code>Country         Uganda\nCapital        Kampala\nPopulation    11190846\nLake            Albert\nName: 0, dtype: object</code>\n</pre> <p>We can get a range of rows by passing into <code>iloc[]</code> a range of indexes. The demonstration below returns rows of indexes 0 until but not including 2 ie 0 for Uganda and 1 for Kenya</p> <pre><code># Get specific rows (using subsetting)\ncountry_data.iloc[0:2]\n</code></pre> Country Capital Population Lake 0 Uganda Kampala 11190846 Albert 1 Kenya Nairobi 1303171035 Turkana <p>We can be able to pickout only rows whose values satisfy a specific condition, this trick is called boolean indexing. In the example below, we find all rows whose contry is Uganda</p> <pre><code># get all rows that have a column-value matching a specific value\n# eg where country is Belgium\ncountry_data[country_data['Country'] == 'Uganda']\n</code></pre> Country Capital Population Lake 0 Uganda Kampala 11190846 Albert <pre><code># Think about this\ncountry_data['Country'] == 'Tanzania'\n</code></pre> <pre>\n<code>0    False\n1    False\n2     True\nName: Country, dtype: bool</code>\n</pre> <p>You donot have to submit that </p> <pre><code>country_data\n</code></pre> Country Capital Population Lake 0 Uganda Kampala 11190846 Albert 1 Kenya Nairobi 1303171035 Turkana 2 Tanzania Dodoma 207847528 Tanganyika <p>We may realize that all our population values are terribly wrong and may choose the entire column. we can do this by using the <code>drop()</code> method. We specify the column name and <code>axis=1</code> to drop a column. </p> <pre><code># drop a column from a dataframe\ncountry_data.drop(\n    labels='Population', \n    axis=1\n)\n</code></pre> Country Capital Lake 0 Uganda Kampala Albert 1 Kenya Nairobi Turkana 2 Tanzania Dodoma Tanganyika <p>To drop many columns, we can pass all the columns to the <code>drop()</code> method as a list or tuple, and again specify the <code>axis=1</code>.</p> <pre><code>country_data.drop(\n    labels=['Lake', 'Population'], \n    axis=1\n)\n</code></pre> Country Capital 0 Uganda Kampala 1 Kenya Nairobi 2 Tanzania Dodoma <p>Another way to drop many columns is by passing them to the <code>drop()</code> method as a list value to the <code>columns</code> parameter. In this case we don't need to specify the <code>axis</code>.</p> <pre><code># You can drop many columns by passing in a columns list\ncountry_data.drop(columns=['Country', 'Population'])\n</code></pre> Capital Lake 0 Kampala Albert 1 Nairobi Turkana 2 Dodoma Tanganyika <p>To drop a row or many rows, we shall pass the index(es) as labels to the <code>drop</code> method method and optionally set <code>axis=0</code>. It turns out that the default value for <code>axis</code> is actually 0. Below, we have some two examples.</p> <pre><code># how to drop 1 row\ncountry_data.drop(labels=0) # drops out Uganda\n</code></pre> Country Capital Population Lake 1 Kenya Nairobi 1303171035 Turkana 2 Tanzania Dodoma 207847528 Tanganyika <pre><code># how to drop row data\ncountry_data.drop(labels=[0, 2], axis=0)\n</code></pre> Country Capital Population Lake 1 Kenya Nairobi 1303171035 Turkana <p>This drops rows in indexes 0 and 2, ie Uganda and Tanzania</p> <pre><code>country_data = pd.DataFrame({\n    'Country': ['Uganda', 'Kenya', 'Tanzania'],\n    'Capital': ['Kampala', None, None],\n    'Population': [11190846, 1303171035, 207847528]\n    })\n\ncountry_data\n</code></pre> Country Capital Population 0 Uganda Kampala 11190846 1 Kenya None 1303171035 2 Tanzania None 207847528 <p>We can use the <code>shape</code> attribute to obtain the number of rows and columns available in our dataset as illustrated.</p> <pre><code># shape of a dataframe (ie rows, columns)\ncountry_data.shape\n</code></pre> <pre>\n<code>(3, 3)</code>\n</pre> <p>If you're only interested in the number of rows you can use the <code>len()</code> method to find that eg</p> <pre><code># len of a dataframe (ie no of rows)\nlen(country_data)\n</code></pre> <pre>\n<code>3</code>\n</pre> <p>If you are interested in looking at the columns (column names), you can use the <code>columns</code> attribute to obtain the <code>Index</code> object of column names</p> <pre><code># Get all columns in a dataframe\ncountry_data.columns\n</code></pre> <pre>\n<code>Index(['Country', 'Capital', 'Population'], dtype='object')</code>\n</pre> <p>We can also use the <code>len()</code> method on this <code>Index</code> object to obtain the number of columns</p> <pre><code>len(country_data.columns)\n</code></pre> <pre>\n<code>3</code>\n</pre> <p>We can use the <code>info()</code> method to find some information on our dataframe ie:</p> <ul> <li> Columns (All columns in the dataframe)</li> <li> Non-Null Count (Number of non null values per column)</li> <li> Dtype (Data type each column)</li> <li> Total number of entries (rows)</li> </ul> <pre><code># get some basic info about the dataframe\ncountry_data.info()\n</code></pre> <pre>\n<code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   Country     3 non-null      object\n 1   Capital     1 non-null      object\n 2   Population  3 non-null      int64 \ndtypes: int64(1), object(2)\nmemory usage: 204.0+ bytes\n</code>\n</pre> <p>By using the <code>count()</code> method on the dataframe, we can obtain the number of non-null values per a column</p> <pre><code># Count non-null values in each column\ncountry_data.count()\n</code></pre> <pre>\n<code>Country       3\nCapital       1\nPopulation    3\ndtype: int64</code>\n</pre> <p>Finally, we can use the <code>describe()</code> to obtain some quick summary (descriptive) statistics about our data eg count, mean, standard deviation, minimum and maximum values, percentile</p> <pre><code># summary statistics\ncountry_data.describe()\n</code></pre> Population count 3.000000e+00 mean 5.074031e+08 std 6.961346e+08 min 1.119085e+07 25% 1.095192e+08 50% 2.078475e+08 75% 7.555093e+08 max 1.303171e+09"},{"location":"Python/Libraries/13_pandas/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Libraries/13_pandas/#creating-dataframes","title":"Creating Dataframes","text":"<p>The reason why data analysts like pandas is because pandas provides them with a very powerful data structure called a dataframe. A dataframe is a 2D structure that offers us rows and columns similar to tables in excel, sql etc</p>"},{"location":"Python/Libraries/13_pandas/#creating-dataframes-from-existing-files","title":"Creating dataframes from existing files","text":"<p>If you already have some data in maybe an excel, csv, or stata file, you can be able to load it into a dataframe and then perform manipulation.</p> <p>Info</p> <p>The dataset used in this lesson can be downloaded from our datasets site; DATAIDEA Datasets or using this drive link; Demo Household Dataset</p>"},{"location":"Python/Libraries/13_pandas/#creating-a-dataframe-from-a-dictionary","title":"Creating a DataFrame from a Dictionary","text":"<p>For the previous case, we may be having some data already, but sometimes we may want to create a dataframe from scratch. We can create pandas dataframes using two major ways:</p> <ul> <li> Using a dictionary</li> <li> Using a 2D list</li> </ul> <p>We've met dictionaries and lists in the Containers/Collections module of the Python 3 Beginner Course.</p>"},{"location":"Python/Libraries/13_pandas/#concatenating-dataframes","title":"Concatenating DataFrames","text":""},{"location":"Python/Libraries/13_pandas/#sampling-values-in-the-dataframe","title":"Sampling values in the DataFrame","text":"<p>In this section, we are gonna look at how to pick out some sections or parts of the data. We'll look at <code>head()</code>, <code>tail()</code> and <code>sample()</code>.</p> <p>To demonstrate these, we'll continue with our concatenated dataframe from the previous section</p>"},{"location":"Python/Libraries/13_pandas/#selection","title":"Selection","text":"<p>In this section we are gonna look at some tricks and techniques we can use to pick some really specific values from the dataframe</p>"},{"location":"Python/Libraries/13_pandas/#selecting-boolean-indexing-and-setting","title":"Selecting, Boolean Indexing and Setting","text":"<p>To demonstrate these, we're creating a little countries dataframe. As you may observe we use <code>pd.DataFrame()</code> to create our dataframe and notice we're passing in a dictionary for the value of data.</p>"},{"location":"Python/Libraries/13_pandas/#dropping","title":"Dropping","text":"<p>In this part we will learn some tricks and techniques to drop or remove some data from a dataframe.</p>"},{"location":"Python/Libraries/13_pandas/#research-on","title":"Research on:","text":"<p> sort and rank data</p>"},{"location":"Python/Libraries/13_pandas/#retrieving-information-about-dataframe","title":"Retrieving information about DataFrame","text":"<p>Pandas offers us some quick way with which we can find some quick information about our dataset (or dataframe)</p>"},{"location":"Python/Libraries/13_pandas/#basic-information","title":"Basic Information","text":""},{"location":"Python/Libraries/13_pandas/#summary","title":"Summary","text":""},{"location":"Python/Libraries/13_pandas/#research","title":"Research","text":"<p>Find out how to get for specific columns:</p> <ul> <li> mean</li> <li> median</li> <li> cummulative sum</li> <li> minimum</li> <li> maximum</li> </ul>"},{"location":"Python/Libraries/13_pandas/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Libraries/31_matplotlib_refined/","title":"31 matplotlib refined","text":"<p>title: Matplotlib Crash Course author: Juma Shafara date: \"2024-01\" date-modified: \"2024-09-18\" description: Matplotlib is a powerful plotting library in Python commonly used for data visualization. keywords: [python visualization, matplotlib, bar chart, histogram, scatter plot, line plot, box plot, pie chart, stacked bar chart, quiz, python quiz, dataidea, data science,]</p> <p></p> <p>Matplotlib is a powerful plotting library in Python commonly used for data visualization. </p> <p>When working with datasets, you can use Matplotlib to create various plots to explore and visualize the data. </p> <p>Here are some major plots you can create using Matplotlib with the Titanic dataset:</p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># # Uncomment and run this cell to install the libraries\n# !pip install pandas matplotlib dataidea\n</code></pre> <pre><code># import the libraries, packages and modules\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom dataidea.datasets import loadDataset\n</code></pre> <p>Let's demonstrate each of the plots using the Titanic dataset.  We'll first load the dataset and then create each plot using Matplotlib.</p> <pre><code># Load the Titanic dataset\ntitanic_df = loadDataset('titanic')\n</code></pre> <p>We can load this dataset like this because it is inbuilt in the dataidea package</p> <pre><code>titanic_df.head(n=5)\n</code></pre> pclass survived name sex age sibsp parch ticket fare cabin embarked boat body home.dest 0 1.0 1.0 Allen, Miss. Elisabeth Walton female 29.0000 0.0 0.0 24160 211.3375 B5 S 2 NaN St Louis, MO 1 1.0 1.0 Allison, Master. Hudson Trevor male 0.9167 1.0 2.0 113781 151.5500 C22 C26 S 11 NaN Montreal, PQ / Chesterville, ON 2 1.0 0.0 Allison, Miss. Helen Loraine female 2.0000 1.0 2.0 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON 3 1.0 0.0 Allison, Mr. Hudson Joshua Creighton male 30.0000 1.0 2.0 113781 151.5500 C22 C26 S NaN 135.0 Montreal, PQ / Chesterville, ON 4 1.0 0.0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25.0000 1.0 2.0 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON <ol> <li>Bar Plot: You can create a bar plot to visualize categorical data such as the number of passengers in each class (first class, second class, third class), the number of survivors vs. non-survivors, or the number of passengers embarked from each port (Cherbourg, Queenstown, Southampton).</li> </ol> <pre><code># 1. Bar Plot - Number of passengers in each class\nclass_counts = titanic_df.pclass.value_counts()\nclasses = class_counts.index\ncounts = class_counts.values\n\nplt.bar(x=classes, height=counts, color='#008374')\nplt.title('Number of Passengers Per Passenger Class')\nplt.xlabel('Passenger Class')\nplt.ylabel('Number of Passengers')\n\nplt.show()\n</code></pre> <p>It's easy to see from the graph that the 3rd class had the largest number of passengers, followed by the 1st class and 2nd class comes last</p> <ol> <li>Histogram: Histograms are useful for visualizing the distribution of continuous variables such as age or fare. You can create histograms to see the age distribution of passengers or the fare distribution.</li> </ol> <pre><code># 2. Histogram - Age distribution of passengers\nages = titanic_df.age\nplt.hist(x=ages, bins=20, color='#008374', \n         edgecolor='#66FDEE')\nplt.title('Age Distribution of Passengers')\nplt.ylabel('Frequency')\nplt.xlabel('Age')\nplt.show()\n</code></pre> <p>From the histogram we can observe that:</p> <ul> <li> The majority of the people we of ages between 15 and 35</li> <li> Fewer older people(above 60 years) boarded the titanic (below 20)t</li> </ul> <ol> <li>Box Plot: A box plot can be used to show the distribution of a continuous variable across different categories. For example, you can create a box plot to visualize the distribution of age or fare across different passenger classes.</li> </ol> <p>3.1. Age distribution boxplot</p> <pre><code># 3.1 Age distribution boxplot\nages = titanic_df.age.dropna()\nplt.boxplot(x=ages, vert=False,)\nplt.title('Age Distribution of Passengers')\nplt.xlabel('Age')\nplt.show()\n</code></pre> <p>Features of a box plot:</p> <p> Box: The box in a boxplot represents the interquartile range (IQR), which contains the middle 50% of the data. The top and bottom edges of the box are the third quartile (Q3) and the first quartile (Q1), respectively.</p> <p> Median Line: A line inside the box indicates the median (Q2) of the data, which is the middle value of the dataset.</p> <p> Whiskers: The whiskers extend from the edges of the box to the smallest and largest values within 1.5 times the IQR from Q1 and Q3. They represent the range of the bulk of the data.</p> <p> Outliers: Data points that fall outside the whiskers are considered outliers. They are typically plotted as individual points. Outliers can be indicative of variability or errors in the data.</p> <p> Minimum and Maximum: The ends of the whiskers show the minimum and maximum values within the range of 1.5 times the IQR from the first and third quartiles.</p> <p>Meaning: A boxplot provides a visual summary of several important aspects of a dataset:</p> <ul> <li>Central Tendency: The median line shows the central point of the data.</li> <li>Spread: The IQR (the length of the box) shows the spread of the middle 50% of the data.</li> <li>Symmetry and Skewness: The relative position of the median within the box and the length of the whiskers can indicate whether the data is symmetric or skewed.</li> <li>Outliers: Individual points outside the whiskers highlight potential outliers.</li> </ul> <p>Boxplots are particularly useful for comparing distributions between several groups or datasets and identifying outliers and potential anomalies.</p> <p>3.2 Age Distribution Across Passenger Classes</p> <pre><code># 3. Box Plot - Distribution of age across passenger classes\nplt.boxplot([titanic_df[titanic_df['pclass'] == 1]['age'].dropna(),\n             titanic_df[titanic_df['pclass'] == 2]['age'].dropna(),\n             titanic_df[titanic_df['pclass'] == 3]['age'].dropna()],\n            labels=['1st Class', '2nd Class', '3rd Class'])\nplt.xlabel('Passenger Class')\nplt.ylabel('Age')\nplt.title('Distribution of Age Across Passenger Classes')\nplt.show()\n</code></pre> <pre>\n<code>/tmp/ipykernel_16695/4289029800.py:2: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n  plt.boxplot([titanic_df[titanic_df['pclass'] == 1]['age'].dropna(),\n</code>\n</pre> <ol> <li>Scatter Plot: Scatter plots are helpful for visualizing the relationship between two continuous variables. You can create scatter plots to explore relationships such as age vs. fare. Read more about the scatter plot from the Matplotlib documentation</li> </ol> <pre><code># 4. Scatter Plot - Age vs. Fare\nplt.scatter(\n    x=titanic_df['age'], \n    y=titanic_df['fare'], \n    alpha=.5, \n    c=titanic_df['survived'], \n    cmap=ListedColormap(['#008374', '#000000'])\n)\nplt.xlabel('Age')\nplt.ylabel('Fare')\nplt.title('Age vs. Fare')\nplt.colorbar(label='Survived')  \nplt.show()\n</code></pre> <p>I don't about you but for me I don't see a linear relationship between the age and fare of the titanic passengers</p> <ol> <li>Pie Chart: Pie charts can be used to visualize the proportion of different categories within a dataset. For example, you can create a pie chart to show the proportion of male vs. female passengers or the proportion of survivors vs. non-survivors.</li> </ol> <pre><code># 5. Pie Chart - Proportion of male vs. female passengers\ngender_counts = titanic_df['sex'].value_counts()\nplt.pie(x=gender_counts, labels=gender_counts.index, \n        autopct='%1.1f%%', startangle=90, \n        colors=['#008374', '#66FDEE'])\nplt.title('Proportion of Male vs. Female Passengers')\nplt.legend(loc='lower right')\nplt.show()\n</code></pre> <ol> <li>Stacked Bar Plot: Stacked bar plots can be used to compare the composition of different categories across groups. For example, you can create a stacked bar plot to compare the proportion of survivors and non-survivors within each passenger class.</li> </ol> <pre><code># 6. Stacked Bar Plot - Survival status within each passenger class\nsurvival_counts = titanic_df.groupby(['pclass', 'survived']).size().unstack()\nsurvival_counts.plot(kind='bar', stacked=True,  \n                     color=['#008374', '#66FDEE'])\nplt.xlabel('Passenger Class')\nplt.ylabel('Number of Passengers')\nplt.title('Survival Status Within Each Passenger Class')\nplt.legend(['Did not survive', 'Survived'])\nplt.show()\n</code></pre> <pre><code>titanic_df.groupby(['pclass', 'survived']).size().unstack()\n</code></pre> survived 0.0 1.0 pclass 1.0 123 200 2.0 158 119 3.0 528 181 <p>We observe that:</p> <ul> <li> More passengers in class 1 survived than those that did not survive (200 vs 123)</li> <li> Most of the passengers in class 3 did not survive (528 vs 181)</li> <li> Slightly more passengers did not survive as compared to those that survived in class 2 (152 vs 119)</li> </ul> <ol> <li>Line Plot: Line plots can be useful for visualizing trends over time or continuous variables. While the Titanic dataset may not have explicit time data, you can still use line plots to visualize trends such as the change in survival rate with increasing age or fare.</li> </ol> <pre><code># 7. Line Plot - Mean age of passengers by passenger class\nmean_age_by_class = titanic_df.groupby('pclass')['age'].mean()\nplt.plot(mean_age_by_class.index, mean_age_by_class.values, \n         marker='*', color='#008374')\nplt.xlabel('Passenger Class')\nplt.ylabel('Mean Age')\nplt.title('Mean Age of Passengers by Passenger Class')\nplt.show()\n</code></pre> <p>We can quickly see the average ages for each passenger class, ie:</p> <ul> <li> Around 39 for first class</li> <li> Around 30 for second class</li> <li> Around 25 for third class</li> </ul> <p>These are some of the major plots you can create using Matplotlib. Each plot serves a different purpose and can help you gain insights into the data and explore relationships between variables.</p> <pre><code>air_passengers_data = loadDataset('air_passengers')\nair_passengers_data.head()\n</code></pre> Month Passengers 0 1949-01 112 1 1949-02 118 2 1949-03 132 3 1949-04 129 4 1949-05 121 <pre><code>air_passengers_data['Month'] = pd.to_datetime(air_passengers_data.Month)\nplt.plot('Month', 'Passengers', data=air_passengers_data, color='#008374')\nplt.xlabel('Years')\nplt.ylabel('Number of Passengers')\nplt.show()\n</code></pre> <p>We can observe that the number of passengers seems to increase with time</p>"},{"location":"Python/Libraries/31_matplotlib_refined/#what-is-matploblib","title":"What is Matploblib","text":""},{"location":"Python/Libraries/31_matplotlib_refined/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Libraries/31_matplotlib_refined/#review","title":"Review","text":"<p>Congratulations on reaching the end of this tutorial. In this tutorial, we have learned the basic graphs and how to interprete them. ie</p> <ul> <li> Bar chart</li> <li> Histogram</li> <li> Scatter plot</li> <li> Line plot</li> <li> Box plot</li> <li> Pie chart</li> <li> Stacked bar chart</li> </ul>"},{"location":"Python/Libraries/31_matplotlib_refined/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Libraries/32_data_exploration_and_cleaning_exercise/","title":"32 data exploration and cleaning exercise","text":"<p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <ol> <li>Load demo.xlsx dataset</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Rename the columns as suggested below</li> </ol> Old name New name Age age Gender gender Marital Status marital_status Address address Income income Income Category income_category Job Category job_category <pre><code># your solution\n</code></pre> <ol> <li>Display all the columns in the dataset</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Display some basic statistics about the numeric variables in the dataset</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Display some basic statistics about the categorical variables in the dataset</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>What are the unique observations under gender?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Can you fix any problems observed under the gender, give brief explanations why and how</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>How many observations have 'no answer' for marital status?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write some piece of code to return only numeric variables from the dataset</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Are there any missing values in the dataset?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Are there any outliers in the income variable?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Investigate the relationship between age and income</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>How many people earn more than 300 units?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>What data type is the marital status?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Create dummy variables for gender</li> </ol> <pre><code># your solution\n</code></pre> <p>END</p>"},{"location":"Python/Libraries/32_data_exploration_and_cleaning_exercise/#data-exploration-and-cleaning-exercise","title":"Data Exploration and Cleaning Exercise","text":""},{"location":"Python/Libraries/32_data_exploration_and_cleaning_exercise/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Libraries/32_data_exploration_and_cleaning_exercise/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Libraries/nobel_prize_laureates_exercise/","title":"Nobel prize laureates exercise","text":"<p>title: Nobel Prize Laureates Exercise author: Juma Shafara date: \"2024-01\" date-modified: \"2024-07-25\" description: Here are ten questions based on the Nobel Prize laureates dataset. Each question involves a mix of data exploration, cleaning, and analysis tasks.  keywords: []</p> <p></p> <p>Here are ten questions based on the Nobel Prize laureates dataset. Each question involves a mix of data exploration, cleaning, and analysis tasks. You can use Python and libraries such as pandas, matplotlib to solve these questions.</p> <p>You can download the Nobel Prize Laureates dataset from opendatasoft</p> <ol> <li>How many Nobel Prize laureates are included in the dataset?</li> </ol> <pre><code># your solution here\n</code></pre> <ol> <li>Which country has the highest number of Nobel laureates?</li> </ol> <pre><code># your solution here\n</code></pre> <p></p> <ol> <li>What is the distribution of Nobel laureates across different prize categories?</li> </ol> <pre><code># your solution here\n</code></pre> <ol> <li>How many Nobel laureates were awarded in each decade?</li> </ol> <pre><code># your solution here\n</code></pre> <ol> <li>Are there any missing values in the dataset? If so, in which columns?</li> </ol> <pre><code># your solution here\n</code></pre> <ol> <li>Perform data cleaning by handling missing values appropriately. Describe your approach.</li> </ol> <pre><code># your solution here\n</code></pre> <p></p> <ol> <li>Visualize the distribution of Nobel laureates' birth countries on a world map.</li> </ol> <pre><code># your solution here\n</code></pre> <ol> <li>Is there any correlation between a laureate's birth year and the year they were awarded the Nobel Prize? Visualize if there's any relationship.</li> </ol> <pre><code># your solution here\n</code></pre> <ol> <li>Perform anomaly detection on the birth years of laureates. Identify and explain any outliers.</li> </ol> <pre><code># your solution here\n</code></pre> <p> Don't miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it's easy and safe. </p> <ol> <li>Based on the dataset, can you identify any interesting trends or patterns regarding Nobel laureates' demographics or the fields in which they were awarded?</li> </ol> <pre><code># your solution here\n</code></pre> <p>These questions cover various aspects of data exploration, cleaning, visualization, and analysis using the Nobel Prize laureates dataset. You can explore and analyze the dataset to find answers to these questions and gain insights into the demographics and distribution of Nobel laureates over time and across different categories. </p> <p></p>"},{"location":"Python/Libraries/visualization_quiz/","title":"Visualization quiz","text":"<p>title: Data Visualization Quiz author: Juma Shafara date: \"2024-01\" date-modified: \"2024-07-25\" keywords: [python visualization, matplotlib, bar chart, histogram, scatter plot, line plot, box plot, pie chart, stacked bar chart, quiz, python quiz, dataidea, data science,]</p> <p></p> <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Libraries/visualization_quiz/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Python/Libraries/visualization_quiz/#question-1-which-type-of-chart-is-used-to-show-the-relationship-between-two-continuous-variables","title":"Question 1: Which type of chart is used to show the relationship between two continuous variables?","text":"<p>a) Bar chart b) Pie chart c) Scatter plot d) Area chart  </p>"},{"location":"Python/Libraries/visualization_quiz/#question-2-what-is-the-primary-purpose-of-a-radar-chart","title":"Question 2: What is the primary purpose of a radar chart?","text":"<p>a) Comparing multiple categories across a few variables b) Showing hierarchical data c) Displaying trends over time d) Visualizing correlations between two variables  </p>"},{"location":"Python/Libraries/visualization_quiz/#question-3-in-a-stacked-bar-chart-each-bar-is-divided-into-segments-to-represent","title":"Question 3: In a stacked bar chart, each bar is divided into segments to represent:","text":"<p>a) Different categories of a single variable b) Time periods c) Trends over time d) Correlation between variables  </p>"},{"location":"Python/Libraries/visualization_quiz/#question-4-when-should-a-heat-map-be-used","title":"Question 4: When should a heat map be used?","text":"<p>a) Showing proportions within a population b) Comparing individual data points c) Visualizing spatial relationships and correlations d) Displaying data distribution  </p>"},{"location":"Python/Libraries/visualization_quiz/#question-5-which-type-of-chart-is-best-suited-for-comparing-parts-of-a-whole","title":"Question 5: Which type of chart is best suited for comparing parts of a whole?","text":"<p>a) Scatter plot b) Histogram c) Pie chart d) Line chart  </p>"},{"location":"Python/Libraries/visualization_quiz/#question-6-which-chart-type-is-used-to-detect-outliers-and-visualize-the-distribution-of-a-dataset","title":"Question 6: Which chart type is used to detect outliers and visualize the distribution of a dataset?","text":"<p>a) Box plot b) Area chart c) Bubble chart d) Radar chart  </p>"},{"location":"Python/Libraries/visualization_quiz/#question-7-what-does-a-bubble-chart-represent","title":"Question 7: What does a bubble chart represent?","text":"<p>a) Relationship between two continuous variables b) Relationship between two categorical variables c) Relationship between three variables using two continuous variables and one categorical variable d) Relationship between three continuous variables  </p>"},{"location":"Python/Libraries/visualization_quiz/#question-8-which-chart-type-is-suitable-for-showing-hierarchical-data-with-nested-categories","title":"Question 8: Which chart type is suitable for showing hierarchical data with nested categories?","text":"<p>a) Tree map b) Scatter plot c) Line chart d) Histogram  </p>"},{"location":"Python/Libraries/visualization_quiz/#question-9-when-is-a-bar-chart-more-appropriate-than-a-pie-chart","title":"Question 9: When is a bar chart more appropriate than a pie chart?","text":"<p>a) Comparing parts of a whole b) Showing trends over time c) Displaying a single continuous variable d) Visualizing correlations  </p>"},{"location":"Python/Libraries/visualization_quiz/#question-10-what-is-the-main-purpose-of-a-histogram","title":"Question 10: What is the main purpose of a histogram?","text":"<p>a) Showing hierarchical data b) Visualizing correlations c) Displaying the distribution of a single continuous variable d) Comparing multiple categories across variables  </p>"},{"location":"Python/Libraries/visualization_quiz/#answers","title":"Answers:","text":"<ol> <li>c) Scatter plot</li> <li>a) Comparing multiple categories across a few variables</li> <li>a) Different categories of a single variable</li> <li>c) Visualizing spatial relationships and correlations</li> <li>c) Pie chart</li> <li>a) Box plot</li> <li>c) Relationship between three variables using two continuous variables and one categorical variable</li> <li>a) Tree map</li> <li>a) Comparing parts of a whole</li> <li>c) Displaying the distribution of a single continuous variable</li> </ol>"},{"location":"Python/Libraries/visualization_quiz/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Python/Python%20Builtin/filter_function/","title":"The <code>filter()</code> function in Python","text":"<p>The filter() function is used to filter elements from an iterable based on a condition.</p>"},{"location":"Python/Python%20Builtin/filter_function/#syntax","title":"Syntax","text":"<pre><code>filter(function, iterable)\n</code></pre> <p><code>function</code>: A function that returns True or False.</p> <p><code>iterable</code>: The list (or tuple, etc.) to filter.</p> <p>It returns a filter object, which you can convert to a list, set, or tuple.</p>"},{"location":"Python/Python%20Builtin/filter_function/#example","title":"Example","text":"<p>Let\u2019s say we want to filter out only even numbers from a list.</p>"},{"location":"Python/Python%20Builtin/filter_function/#without-filter","title":"Without <code>filter()</code>:","text":"<pre><code>numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\neven_numbers = []\n\nfor num in numbers:\n  if num % 2 == 0:\n    even_numbers.append(num)\n\nprint(even_numbers)\n</code></pre> <pre><code>[2, 4, 6, 8]\n</code></pre>"},{"location":"Python/Python%20Builtin/filter_function/#with-filter","title":"With <code>filter()</code>:","text":"<pre><code>even_numbers = filter(lambda num: num % 2 == 0, numbers)\nprint(list(even_numbers))\n</code></pre> <pre><code>[2, 4, 6, 8]\n</code></pre>"},{"location":"Python/Python%20Builtin/filter_function/#using-named-functions","title":"Using named functions:","text":"<p>We can also use named functions instead of lambda functions, as of the example below</p> <pre><code>def is_even(num):\n  return num % 2 == 0\n\neven_numbers = filter(is_even, numbers)\n\nprint(list(even_numbers))\n</code></pre> <pre><code>[2, 4, 6, 8]\n</code></pre>"},{"location":"Python/Python%20Builtin/filter_function/#using-filter-with-strings","title":"Using <code>filter()</code> with strings:","text":"<p>In the example below, we filter out the \"short\" words from the list of words</p> <pre><code>words = ['data', 'ai', 'machine', 'learning']\n\nlong_words = filter(lambda word: len(word) &gt; 4, words)\n\nprint(list(long_words))\n</code></pre> <pre><code>['machine', 'learning']\n</code></pre>"},{"location":"Python/Python%20Builtin/filter_function/#using-filter-to-clean-data","title":"Using <code>filter()</code> to clean data:","text":"<p>In the example below, we remove all the None values from the list</p> <pre><code>data = [10, None, 25, None, 40, 0]\n\ncleaned = filter(lambda value: value is not None, data)\nprint(list(cleaned))\n</code></pre> <pre><code>[10, 25, 40, 0]\n</code></pre>"},{"location":"Python/Python%20Builtin/map_function/","title":"The <code>map()</code> function:","text":"<p>The <code>map()</code> function in Python is used to apply a function to every item in an iterable\u2014like a list or a tuple\u2014and returns a new map object with the results.</p>"},{"location":"Python/Python%20Builtin/map_function/#syntax","title":"Syntax:","text":"<p><code>map(function, iterable)</code></p> <ul> <li><code>function</code>: This is the function you want to apply to each item.</li> <li><code>iterable</code>: This is the list, tuple, or any other iterable whose items you want to process.</li> </ul>"},{"location":"Python/Python%20Builtin/map_function/#basic-example","title":"Basic Example:","text":"<p>Imagine we want to square every number in a list.</p>"},{"location":"Python/Python%20Builtin/map_function/#without-map","title":"Without <code>map()</code>:","text":"<pre><code>numbers = [1, 2, 3]\nsquared = []\n\nfor num in numbers:\n  square = num ** 2\n  squared.append(square)\n\nprint(squared)\n</code></pre> <pre><code>[1, 4, 9]\n</code></pre>"},{"location":"Python/Python%20Builtin/map_function/#without-map_1","title":"Without <code>map()</code>:","text":"<pre><code>squared = map(lambda num: num ** 2, numbers)\nprint(list(squared))\n</code></pre> <pre><code>[1, 4, 9]\n</code></pre> <p>The <code>map</code></p> <pre><code>a = [1, 2, 3]\nb = [4, 5, 6]\n\nsumms = []\n\nfor count in range(len(a)):\n  summs.append(a[count] + b[count])\n\nprint(summs)\n</code></pre> <pre><code>[5, 7, 9]\n</code></pre> <pre><code>summs = map(lambda x, y: x + y, a, b)\nprint(list(summs))\n</code></pre> <pre><code>[5, 7, 9]\n</code></pre> <pre><code>def add(x, y):\n  return x + y\n\nsumms = map(add, a, b)\nprint(list(summs))\n</code></pre> <pre><code>[5, 7, 9]\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Python/Python%20Builtin/reduce_function/","title":"Python <code>reduce()</code> Function Explained","text":""},{"location":"Python/Python%20Builtin/reduce_function/#syntax","title":"Syntax","text":"<pre><code>from functools import reduce\n\nreduce(function, iterable, initializer)\n</code></pre> <pre><code>\n</code></pre> <pre><code>numbers = [1, 2, 3, 4]\ntotal = 0\n\nfor num in numbers:\n  total += num\n\nprint(total)\n</code></pre> <pre><code>10\n</code></pre> <pre><code>from functools import reduce\n\nnumbers = [1, 2, 3, 4]\nresult = reduce(lambda x, y: x + y, numbers)\nprint(result)\n</code></pre> <pre><code>10\n</code></pre> <pre><code>numbers = [1, 2, 3, 4]\n\ndef add(a, b):\n  return a * b\n\nresult = reduce(add, numbers)\nprint(result)\n</code></pre> <pre><code>24\n</code></pre> <pre><code>numbers = [1, 2, 3, 4]\nresult = reduce(lambda x, y: x + y, numbers, 10)\nprint(result)\n</code></pre> <pre><code>20\n</code></pre> <pre><code>numbers = [4, 8, 1, 6, 10, 3]\n\nresult = reduce(lambda x, y: x if x &gt; y else y, numbers)\nprint(result)\n</code></pre> <pre><code>10\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Time%20Series/90_introduction/","title":"Introduction","text":"<p>title: What is Time Series  author: Juma Shafara date: \"2024-05\" description: Objective of time series analysis is to understand how change in time affect the dependent variables and accordingly predict values for future time intervals. keywords: [time series analysis, time series characteristics, trend, seasonality, cyclicity, irregularities]</p> <p></p> <p>Any data recorded with some fixed interval of time is called as time series data. This fixed interval can be hourly, daily, monthly or yearly. e.g. hourly temperature reading, daily changing fuel prices, monthly electricity bill, annul company profit report etc. In time series data, time will always be independent variable and there can be one or many dependent variable. </p> <p>Sales forecasting time series with shampoo sales for every month will look like this, </p> <p></p> <p>In above example since there is only one variable dependent on time so its called as univariate time series. If there are multiple dependent variables, then its called as multivariate time series.</p> <p>Objective of time series analysis is to understand how change in time affect the dependent variables and accordingly predict values for future time intervals.</p>"},{"location":"Time%20Series/90_introduction/#time-series-characteristics","title":"Time Series Characteristics","text":"<p>Mean, standard deviation and seasonality defines different characteristics of the time series. </p> <p></p> <p>Important characteristics of the time series are as below</p>"},{"location":"Time%20Series/90_introduction/#trend","title":"Trend","text":"<p>Trend represent the change in dependent variables with respect to time from start to end. In case of increasing trend dependent variable will increase with time and vice versa. It's not necessary to have definite trend in time series, we can have a single time series with increasing and decreasing trend. In short trend represent the varying mean of time series data.</p> <p></p>"},{"location":"Time%20Series/90_introduction/#seasonality","title":"Seasonality","text":"<p>If observations repeats after fixed time interval then they are referred as seasonal observations. These seasonal changes in data can occur because of natural events or man-made events. For example every year warm cloths sales increases just before winter season. So seasonality represent the data variations at fixed intervals.</p> <p></p>"},{"location":"Time%20Series/90_introduction/#irregularities","title":"Irregularities","text":"<p>This is also called as noise. Strange dips and jump in the data are called as irregularities. These fluctuations are caused by uncontrollable events like earthquakes, wars, flood, pandemic etc. For example because of COVID-19 pandemic there is huge demand for hand sanitizers and masks.</p> <p></p>"},{"location":"Time%20Series/90_introduction/#cyclicity","title":"Cyclicity","text":"<p>Cyclicity occurs when observations in the series repeats in random pattern. Note that if there is any fixed pattern then it becomes seasonality, in case of cyclicity observations may repeat after a week, months or may be after a year. These kinds of patterns are much harder to predict.</p> <p></p> <p>Time series data which has above characteristics is called as 'Non-Stationary Data'. For any analysis on time series data we must convert it to 'Stationary Data'</p> <p>The general guideline is to estimate the trend and seasonality in the time series, and then make the time series stationary for data modeling. In data modeling step statistical techniques are used for time series analysis and forecasting. Once we have the predictions, in the final step forecasted values converted into the original scale by applying trend and seasonality constraints back.</p>"},{"location":"Time%20Series/90_introduction/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Time%20Series/91_analysis/","title":"Analysis","text":"<p>title: Time Series Analysis author: Juma Shafara date: \"2024-05\" description: Objective of time series analysis is to understand how change in time affect the dependent variables and accordingly predict values for future time intervals. keywords: [time series analysis, time series characteristics, trend, seasonality, cyclicity, irregularities]</p> <p></p> <p>As name suggest its analysis of the time series data to identify the patterns in it. I will briefly explain the different techniques and test for time series data analysis.</p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Time%20Series/91_analysis/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Time%20Series/91_analysis/#air-passengers-data-set","title":"Air Passengers Data Set","text":"<p>We have a monthly time series data of the air passengers from 1 Jan 1949 to 1 Dec 1960. Each row contains the air passenger number for a month of that particular year. Objective is to build a model to forecast the air passenger traffic for future months.</p>"},{"location":"Time%20Series/91_analysis/#decomposition-of-time-series","title":"Decomposition of Time Series","text":"<p>Time series decomposition helps to deconstruct the time series into several component like trend and seasonality for better visualization of its characteristics. Using time-series decomposition makes it easier to quickly identify a changing mean or variation in the data</p> <p></p>"},{"location":"Time%20Series/91_analysis/#stationary-data","title":"Stationary Data","text":"<p>For accurate analysis and forecasting trend and seasonality is removed from the time series and converted it into stationary series. Time series data is said to be stationary when statistical properties like mean, standard deviation are constant and there is no seasonality. In other words statistical properties of the time series data should not be a function of time.</p> <p></p>"},{"location":"Time%20Series/91_analysis/#test-for-stationarity","title":"Test for Stationarity","text":"<p>Easy way is to look at the plot and look for any obvious trend or seasonality. While working on real world data we can also use more sophisticated methods like rolling statistic and Augmented Dickey Fuller test to check stationarity of the data.</p>"},{"location":"Time%20Series/91_analysis/#rolling-statistics","title":"Rolling Statistics","text":"<p>In rolling statistics technique we define a size of window to calculate the mean and standard deviation throughout the series. For stationary series mean and standard deviation shouldn't change with time.</p>"},{"location":"Time%20Series/91_analysis/#augmented-dickey-fuller-adf-test","title":"Augmented Dickey Fuller (ADF) Test","text":"<p>I won't go into the details of how this test works. I will concentrate more on how to interpret the result of this test to determine the stationarity of the series. ADF test will return 'p-value' and 'Test Statistics' output values. * p-value &gt; 0.05: non-stationary. * p-value &lt;= 0.05: stationary. * Test statistics: More negative this value more likely we have stationary series. Also, this value should be smaller than critical values(1%, 5%, 10%). For e.g. If test statistic is smaller than the 5% critical values, then we can say with 95% confidence that this is a stationary series</p>"},{"location":"Time%20Series/91_analysis/#convert-non-stationary-data-to-stationary-data","title":"Convert Non-Stationary Data to Stationary Data","text":"<p>Accounting for the time series data characteristics like trend and seasonality is called as making data stationary. So by making the mean and variance of the time series constant, we will get the stationary data. Below are the few technique used for the same\u2026</p>"},{"location":"Time%20Series/91_analysis/#differencing","title":"Differencing","text":"<p>Differencing technique helps to remove the trend and seasonality from time series data. Differencing is performed by subtracting the previous observation from the current observation. The differenced data will contain one less data point than original data. So differencing actually reduces the number of observations and stabilize the mean of a time series.</p> \\[d = t - t0\\] <p>After performing the differencing it's recommended to plot the data and  visualize the change. In case there is not sufficient improvement you can perform second order or even third order differencing.</p>"},{"location":"Time%20Series/91_analysis/#transformation","title":"Transformation","text":"<p>A simple but often effective way to stabilize the variance across time is to apply a power transformation to the time series. Log, square root, cube root are most commonly used transformation techniques. Most of the time you can pick the type of growth of the time series and accordingly choose the transformation method. For. e.g. A time series that has a quadratic growth trend can be made linear by taking the square root. In case differencing don't work, you may first want to use one of above transformation technique to remove the variation from the series. </p> <p></p>"},{"location":"Time%20Series/91_analysis/#moving-average","title":"Moving Average","text":"<p>In moving averages technique, a new series is created by taking the averages of data points from original series. In this technique we can use two or more raw data points to calculate the average. This is also called as 'window width (w)'. Once window width is decided, averages are calculated from start to the end for each set of w consecutive values, hence the name moving averages. It can also be used for time series forecasting.</p> <p></p>"},{"location":"Time%20Series/91_analysis/#weighted-moving-averageswma","title":"Weighted Moving Averages(WMA)","text":"<p>WMA is a technical indicator that assigns a greater weighting to the most recent data points, and less weighting to data points in the distant past. The WMA is obtained by multiplying each number in the data set by a predetermined weight and summing up the resulting values. There can be many techniques for assigning weights. A popular one is exponentially weighted moving average where weights are assigned to all the previous values with a decay factor.</p>"},{"location":"Time%20Series/91_analysis/#centered-moving-averagescms","title":"Centered Moving Averages(CMS)","text":"<p>In a centered moving average, the value of the moving average at time t is computed by centering the window around time t and averaging across the w values within the window. For example, a center moving average with a window of 3 would be calculated as</p> <p>\\(\\(CMA(t) = mean(t-1, t, t+1)\\)\\)</p> <p>CMA is very useful for visualizing the time series data</p>"},{"location":"Time%20Series/91_analysis/#trailing-moving-averagestma","title":"Trailing Moving Averages(TMA)","text":"<p>In trailing moving average, instead of averaging over a window that is centered around a time period of interest, it simply takes the average of the last w values. For example, a trailing moving average with a window of 3 would be calculated as:</p> \\[TMA(t) = mean(t-2, t-1, t)\\] <p>TMA are useful for forecasting.</p>"},{"location":"Time%20Series/91_analysis/#correlation","title":"Correlation","text":"<ul> <li>Most important point about values in time series is its dependence on the previous values.</li> <li>We can calculate the correlation for time series observations with previous time steps, called as lags.</li> <li>Because the correlation of the time series observations is calculated with values of the same series at previous times, this is called an autocorrelation or serial correlation.</li> <li>To understand it better lets consider the example of fish prices. We will use below notation to represent the fish prices. <ul> <li>\\(P(t)\\)= Fish price of today</li> <li>\\(P(t-1)\\) = Fish price of last month</li> <li>\\(P(t-2)\\) =Fish price of last to last month</li> </ul> </li> <li>Time series of fish prices can be represented as \\(P(t-n),..... P(t-3), P(t-2),P(t-1), P(t)\\)</li> <li>So if we have fish prices for last few months then it will be easy for us to predict the fish price for today (Here we are ignoring all other external factors that may affect the fish prices)</li> </ul> <p>All the past and future data points are related in time series and ACF and PACF functions help us to determine correlation in it.</p>"},{"location":"Time%20Series/91_analysis/#auto-correlation-function-acf","title":"Auto Correlation Function (ACF)","text":"<ul> <li>ACF tells you how correlated points are with each other, based on how many time steps they are separated by.</li> <li>Now to understand it better lets consider above example of fish prices. Let's try to find the correlation between fish price for current month P(t) and two months ago P(t-2). Important thing to note that, fish price of two months ago can directly affect the today's fish price or it can indirectly affect the fish price through last months price P(t-1)</li> <li>So ACF consider the direct as well indirect effect between the points while determining the correlation</li> </ul>"},{"location":"Time%20Series/91_analysis/#partial-auto-correlation-function-pacf","title":"Partial Auto Correlation Function (PACF)","text":"<ul> <li>Unlike ACF, PACF only consider the direct effect between the points while determining the correlation</li> <li>In case of above fish price example PACF will determine the correlation between fish price for current month P(t) and two months ago P(t-2) by considering only P(t) and P(t-2) and ignoring P(t-1)</li> </ul>"},{"location":"Time%20Series/91_analysis/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Time%20Series/92_forecasting/","title":"Forecasting","text":"<p>title: Time Series Forecasting author: Juma Shafara date: \"2024-05\" description: Objective of time series analysis is to understand how change in time affect the dependent variables and accordingly predict values for future time intervals. keywords: [time series analysis, time series characteristics, trend, seasonality, cyclicity, irregularities]</p> <p></p> <p>Forecasting refers to the future predictions based on the time series data analysis. Below are the steps performed during time series forecasting</p> <ul> <li>Step 1: Understand the time series characteristics like trend, seasonality etc</li> <li>Step 2: Do the analysis and identify the best method to make the time series stationary</li> <li>Step 3: Note down the transformation steps performed to make the time series stationary and make sure that the reverse transformation of data is possible to get the original scale back</li> <li>Step 4: Based on data analysis choose the appropriate model for time series forecasting</li> <li>Step 5: We can assess the performance of a model by applying simple metrics such as residual sum of squares(RSS). Make sure to use whole data for prediction.</li> <li>Step 6: Now we will have an array of predictions which are in transformed scale. We just need to apply the reverse transformation to get the prediction values in original scale.</li> <li>Step 7: At the end we can do the future forecasting and get the future forecasted values in original scale.</li> </ul>"},{"location":"Time%20Series/92_forecasting/#models-used-for-time-series-forecasting","title":"Models Used For Time Series Forecasting","text":"<ul> <li>Autoregression (AR)</li> <li>Moving Average (MA)</li> <li>Autoregressive Moving Average (ARMA)</li> <li>Autoregressive Integrated Moving Average (ARIMA)</li> <li>Seasonal Autoregressive Integrated Moving-Average (SARIMA)</li> <li>Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)</li> <li>Vector Autoregression (VAR)</li> <li>Vector Autoregression Moving-Average (VARMA)</li> <li>Vector Autoregression Moving-Average with Exogenous Regressors (VARMAX)</li> <li>Simple Exponential Smoothing (SES)</li> <li>Holt Winter\u2019s Exponential Smoothing (HWES)</li> </ul> <p>Next part of this article we are going to analyze and forecast air passengers time series data using ARIMA model. Brief introduction of ARIMA model is as below</p>"},{"location":"Time%20Series/92_forecasting/#arima","title":"ARIMA","text":"<ul> <li>ARIMA stands for Auto-Regressive Integrated Moving Averages. It is actually a combination of AR and MA model. </li> <li>ARIMA has three parameters 'p' for the order of Auto-Regressive (AR) part, 'q' for the order of Moving Average (MA) part and 'd' for the order of integrated part. </li> </ul>"},{"location":"Time%20Series/92_forecasting/#auto-regressive-ar-model","title":"Auto-Regressive (AR) Model:","text":"<ul> <li>As the name indicates, its the regression of the variables against itself. In this model linear combination of the past values are used to forecast the future values. </li> <li>To figure out the order of AR model we will use PACF function</li> </ul>"},{"location":"Time%20Series/92_forecasting/#integrationi","title":"Integration(I):","text":"<ul> <li>Uses differencing of observations (subtracting an observation from observation at the previous time step) in order to make the time series stationary. Differencing involves the subtraction of the current values of a series with its previous values \\(d\\) number of times.</li> <li>Most of the time value of \\(d = 1\\), means first order of difference.</li> </ul>"},{"location":"Time%20Series/92_forecasting/#moving-average-ma-model","title":"Moving Average (MA) Model:","text":"<ul> <li>Rather than using past values of the forecast variable in a regression, a moving average model uses linear combination of past forecast errors</li> <li>To figure out the order of MA model we will use ACF function</li> </ul>"},{"location":"Time%20Series/92_forecasting/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""},{"location":"Time%20Series/93_python_example/","title":"Python Example","text":"<p>title: TSA Python Example author: Juma Shafara date: \"2024-05\" description: Objective of time series analysis is to understand how change in time affect the dependent variables and accordingly predict values for future time intervals. keywords: [time series analysis, time series characteristics, trend, seasonality, cyclicity, irregularities]</p> <p></p> <p>We have a monthly time series data of the air passengers from 1 Jan 1949 to 1 Dec 1960. Each row contains the air passenger number for a month of that particular year. Objective is to build a model to forecast the air passenger traffic for future months.</p> <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code>from dataidea.packages import np, pd, plt, sns, sm\nfrom dataidea.datasets import loadDataset # allows us to load datasets\nfrom statsmodels.tsa.stattools import adfuller,acf, pacf\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom pylab import rcParams\n</code></pre> <pre><code># Set plot size \nrcParams['figure.figsize'] = 10, 6\n</code></pre> <pre><code># load the dataset\ndata = loadDataset('air_passengers')\n\n# Print the first 5 rows\ndata.head()\n</code></pre> Month Passengers 0 1949-01 112 1 1949-02 118 2 1949-03 132 3 1949-04 129 4 1949-05 121 <pre><code># convert month to datetime\ndata['Month'] = pd.to_datetime(data.Month)\n\n# set month as index\ndata = data.set_index(data.Month)\ndata.drop('Month', axis = 1, inplace = True)\n\n# look at the first 5 rows\ndata.head()\n</code></pre> Passengers Month 1949-01-01 112 1949-02-01 118 1949-03-01 132 1949-04-01 129 1949-05-01 121 <pre><code># plt.figure(figsize= (10,6))\nplt.plot(data)\nplt.xlabel('Years')\nplt.ylabel('No of Air Passengers')\nplt.title('Trend of the Time Series')\nplt.show()\n</code></pre> <p>As you can see from above plot there is upward trend of number of passenger for every year. </p> <pre><code># To plot the seasonality we are going to create a temp dataframe \n# and add columns for Month and Year values\ndata_temp = data.copy()\ndata_temp['Year'] = pd.DatetimeIndex(data_temp.index).year\ndata_temp['Month'] = pd.DatetimeIndex(data_temp.index).month\n\n# Stacked line plot\nplt.figure(figsize=(10,10))\nplt.title('Seasonality of the Time Series')\nsns.pointplot(x='Month',y='Passengers',hue='Year',data=data_temp)\n</code></pre> <pre>\n<code>&lt;Axes: title={'center': 'Seasonality of the Time Series'}, xlabel='Month', ylabel='Passengers'&gt;</code>\n</pre> <p>From above graph we can say that every year in month of July we observe maximum number of passengers and similarly minimum number of passenger in the month of November.</p> <pre><code>from statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomposition = seasonal_decompose(\n    data, \n    model='additive')\n\nfig = decomposition.plot()\n</code></pre> <p>[ad]</p> <p></p> <pre><code>def stationarity_test(timeseries):\n    # Get rolling statistics for window = 12 i.e. yearly statistics\n    rolling_mean = timeseries.rolling(window=12).mean()\n    rolling_std = timeseries.rolling(window=12).std()\n\n    # Plot rolling statistic\n    plt.figure(figsize= (10,6))\n    plt.xlabel('Years')\n    plt.ylabel('No of Air Passengers')    \n    plt.title('Stationary Test: Rolling Mean and Standard Deviation')\n    plt.plot(timeseries, color= 'blue', label= 'Original')\n    plt.plot(rolling_mean, color= 'green', label= 'Rolling Mean')\n    plt.plot(rolling_std, color= 'red', label= 'Rolling Std')   \n    plt.legend()\n    plt.show()\n\n    # Dickey-Fuller test\n    print('Results of Dickey-Fuller Test')\n    df_test = adfuller(timeseries)\n    df_output = pd.Series(df_test[0:4], index = ['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n    for key, value in df_test[4].items():\n        df_output['Critical Value (%s)' %key] = value\n    print(df_output)\n</code></pre> <pre><code># Lets test the stationarity score with original series data\nstationarity_test(data)\n</code></pre> <pre>\n<code>Results of Dickey-Fuller Test\nTest Statistic                   0.815369\np-value                          0.991880\n#Lags Used                      13.000000\nNumber of Observations Used    130.000000\nCritical Value (1%)             -3.481682\nCritical Value (5%)             -2.884042\nCritical Value (10%)            -2.578770\ndtype: float64\n</code>\n</pre> <p>Though it's clear from visual observation that it's not a stationary series, but still lets have look at the rolling statistics and Duckey Fuller test results</p> <ul> <li>Rolling statistics: Standard deviation has very less variation but mean is increasing continuously.</li> <li>Duckey Fuller Test: Test statistic is way more than the critical values.</li> </ul> <pre><code>data_diff = data.diff(periods = 1) # First order differencing\nplt.xlabel('Years')\nplt.ylabel('No of Air Passengers')    \nplt.title('Convert Non Stationary Data to Stationary Data using Differencing ')\nplt.plot(data_diff)\n</code></pre> <pre>\n<code>[&lt;matplotlib.lines.Line2D&gt;]</code>\n</pre> <p>So from above graph its clear that differencing technique removed the trend from the time series, but variance is still there Now lets run the <code>stationarity_test()</code> to check the effectiveness of the 'Differencing' technique</p> <pre><code>data_diff.dropna(inplace = True)# Data transformation may add na values\nstationarity_test(data_diff)\n</code></pre> <pre>\n<code>Results of Dickey-Fuller Test\nTest Statistic                  -2.829267\np-value                          0.054213\n#Lags Used                      12.000000\nNumber of Observations Used    130.000000\nCritical Value (1%)             -3.481682\nCritical Value (5%)             -2.884042\nCritical Value (10%)            -2.578770\ndtype: float64\n</code>\n</pre> <p>The rolling values appear to be varying slightly, and we can see there is slight upward trend in standard deviation. Also, the test statistic is smaller than the 10% critical but since p-value is greater than 0.05 it is not a stationary series.</p> <p>Note that variance in the series is also affecting above results, which can be removed using transformation technique.</p> <p>Let's also check with transformation technique</p> <pre><code># apply the log transformation\ndata_log = np.log(data)\n\n# let's plot the results\nplt.subplot(211)\nplt.plot(data, label= 'Time Series with Variance')\nplt.legend()\nplt.subplot(212)\nplt.plot(data_log, label='Time Series without Variance (Log Transformation)')\nplt.legend()  \nplt.show()\n</code></pre> <p>Since log transformation has removed the variance from series, lets use this transformed data hence forward.  Note that, Since we are using log transformation, we can use the exponential of the series to get the original scale back. <pre><code>    df = exp(df_log)\n</code></pre></p> <p>Let cross-check the differencing method scores with this log transformed data again.</p> <pre><code># First order differencing\ndata_log_diff = data_log.diff(periods = 1) \n\n# Data transformation may add na values\ndata_log_diff.dropna(inplace = True)\nstationarity_test(data_log_diff)\n</code></pre> <pre>\n<code>Results of Dickey-Fuller Test\nTest Statistic                  -2.717131\np-value                          0.071121\n#Lags Used                      14.000000\nNumber of Observations Used    128.000000\nCritical Value (1%)             -3.482501\nCritical Value (5%)             -2.884398\nCritical Value (10%)            -2.578960\ndtype: float64\n</code>\n</pre> <p>The rolling mean and standard deviation values are okay now. The test statistic is smaller than the 10% critical values but since p-value is greater than 0.05 it is not a stationary series.</p> <p>Let's also check with Moving Average technique\u2026</p> <pre><code>data_log_moving_avg = data_log.rolling(window = 12).mean()\n\n# let's plot the results\nplt.xlabel('Years')\nplt.ylabel('No of Air Passengers')    \nplt.title('Convert Non Stationary Data to Stationary Data using Moving Average')\nplt.plot(data_log, color= 'blue', label='Orignal')\nplt.plot(data_log_moving_avg, color= 'red', label='Moving Average')\nplt.legend()\nplt.show()\n</code></pre> <p>As you can see from above graph that data is more smooth without any variance. If we use the differencing technique with log transformed data and mean average data then we should get better stationarity scores</p> <pre><code>data_log_moving_avg_diff = data_log - data_log_moving_avg\ndata_log_moving_avg_diff.dropna(inplace = True)\nstationarity_test(data_log_moving_avg_diff)\n</code></pre> <pre>\n<code>Results of Dickey-Fuller Test\nTest Statistic                  -3.162908\np-value                          0.022235\n#Lags Used                      13.000000\nNumber of Observations Used    119.000000\nCritical Value (1%)             -3.486535\nCritical Value (5%)             -2.886151\nCritical Value (10%)            -2.579896\ndtype: float64\n</code>\n</pre> <p>As expected now we are able to see some real improvements. p-value is less than 0.05 that means our series is stationary, but we can only say this with 95% of confidence, as test statistics is less than 5% critical value.</p> <p>In order to increase the stationarity of the series lets try to use 'Weighted Moving Average' technique</p> <pre><code>data_log_weighted_avg = data_log.ewm(halflife = 12).mean()\nplt.plot(data_log)\nplt.plot(data_log_weighted_avg, color = 'red')\n</code></pre> <pre>\n<code>[&lt;matplotlib.lines.Line2D&gt;]</code>\n</pre> <p>Notice that WMA follow's no of passenger values more closely than a corresponding Simple Moving Average which also results in more accurate trend direction. Now lets check, the effect of this on stationarity scores!</p> <pre><code>data_log_weighted_avg_diff = data_log - data_log_weighted_avg\nstationarity_test(data_log_weighted_avg_diff)\n</code></pre> <pre>\n<code>Results of Dickey-Fuller Test\nTest Statistic                  -3.601262\np-value                          0.005737\n#Lags Used                      13.000000\nNumber of Observations Used    130.000000\nCritical Value (1%)             -3.481682\nCritical Value (5%)             -2.884042\nCritical Value (10%)            -2.578770\ndtype: float64\n</code>\n</pre> <p>Test statistic is smaller than the 1% critical value, which is better than the previous case. Note that in this case there will be no missing values as all values from starting are given weights. So it\u2019ll work even with no previous values.</p> <p>There is one more way to obtain better stationarity is by using the residual data from time series decomposition.</p> <pre><code>import statsmodels.api as smapi\n\ndecomposition = smapi.tsa.seasonal_decompose(data_log,period =12)\nfig = decomposition.plot()\n</code></pre> <p>Here we can see that the trend and seasonality are separated out from log transformed data, and we can now check the stationarity of the residuals</p> <pre><code>data_log_residual = decomposition.resid\ndata_log_residual.dropna(inplace = True)\nstationarity_test(data_log_residual)\n</code></pre> <pre>\n<code>Results of Dickey-Fuller Test\nTest Statistic                -6.332387e+00\np-value                        2.885059e-08\n#Lags Used                     9.000000e+00\nNumber of Observations Used    1.220000e+02\nCritical Value (1%)           -3.485122e+00\nCritical Value (5%)           -2.885538e+00\nCritical Value (10%)          -2.579569e+00\ndtype: float64\n</code>\n</pre> <p>The Dickey-Fuller test statistic is significantly lower than the 1% critical value and p-value is almost 0. So this time series is very close to stationary. This concludes our time series analysis and data transformation to get the stationary series. Now we can start modeling it for forecast.</p> <pre><code>lag_acf = acf(data_log_diff, nlags=20)\nlag_pacf = pacf(data_log_diff, nlags=20, method='ols')\n\n# Plot ACF: \nplt.subplot(121) \nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\n# Draw 95% confidence interval line\nplt.axhline(y=-1.96/np.sqrt(len(data_log_diff)),linestyle='--',color='red')\nplt.axhline(y=1.96/np.sqrt(len(data_log_diff)),linestyle='--',color='red')\nplt.xlabel('Lags')\nplt.title('Autocorrelation Function')\n\n#Plot PACF:\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\n# Draw 95% confidence interval line\nplt.axhline(y=-1.96/np.sqrt(len(data_log_diff)),linestyle='--',color='red')\nplt.axhline(y=1.96/np.sqrt(len(data_log_diff)),linestyle='--',color='red')\nplt.xlabel('Lags')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()\n</code></pre> <p>From above graph its clear that p=2 and q=2. Now we have the ARIMA parameters values, lets make 3 different ARIMA models considering individual as well as combined effects. We will also print the RSS(Residual Sum of Square) metric for each. Please note that here RSS is for the values of residuals and not actual series.</p> <pre><code># # freq = 'MS' &amp;gt; The frequency of the time-series MS = calendar month begin\n# # The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters to use\n# model = ARIMA(data_log, order=(2, 1, 0), freq = 'MS')  \n# results_AR = model.fit()\n# plt.plot(data_log_diff)\n# plt.plot(results_AR.fittedvalues, color='red')\n# plt.title('AR Model, RSS: %.4f'% sum((results_AR.fittedvalues - data_log_diff['Passengers'])**2))\n# plt.show()\n</code></pre> <pre><code># model = ARIMA(data_log, order=(0, 1, 2), freq = 'MS')  \n# results_MA = model.fit()  \n# plt.plot(data_log_diff)\n# plt.plot(results_MA.fittedvalues, color='red')\n# plt.title('MA Model, RSS: %.4f'% sum((results_MA.fittedvalues-data_log_diff['Passengers'])**2))\n</code></pre> <pre><code># model = ARIMA(data_log, order=(2, 1, 2), freq = 'MS')  \n# results_ARIMA = model.fit(disp=-1)  \n# plt.plot(data_log_diff)\n# plt.plot(results_ARIMA.fittedvalues, color='red')\n# plt.title('Combined Model, RSS: %.4f'% sum((results_ARIMA.fittedvalues-data_log_diff['Passengers'])**2))\n</code></pre> <p>Here we can see that the AR and MA models have almost the same RSS score but combined is significantly better. So we will go ahead with combined ARIMA model and use it for predictions.</p> <pre><code># # Create a separate series of predicted values\n# predictions_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\n\n# print('Total no of predictions: ', len(predictions_diff))\n# predictions_diff.head()\n</code></pre> <p>Since we are using first order of differencing(d =1), there is no prediction available for first value (1949-02-01) of original series. In order to remove 'differencing transformation' from the prediction values we are going to add these differences consecutively to the base number. An easy way to do it is to first determine the cumulative sum at index and then add it to the base number. We are going to use pandas cumsum() function for it.</p> <pre><code># predictions_diff_cumsum = predictions_diff.cumsum()\n# predictions_diff_cumsum.head()\n</code></pre> <p>Above values once added to the base number will completely remove the differencing transformation. For this, lets create a series with all values as base number and add the 'predictions_diff_cumsum' to it.</p> <pre><code># predictions_log = pd.Series(df_log['Passengers'].iloc[0], index=df_log.index) # Series of base number\n# predictions_log = predictions_log.add(predictions_diff_cumsum,fill_value=0)\n# predictions_log.head()\n</code></pre> <p>So as of now we have removed the differencing transformation, now lets remove the log transformation to get the original scale back.</p> <pre><code># predictions = np.exp(predictions_log)\n# plt.plot(df)\n# plt.plot(predictions)\n</code></pre> <pre><code># df_predictions =pd.DataFrame(predictions, columns=['Predicted Values'])\n# pd.concat([df,df_predictions],axis =1).T\n</code></pre> <pre><code># results_ARIMA.plot_predict(start = 1, end= 204) \n</code></pre> <pre><code># # Forecasted values in original scale will be\n# forecast_values_log_scale = results_ARIMA.forecast(steps = 60)\n# forecast_values_original_scale = np.exp(forecast_values_log_scale[0])\n\n# forecast_date_range= pd.date_range(\"1961-01-01\", \"1965-12-01\", freq=\"MS\")\n\n# df_forecast =pd.DataFrame(forecast_values_original_scale, columns=['Forecast'])\n# df_forecast['Month'] = forecast_date_range\n\n# df_forecast[['Month', 'Forecast']]\n</code></pre> <p> To be among the first to hear about future updates, simply enter your email below, follow us on   (formally Twitter), or subscribe to our  YouTube channel. </p> <p></p>"},{"location":"Time%20Series/93_python_example/#dont-miss-any-updates","title":"Don't Miss Any Updates!","text":""},{"location":"Time%20Series/93_python_example/#import-the-library","title":"Import The Library","text":"<ul> <li>dataidea: allows us to quickly load most common packages and datasets for the course</li> <li>statsmodels: Using statsmodels module classes and functions for time series analysis and forecasting </li> <li>adfuller: Augmented Dickey-Fuller</li> <li>ACF: Auto Correlation Function</li> <li>PACF: Partial Auto Correlation Function</li> <li>ARIMA: Autoregressive Integrated Moving Average ARIMA(p,d,q) Model</li> <li>sm.tsa.seasonal.seasonal_decompose: For decomposition of time series</li> <li>rcParams: To change the matplotlib properties like figure size</li> </ul>"},{"location":"Time%20Series/93_python_example/#understanding-the-data","title":"Understanding The Data","text":"<ul> <li>Dataframe <code>data</code> contains the time series data. There are two columns <code>Month</code> and <code>Passengers</code>. <code>Month</code> column contains the value of month in that year and passenger column contains the number of air passengers for that particular month.</li> <li>As you may have noticed <code>Month</code> column datatype is <code>Object</code>, so we are going to convert it to <code>datetime</code></li> <li>To make plotting easier, we set the index of pandas dataframe <code>data</code> to the <code>Month</code> column so that it will act as x-axis &amp; Passenger column as y-axis</li> </ul>"},{"location":"Time%20Series/93_python_example/#time-series-characteristics","title":"Time Series Characteristics","text":""},{"location":"Time%20Series/93_python_example/#trend","title":"Trend","text":""},{"location":"Time%20Series/93_python_example/#variance","title":"Variance","text":"<p>In above graph you can clearly see that the variation is also increasing with the level of the series. You will see in the later part of this exercise how we handle the variance to increase the stationarity of the series.</p>"},{"location":"Time%20Series/93_python_example/#seasonality","title":"Seasonality","text":"<p>We can also see the graph going up and down at regular interval, that is the sign of seasonality. Let's plot the graph for few months to visualize for seasonality.</p>"},{"location":"Time%20Series/93_python_example/#decomposition-of-time-series","title":"Decomposition of Time Series","text":"<p>Let's now use the decomposition technique to deconstruct the time series data into several component like trend and seasonality for visualization of time series characteristics.</p> <p>Here we are going to use 'additive' model because it is quick to develop, fast to train, and provide interpretable patterns.</p>"},{"location":"Time%20Series/93_python_example/#time-series-analysis","title":"Time Series Analysis","text":"<p>So our time series has variance, trend and seasonality characteristics. During our analysis we are going to try multiple techniques to make time series stationary and record the stationarity scores for each method. Finally, we will select the method, which is easy for inverse transformation easy and give best stationarity score.</p>"},{"location":"Time%20Series/93_python_example/#check-for-stationarity","title":"Check for Stationarity","text":"<p>We are going to use rolling statistics and Dickey-Fuller test to check the stationarity of the time series</p>"},{"location":"Time%20Series/93_python_example/#convert-non-stationary-data-to-stationary-data","title":"Convert Non-Stationary Data to Stationary Data","text":"<p>Let's first use the differencing technique to obtain the stationarity.</p>"},{"location":"Time%20Series/93_python_example/#differencing","title":"Differencing","text":"<p>To transform the series using Differencing we will use the <code>diff()</code> method of pandas. A benefit of using the Pandas function, in addition to requiring less code, is that it maintains the date-time information for the differenced series</p> <p>\\(\\(Y_t' = Y_t - Y_{t-1}\\)\\)</p>"},{"location":"Time%20Series/93_python_example/#transformation","title":"Transformation","text":"<p>Since variance is proportional to the levels, we are going to use the log transformation.</p>"},{"location":"Time%20Series/93_python_example/#moving-average","title":"Moving Average","text":"<p>Since we have time series data from 1 Jan 1949 to 1 Dec 1960, we will define a yearly window for moving average. Window size = 12. Note that we are going to use Log transformed data.</p>"},{"location":"Time%20Series/93_python_example/#weighted-moving-average-wma","title":"Weighted Moving Average (WMA)","text":"<p>Here we are going to use exponentially weighted moving average with parameter \u2018halflife = 12\u2019. This parameter defines the amount of exponential decay. This is just an assumption here and would depend largely on the business domain.</p>"},{"location":"Time%20Series/93_python_example/#decomposition-of-time-series_1","title":"Decomposition of Time Series","text":"<p>Let's now use the decomposition technique to deconstruct the log transformed time series data, so that we can check the stationarity using residual data.</p>"},{"location":"Time%20Series/93_python_example/#forecasting","title":"Forecasting","text":"<ul> <li>Though using residual values gives us very good results, but it's relatively difficult to add noise and seasonality back into predicted residuals in this case. </li> <li>So we are going to make model on the time series(<code>df_log_diff</code>), where we have used log transformation and differencing technique. This is one of the most popular and beginner-friendly technique. As per our time series analysis <code>df_log_diff</code> is not a perfectly stationary series, that's why we are going to use statistical models like ARIMA to forecast the data.</li> <li>Remember that ARIMA model uses three parameters, <code>p</code> for the order of Auto-Regressive (AR) part, <code>q</code> for the order of Moving Average (MA) part and <code>d</code> for the order of integrated part. We are going to use <code>d=1</code> but to find the value for <code>p</code> and <code>q</code> lets plot ACF and PACF.</li> <li>Note that since we are using <code>d=1</code>, first order of differencing will be performed on given series. Since first value of time series don't have any value to subtract from resulting series will have one less value from original series</li> </ul>"},{"location":"Time%20Series/93_python_example/#acf-and-pacf-plots","title":"ACF and PACF Plots","text":"<ul> <li>To figure out the order of AR model(p) we will use PACF function. <code>p</code> = the lag value where the PACF chart crosses the upper confidence interval for the first time</li> <li>To figure out the order of MA model(<code>q</code>) we will use ACF function. <code>q</code> = the lag value where the ACF chart crosses the upper confidence interval for the first time</li> </ul>"},{"location":"Time%20Series/93_python_example/#ar-model","title":"AR Model","text":"<p>Since <code>q</code> is MA model parameter we will keep its value as <code>0</code>.</p>"},{"location":"Time%20Series/93_python_example/#ma-model","title":"MA Model","text":"<p>Since 'p' is AR model parameter we will keep its value as '0'.</p>"},{"location":"Time%20Series/93_python_example/#combined-model","title":"Combined Model","text":""},{"location":"Time%20Series/93_python_example/#prediction-and-reverse-transformation","title":"Prediction and Reverse Transformation","text":"<ul> <li>We will create a separate series of predicted values using ARIMA model</li> <li>Reverse transform the predicted values to get the original scale back</li> <li>Compare the predicted values with original values and plot them</li> </ul>"},{"location":"Time%20Series/93_python_example/#future-forecasting","title":"Future Forecasting","text":"<ul> <li>We have data from 1 Jan 1949 to 1 Dec 1960. 12 years of data with passenger number observation for each month i.e. 144 total observations.</li> <li>If we want to forecast for next 5 years or 60 months then, \u2018end\u2019 count will be &gt;  144 + 60 = 204.</li> <li>We are going to use statsmodels plot_predict() method for it</li> </ul>"},{"location":"Time%20Series/93_python_example/#whats-on-your-mind-put-it-in-the-comments","title":"What's on your mind? Put it in the comments!","text":""}]}